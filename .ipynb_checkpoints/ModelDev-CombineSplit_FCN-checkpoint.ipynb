{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf617ae-7cd6-4168-9b2a-c61a35fcaf7c",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7287c-5c7e-4d25-9085-f3971b3d5784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#uncomment this below code to install imblearn package\n",
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95c2128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pyreadr\n",
    "\n",
    "#statistics\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "\n",
    "import cudf #gpu-powered DataFrame (Pandas alternative)\n",
    "\n",
    "#imbalance handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, RepeatedEditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "#internal validation\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV, PredefinedSplit, train_test_split\n",
    "\n",
    "#performance metrices\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score, matthews_corrcoef, auc, average_precision_score, roc_auc_score, balanced_accuracy_score, roc_curve, accuracy_score\n",
    "\n",
    "#Models selection\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from cuml.svm import SVC #gpu-powered SVM\n",
    "\n",
    "#Tree pruning\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "\n",
    "#save and load trained model\n",
    "import pickle\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2fa869-bb96-4ebc-9d51-2b542e9486e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "# trainingData = pd.read_csv('../FinalData/trainingDataEncoded_08102023.csv')\n",
    "# validationData = pd.read_csv('../FinalData/validationDataEncoded_08102023.csv')\n",
    "# evaluationData = pd.read_csv('../FinalData/evaluationDataEncoded_08102023.csv')\n",
    "# evaluationDataWales = pd.read_csv('../FinalData/evaluationDataWalesEncoded_08102023.csv')\n",
    "# evaluationDataScotland = pd.read_csv('../FinalData/evaluationDataScotlandEncoded_08102023.csv')\n",
    "\n",
    "trainingData, validationData, internalEvaluationData, evaluationData, evaluationDataWales, evaluationDataScotland = pickle.load(open('../FinalData/dataset_scaled_2vs1_09122023.sav', 'rb'))\n",
    "\n",
    "# trainingData = trainingData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# validationData = validationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# internalEvaluationData = internalEvaluationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationData = evaluationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationDataWales = evaluationDataWales.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationDataScotland = evaluationDataScotland.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11624789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Data loader\n",
    "# trainingData = pd.read_csv('../FinalData/trainingDataEncoded_2vs1_16112023.csv')\n",
    "# validationData = pd.read_csv('../FinalData/validationDataEncoded_2vs1_16112023.csv')\n",
    "# internalEvaluationData = pd.read_csv('../FinalData/internalEvaluationDataEncoded_2vs1_16112023.csv')\n",
    "# evaluationData = pd.read_csv('../FinalData/evaluationDataEncoded_2vs1_16112023.csv')\n",
    "# evaluationDataWales = pd.read_csv('../FinalData/evaluationDataWalesEncoded_2vs1_16112023.csv')\n",
    "# evaluationDataScotland = pd.read_csv('../FinalData/evaluationDataScotlandEncoded_2vs1_16112023.csv')\n",
    "\n",
    "trainingData = trainingData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "validationData = validationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "internalEvaluationData = internalEvaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationData = evaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataWales = evaluationDataWales.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataScotland = evaluationDataScotland.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09186b49-193f-4c37-9430-4d8f01788601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "12months\n",
      "0    0.880768\n",
      "1    0.119232\n",
      "Name: 12months, dtype: float64\n",
      "0    0.880771\n",
      "1    0.119229\n",
      "Name: 12months, dtype: float64\n",
      "0    0.836507\n",
      "1    0.163493\n",
      "Name: 12months, dtype: float64\n",
      "0    0.838914\n",
      "1    0.161086\n",
      "Name: 12months, dtype: float64\n",
      "0    0.832189\n",
      "1    0.167811\n",
      "Name: 12months, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_outcomes = ['12months'] \n",
    "for target_outcome in target_outcomes:\n",
    "    print('#######################################################')\n",
    "    print(target_outcome)\n",
    "    print(trainingData[target_outcome].value_counts(normalize=True))\n",
    "    print(validationData[target_outcome].value_counts(normalize=True))\n",
    "    # print(internalEvaluationData[target_outcome].value_counts(normalize=True))\n",
    "    print(evaluationData[target_outcome].value_counts(normalize=True))\n",
    "    print(evaluationDataWales[target_outcome].value_counts(normalize=True))\n",
    "    print(evaluationDataScotland[target_outcome].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f7e171-4ac8-4a82-9aac-5c9e2128484c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features size:  67\n",
      "['sex', 'age', 'CharlsonScore', 'BTS_step', 'average_daily_dose_ICS', 'prescribed_daily_dose_ICS', 'ICS_medication_possesion_ratio', 'numOCS', 'PriorEducation', 'numPCS', 'numPCSAsthma', 'numAntibioticsEvents', 'numAntibioticswithLRTI', 'numOCSwithLRTI', 'numAsthmaAttacks', 'numAcuteRespEvents', 'numHospEvents', 'rhinitis', 'cardiovascular', 'heartfailure', 'psoriasis', 'anaphylaxis', 'diabetes', 'ihd', 'anxiety', 'eczema', 'nasalpolyps', 'ethnic_group_Asian - ethnic group', 'ethnic_group_Black - ethnic group', 'ethnic_group_Mixed ethnic census group', 'ethnic_group_Other ethnic group', 'ethnic_group_White - ethnic group', 'ethnic_group_not_recorded', 'smokingStatus_Active Smoker', 'smokingStatus_Former Smoker', 'smokingStatus_Non Smoker', 'DeviceType_BAI', 'DeviceType_DPI', 'DeviceType_NEB', 'DeviceType_pMDI', 'DeviceType_unknown', 'cat_BMI_normal', 'cat_BMI_not recorded', 'cat_BMI_obese', 'cat_BMI_overweight', 'cat_BMI_underweight', 'imd_decile_0', 'imd_decile_1', 'imd_decile_2', 'imd_decile_3', 'imd_decile_4', 'imd_decile_5', 'imd_decile_6', 'imd_decile_7', 'imd_decile_8', 'imd_decile_9', 'imd_decile_10', 'PEFStatus_60-80', 'PEFStatus_less than 60', 'PEFStatus_more than 80', 'PEFStatus_not_recorded', 'EosinophilLevel_high', 'EosinophilLevel_normal', 'EosinophilLevel_unknown', 'system_EMIS', 'system_SystemOne', 'system_Vision']\n"
     ]
    }
   ],
   "source": [
    "#Define feature candidates\n",
    "\n",
    "features_columns = trainingData.columns.to_list()\n",
    "exclude_columns = ['patid', 'practice_id', #identifier\n",
    "                   'BMI', #use the categorical instead\n",
    "                   'ethnicity', #use ethnic_group instead\n",
    "                   'Spacer',  #all zero\n",
    "                   \n",
    "                   'outcome_3months', 'outcome_6months', 'outcome_9months', 'outcome_12months', 'outcome_15months', 'outcome_18months', \n",
    "                   'outcome_21months', 'outcome_24months', 'outcome_combined_6months', 'outcome_combined_9months', 'outcome_combined_12months', \n",
    "                   'outcome_combined_15months', 'outcome_combined_18months', 'outcome_combined_24months', '3months', '6months', '9months', '12months', '24months', #outcomes variable\n",
    "                   \n",
    "                   'postcode_district', 'County', 'LocalAuthority', 'OutputAreaClassification', #location related variables, use IMD decile only\n",
    "                   \n",
    "                   'cat_age', 'cat_average_daily_dose_ICS', 'cat_prescribed_daily_dose_ICS', 'cat_ICS_medication_possesion_ratio', 'cat_numOCS', 'cat_numOCSEvents', \n",
    "                   'cat_numOCSwithLRTI', 'cat_numAcuteRespEvents', 'cat_numAntibioticsEvents', 'cat_numAntibioticswithLRTI', 'cat_numAsthmaAttacks', 'cat_numHospEvents', \n",
    "                   'cat_numPCS', 'cat_numPCSAsthma', #use continous vars instead\n",
    "                   \n",
    "                   'count_rhinitis', 'count_cardiovascular', 'count_heartfailure',\n",
    "                   'count_psoriasis', 'count_anaphylaxis', 'count_diabetes', 'count_ihd',\n",
    "                   'count_anxiety', 'count_eczema', 'count_nasalpolyps',\n",
    "                   'count_paracetamol', 'count_nsaids', 'count_betablocker', #use binary ones\n",
    "                   \n",
    "                   'paracetamol', 'nsaids', 'betablocker', #no data in evaluation\n",
    "                   \n",
    "                   'numOCSEvents', #duplicate with numOCS\n",
    "                   \n",
    "                   'month_12', 'month_4', 'month_5', 'month_10', 'month_1', 'month_6', 'month_3', \n",
    "                   'month_11', 'month_8', 'month_9', 'month_7', 'month_2', #month of attacks\n",
    "                   \n",
    "                   # 'system_EMIS', 'system_SystemOne', 'system_Vision', #primary care system used\n",
    "                  ]\n",
    "exclude_columns = exclude_columns + [x for x in features_columns if '_count' in x] #filter out commorbid count variables\n",
    "features_columns = [x for x in features_columns if x not in exclude_columns]\n",
    "print('Features size: ', len(features_columns))\n",
    "print(features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037f720-0ed4-4e4a-84c8-f511fadac736",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4db59da-1f72-428e-9203-08d62c6d9090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model evaluation function\n",
    "\n",
    "def summariseResult (testX, testY, model):\n",
    "    preds = model.predict(testX)\n",
    "    # preds = [x[1] for x in preds]\n",
    "    # tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    # specificity = tn / (tn+fp)\n",
    "    # sensitivity = tp / (tp+fn)\n",
    "    # ppv = 100*tp/(tp+fp)\n",
    "    # npv = 100*tn/(fn+tn)\n",
    "    # acc = accuracy_score(testY, preds)\n",
    "    # f1score = f1_score(testY, preds, average = 'binary')\n",
    "    # balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    # aucscore = auc(fpr, tpr)\n",
    "    aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(aucscore,4), np.round(auprc,4)\n",
    "    # return np.round(acc,4), np.round(specificity,4), np.round(sensitivity,4), np.round(aucscore,4), np.round(auprc,4), np.round(balanceacc,4), np.round(f1score,4), np.round(ppv,4), np.round(npv,4)\n",
    "\n",
    "#Fix model name for visualisation\n",
    "\n",
    "def modelNameFixer(x):\n",
    "    if 'liblinear' in x:\n",
    "        return 'Lasso'\n",
    "    elif 'GaussianNB' in x:\n",
    "        return 'GNB'\n",
    "    elif 'SVC' in x:\n",
    "        return 'SVC'\n",
    "    elif 'RandomForest' in x:\n",
    "        return 'RF'\n",
    "    elif 'XGB' in x:\n",
    "        return 'XGBoost'\n",
    "    elif 'DecisionTree' in x:\n",
    "        return 'DT'\n",
    "    else:\n",
    "        return 'LR'\n",
    "    \n",
    "    \n",
    "# instantiate the model (using the default parameters)\n",
    "def build_models (X_train, y_train, target_outcome, params_dict, model_fodler, fold):\n",
    "    models = [] #list to store all the models\n",
    "    print(\"Building models . . . .\")\n",
    "\n",
    "    #LR\n",
    "    model = 'LR'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    lr_model = LogisticRegression(class_weight='balanced', C = params['C'], max_iter=params['max_iter'], solver=params['solver'], random_state=1234)\n",
    "    lr_model.fit(X_train,y_train)\n",
    "    pickle.dump(lr_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb'))\n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]]) \n",
    "    print(\"LR done\")\n",
    "\n",
    "    #Lasso\n",
    "    model = 'Lasso'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    lasso_model = LogisticRegression(class_weight='balanced',  C = params['C'], max_iter=params['max_iter'], penalty='l1', solver=params['solver'], random_state=1234) #only the LIBLINEAR and SAGA (added in v0.19) solvers handle the L1 penalty\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    pickle.dump(lasso_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb'))\n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"Lasso done\")\n",
    "    \n",
    "    #Elastics\n",
    "    model = 'Elastics'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    elastics_model = LogisticRegression(class_weight='balanced', solver='saga', l1_ratio=params['l1_ratio'], max_iter=params['max_iter'],  penalty = 'elasticnet', random_state=1234)\n",
    "    elastics_model.fit(X_train, y_train)\n",
    "    pickle.dump(elastics_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb'))\n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"Elastics done\")\n",
    "\n",
    "    # #GNB\n",
    "    # model = 'NB'\n",
    "    # params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # # params = eval(params)\n",
    "    # print(params)\n",
    "    # gnb_model = GaussianNB(var_smoothing = params['var_smoothing'])\n",
    "    # gnb_model.fit(X_train, y_train)\n",
    "    # pickle.dump(gnb_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb'))  \n",
    "    # models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    # print(\"GNB done\")\n",
    "\n",
    "    # # #SVM\n",
    "    # model = 'SVM'\n",
    "    # params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # # params = eval(params)\n",
    "    # print(params)\n",
    "    # svc_model = SVC(class_weight='balanced', C = params['C'], gamma=params['gamma'], kernel='rbf', random_state=1234, cache_size=1000)\n",
    "    # svc_model.fit(X_train,y_train)\n",
    "    # pickle.dump(svc_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb')) \n",
    "    # models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    # print(\"SVM done\")\n",
    "\n",
    "    #DT\n",
    "    model = 'DT'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    dt_model = DecisionTreeClassifier(class_weight='balanced', max_depth=params['max_depth'], criterion=params['criterion'], splitter=params['splitter'], random_state=1234)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    pickle.dump(dt_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb'))    \n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"DT done\")\n",
    "\n",
    "    #RF\n",
    "    model = 'RF'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    rf_model = RandomForestClassifier(class_weight='balanced', max_depth=params['max_depth'], criterion=params['criterion'], n_estimators=params['n_estimators'], random_state=1234)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    pickle.dump(rf_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb'))     \n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"RF done\")\n",
    "\n",
    "    #XGB\n",
    "    model = 'XGB'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    scale_pos_ratio = y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "    xgb_model = xgb.XGBClassifier(objective ='binary:logistic', max_depth = params['max_depth'], n_estimators = params['n_estimators'],   \n",
    "                                  learning_rate=params['learning_rate'], reg_alpha = params['reg_alpha'], reg_lambda = params['reg_lambda'],\n",
    "                                  importance_type = 'gain', scale_pos_weight = scale_pos_ratio, use_label_encoder=False, tree_method='gpu_hist', \n",
    "                                  gpu_id=0, verbosity = 0, random_state = 1234,)\n",
    "    # xgb_model = xgb.XGBClassifier(objective ='binary:logistic', learning_rate = 0.001, tree_method='gpu_hist', gpu_id=0,  verbosity = 0, random_state = 1234)\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    pickle.dump(xgb_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb')) \n",
    "    models.append([model + str(fold),  target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"XGB done\")\n",
    "    \n",
    "    return models\n",
    "    # return [xgb_model]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def is_leaf(inner_tree, index):\n",
    "    # Check whether node is leaf node\n",
    "    return (inner_tree.children_left[index] == TREE_LEAF and \n",
    "            inner_tree.children_right[index] == TREE_LEAF)\n",
    "\n",
    "def prune_index(inner_tree, decisions, index=0):\n",
    "    # Start pruning from the bottom - if we start from the top, we might miss\n",
    "    # nodes that become leaves during pruning.\n",
    "    # Do not use this directly - use prune_duplicate_leaves instead.\n",
    "    if not is_leaf(inner_tree, inner_tree.children_left[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_left[index])\n",
    "    if not is_leaf(inner_tree, inner_tree.children_right[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_right[index])\n",
    "\n",
    "    # Prune children if both children are leaves now and make the same decision:     \n",
    "    if (is_leaf(inner_tree, inner_tree.children_left[index]) and\n",
    "        is_leaf(inner_tree, inner_tree.children_right[index]) and\n",
    "        (decisions[index] == decisions[inner_tree.children_left[index]]) and \n",
    "        (decisions[index] == decisions[inner_tree.children_right[index]])):\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "        ##print(\"Pruned {}\".format(index))\n",
    "\n",
    "def prune_duplicate_leaves(mdl):\n",
    "    # Remove leaves if both \n",
    "    decisions = mdl.tree_.value.argmax(axis=2).flatten().tolist() # Decision for each node\n",
    "    prune_index(mdl.tree_, decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba329484-ef88-4bba-a169-70f029879bf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977620af-cffb-4e5d-a21f-8f5ff3df530d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f225c66-5f1f-4331-a3c1-45e22ab444c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#params\n",
    "params_dict = pd.read_csv('../Models/BS_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80821b01-8f91-4da1-ab92-c7fa3891eb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_params(param_items, best_param):\n",
    "    a = eval(param_items)\n",
    "    b = eval(best_param)\n",
    "    c = {}\n",
    "    for key, value in zip(a,b):\n",
    "        c[key] = value\n",
    "    return c\n",
    "\n",
    "params_dict['params'] = params_dict.apply(lambda x: process_params(x.param_items, x.best_param), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97da62-733f-46ac-aeec-b9ced5aafe30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_dict[params_dict.outcome=='12months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf35146-ffe8-41e8-a831-a840906e440c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#EXECUTE model training\n",
    "summary_result_val = []\n",
    "summary_result_eval = []\n",
    "summary_result_Wales = []\n",
    "summary_result_Scotland = []\n",
    "# cols = ['model_name', 'outcome', 'class_ratio', 'acc','spec','sens','auc', 'auprc', 'balance_accuracy', 'f1_score', 'ppv', 'npv']\n",
    "cols = ['model_name', 'outcome', 'class_ratio', 'auc', 'auprc']\n",
    "model_folder = '../Models_trainValEval/'\n",
    "fold = 0\n",
    "\n",
    "for target_outcome in target_outcomes:\n",
    "    models = pd.DataFrame(columns=['modelname', 'target_outcome', 'class_ratio'])\n",
    "    print(target_outcome)\n",
    "    y = trainingData[target_outcome]\n",
    "    y_val = validationData[target_outcome]\n",
    "    y = pd.concat([y, y_val])\n",
    "    y_internaleval = internalEvaluationData[target_outcome]\n",
    "    y_eval = evaluationData[target_outcome]\n",
    "    y_eval_Wales = evaluationDataWales[target_outcome]\n",
    "    y_eval_Scotland = evaluationDataScotland[target_outcome]\n",
    "    #Build models -> it can be commented if the models have been trained\n",
    "    models_temp = pd.DataFrame(build_models(X, y, target_outcome, params_dict, model_folder, fold), columns=['modelname', 'target_outcome', 'class_ratio'])\n",
    "    models = pd.concat([models,models_temp]).reset_index(drop=True)\n",
    "\n",
    "    #evaluate model\n",
    "    for modelname, target_outcome, classratio in models.values:\n",
    "        # print('======================================================================')\n",
    "        print(modelname)\n",
    "        model = pickle.load(open(model_folder+ target_outcome + '/'+ modelname + '.sav', 'rb'))\n",
    "        summary_result_eval.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval, y_eval, model) )\n",
    "        summary_result_Wales.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval_Wales, y_eval_Wales, model) )       \n",
    "        summary_result_Scotland.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval_Scotland, y_eval_Scotland, model) )       \n",
    "        summary_result_val.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_internaleval, y_internaleval, model) )       \n",
    "\n",
    "\n",
    "summary_result_eval = pd.DataFrame(summary_result_eval, columns=cols)\n",
    "summary_result_eval['model_num'] = summary_result_eval.index\n",
    "\n",
    "summary_result_Wales = pd.DataFrame(summary_result_Wales, columns=cols)\n",
    "summary_result_Wales['model_num'] = summary_result_Wales.index\n",
    "\n",
    "summary_result_Scotland = pd.DataFrame(summary_result_Scotland, columns=cols)\n",
    "summary_result_Scotland['model_num'] = summary_result_Scotland.index\n",
    "\n",
    "summary_result_internaleval = pd.DataFrame(summary_result_val, columns=cols)\n",
    "summary_result_internaleval['model_num'] = summary_result_val.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df0bb2-a000-4236-a576-7d625e72205c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary_result_eval['model_name'] = summary_result_eval.apply(lambda x: modelNameFixer(x['model_name']), axis=1)\n",
    "# summary_result_val['model_name'] = summary_result_val.apply(lambda x: modelNameFixer(x['model_name']), axis=1)\n",
    "# summary_result_Wales['model_name'] = summary_result_Wales.apply(lambda x: modelNameFixer(x['model_name']), axis=1)\n",
    "# summary_result_Scotland['model_name'] = summary_result_Scotland.apply(lambda x: modelNameFixer(x['model_name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993827f6-2c42-4670-8d36-c1e016c7b46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary_result.to_csv('../Models/summary_result_test.csv', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d90777-2b7e-47bf-b71a-4830b584e880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary_result = pd.read_csv('../Models/summary_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3017ee-42a8-4f9f-83f2-af18bbc05fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_result_internaleval['set'] = 'England'\n",
    "summary_result_eval['set'] = 'Wales & Scotland'\n",
    "summary_result_Wales['set'] = 'Wales'\n",
    "summary_result_Scotland['set'] = 'Scotland'\n",
    "\n",
    "combine = pd.concat([summary_result_internaleval, summary_result_eval, \n",
    "                     summary_result_Wales, summary_result_Scotland,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9a3e4-006f-4f11-a7a7-6c3520d3a508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(combine, open('../Models/benchmarking_result_1year.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236a839-728a-46b6-b77a-730c94bf5c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combine\n",
    "data = combine[(combine.outcome=='12months')]\n",
    "bar = sns.catplot(x = 'model_name',       # x variable name\n",
    "            y = 'auc',       # y variable name            \n",
    "            data=data,\n",
    "            kind = \"bar\",\n",
    "            hue = 'set',\n",
    "            # hue_order=['10-fold CV', 'Evaluation Set', 'Wales', 'Scotland'],\n",
    "            height=8,\n",
    "            row='outcome',\n",
    "            aspect=1.8,\n",
    "            errorbar = None,)\n",
    "\n",
    "for items in bar.axes:\n",
    "    for ax in items:\n",
    "        for p in ax.patches:\n",
    "            ax.text(p.get_x() + 0.01, \n",
    "                p.get_height() * 1.005, \n",
    "                '{0:.4f}'.format(p.get_height()), \n",
    "                color='black', rotation=20, fontsize=8)\n",
    "\n",
    "ax.set_ylim(0.5, .9)\n",
    "# ax.set_ylabel('AUC Score', fontsize=13)\n",
    "ax.set_xlabel('Method', fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326ce41-3969-402d-982b-518b3a147c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75c81677-6d74-40ce-a81c-ea773f316362",
   "metadata": {},
   "source": [
    "# DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d1b542-d22a-46df-adbd-f91da32129db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 19:51:28.837566: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-09 19:51:29.006985: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-09 19:51:29.007051: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-09 19:51:29.007906: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-09 19:51:29.096197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Input, concatenate, Reshape, Activation, Flatten, Add, BatchNormalization, Multiply, LeakyReLU, Conv1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.metrics import AUC, SensitivityAtSpecificity\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, Adamax, SGD, Adadelta\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import L1L2, L1, L2\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "#internal validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score, matthews_corrcoef, auc, average_precision_score, roc_auc_score, balanced_accuracy_score, roc_curve, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774743ae-fc94-486e-993d-98d1a45db495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429667, 67)\n",
      "(134271, 67)\n",
      "(26044, 67)\n",
      "(16724, 67)\n",
      "(9320, 67)\n"
     ]
    }
   ],
   "source": [
    "X = trainingData[features_columns]\n",
    "X_val = validationData[features_columns]\n",
    "# X = pd.concat([X, X_val])\n",
    "\n",
    "X_internaleval = internalEvaluationData[features_columns]\n",
    "X_eval = evaluationData[features_columns]\n",
    "X_eval_Wales = evaluationDataWales[features_columns]\n",
    "X_eval_Scotland = evaluationDataScotland[features_columns]\n",
    "\n",
    "print(X.shape)\n",
    "print(X_val.shape)\n",
    "print(X_eval.shape)\n",
    "print(X_eval_Wales.shape)\n",
    "print(X_eval_Scotland.shape)\n",
    "\n",
    "# target_outcomes = ['3months', '6months', '12months', '24months'] \n",
    "target_outcomes = ['12months'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcce882-173d-413e-a385-4c517f088709",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2fe4d-4d8e-424c-8525-cb2414c28a5f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = 50\n",
    "y = trainingData[target_outcomes[0]].values\n",
    "y_val = validationData[target_outcomes[0]].values\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y)/sum(x == 1 for x in y)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "# print(class_weight)\n",
    "set\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1), input_shape = (X.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(Conv1D(64, 3, activation='relu', input_shape = (X.shape[1],)))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adadelta(learning_rate=5e-3, clipvalue=0.3)\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=1000, name='auc'),\n",
    "        AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "    ]\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "    print(model.summary())\n",
    "    history = model.fit(X.values, y, validation_data=(X_val.values, y_val), epochs=200, batch_size=128, class_weight=class_weight,  callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a09cb-c012-4b35-9032-406616539eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auprc'])\n",
    "plt.plot(history.history['val_auprc'])\n",
    "plt.title('model auprc')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('auprc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5336e-f6c4-41ad-9296-fb110e3d2fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53c17e-f382-4c12-9bf5-67dc0044ab9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_internaleval = internalEvaluationData[target_outcomes[0]]\n",
    "y_eval = evaluationData[target_outcomes[0]]\n",
    "y_eval_Wales = evaluationDataWales[target_outcomes[0]]\n",
    "y_eval_Scotland = evaluationDataScotland[target_outcomes[0]]\n",
    "print(model.evaluate(X_internaleval.values, y_internaleval))\n",
    "print(model.evaluate(X_eval.values, y_eval))\n",
    "print(model.evaluate(X_eval_Wales.values, y_eval_Wales))\n",
    "print(model.evaluate(X_eval_Scotland.values, y_eval_Scotland))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ba588-fafb-47ec-80e9-96087ee65eaf",
   "metadata": {},
   "source": [
    "## Keras tunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2fe0b7-25be-4fd5-9c15-89542e2f6388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use only 20% of training data for parameter search\n",
    "ignore, use = train_test_split(trainingData, stratify=trainingData[target_outcomes[0]], test_size=0.2, random_state=1234)\n",
    "search_train, search_val = train_test_split(use, stratify=use[target_outcomes[0]], test_size=0.2, random_state=1234)\n",
    "search_train.reset_index(inplace=True, drop=True)\n",
    "search_val.reset_index(inplace=True, drop=True)\n",
    "X_train = search_train[features_columns]\n",
    "X_val = search_val[features_columns]\n",
    "y_train = search_train[target_outcomes[0]]\n",
    "y_val = search_val[target_outcomes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ccb36bc-2d09-49c7-aa10-86ead66ba6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68747, 67)\n",
      "(17187, 67)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae42f05-5764-4d4e-be1e-7b1ad0d6b105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3868488471391975\n",
      "7.387994143484627\n"
     ]
    }
   ],
   "source": [
    "print(search_train[target_outcomes[0]].value_counts()[0]/search_train[target_outcomes[0]].value_counts()[1])\n",
    "print(search_val[target_outcomes[0]].value_counts()[0]/search_val[target_outcomes[0]].value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e49d8-0d92-467a-92da-943eaa8433b1",
   "metadata": {},
   "source": [
    "### Dense parameters search [unit, activation, kerner_initializer, kernel_regularizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2e6b22-9c88-4aae-aea0-24255e6c9510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = 50\n",
    "hp = keras_tuner.HyperParameters()\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y_train)/sum(x == 1 for x in y_train)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "# print(class_weight)\n",
    "\n",
    "class MyHyperModel_denseLayerParametersSearch(keras_tuner.HyperModel):\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        neurons_layer1 = hp.Int(\"units_layer1\", min_value=32, max_value=128, step=16)\n",
    "        model.add(Dense(units=neurons_layer1, \n",
    "                        kernel_initializer=hp.Choice(\"kernel_initializer_layer1\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "                        kernel_regularizer=L1L2(l1=hp.Float(\"l1_dense1\", min_value=0.0, max_value=1, step=0.02), \n",
    "                                                l2=hp.Float(\"l2_dense1\", min_value=0.0, max_value=1, step=0.02)\n",
    "                                               ),\n",
    "                        input_shape = (X_train.shape[1],)\n",
    "                       )\n",
    "                 )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(hp.Choice(\"activation\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"])))\n",
    "        model.add(Dropout(rate=hp.Float(\"rate\", min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        opt = Adam()\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        ]\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    \n",
    "    # history = model.fit(X.values, y, validation_data=(X_val.values, y_val), epochs=200, batch_size=128, class_weight=class_weight,  callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc66333e-d438-45d6-9695-8846a8ddef17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 40s]\n",
      "val_auc: 0.6971777677536011\n",
      "\n",
      "Best val_auc So Far: 0.7427757978439331\n",
      "Total elapsed time: 00h 19m 50s\n",
      "--- Training time: 1196.2779953479767 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_denseLayerParametersSearch()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"denseParameters\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train.values, y_train, \n",
    "                 validation_data=(X_val.values, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "697aaef1-ee06-4337-a338-2f5ba22970e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ../SeqModel/tuner/denseParameters\n",
      "Showing 3 best trials\n",
      "Objective(name=\"val_auc\", direction=\"max\")\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 64\n",
      "kernel_initializer_layer1: glorot_uniform\n",
      "l1_dense1: 0.0\n",
      "l2_dense1: 0.98\n",
      "activation: elu\n",
      "rate: 0.30000000000000004\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 9\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0012\n",
      "Score: 0.7427757978439331\n",
      "\n",
      "Trial 0004 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 64\n",
      "kernel_initializer_layer1: glorot_uniform\n",
      "l1_dense1: 0.0\n",
      "l2_dense1: 0.98\n",
      "activation: elu\n",
      "rate: 0.30000000000000004\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.7320756912231445\n",
      "\n",
      "Trial 0012 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 64\n",
      "kernel_initializer_layer1: glorot_uniform\n",
      "l1_dense1: 0.0\n",
      "l2_dense1: 0.98\n",
      "activation: elu\n",
      "rate: 0.30000000000000004\n",
      "tuner/epochs: 9\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0004\n",
      "Score: 0.7208909392356873\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410da13-af43-47f4-96cf-c41f396f14e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95702225-489e-49e0-8b87-a53c71a85147",
   "metadata": {},
   "source": [
    "### number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df2bb51d-506e-476e-a016-4492b8037c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = 50\n",
    "hp = keras_tuner.HyperParameters()\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y_train)/sum(x == 1 for x in y_train)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "# print(class_weight)\n",
    "\n",
    "class MyHyperModel_numLayers(keras_tuner.HyperModel):\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0,l2=0.98),\n",
    "                        input_shape = (X_train.shape[1],)\n",
    "                       )\n",
    "                )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"elu\"))\n",
    "        model.add(Dropout(rate=0.3))\n",
    "        \n",
    "        layers = hp.Int(\"num_layers\", min_value=1, max_value=10, step=1)\n",
    "        for i in range(layers):\n",
    "            model.add(Dense(units=64, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0,l2=0.98),\n",
    "                       )\n",
    "                 )\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation(\"elu\"))\n",
    "            model.add(Dropout(rate=0.3))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        opt = Adam()\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        ]\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    \n",
    "    # history = model.fit(X.values, y, validation_data=(X_val.values, y_val), epochs=200, batch_size=128, class_weight=class_weight,  callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fba9c-476a-437a-8fce-f0532c68facd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 39s]\n",
      "val_auc: 0.6870602965354919\n",
      "\n",
      "Best val_auc So Far: 0.6870602965354919\n",
      "Total elapsed time: 00h 00m 39s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "10                |5                 |num_layers\n",
      "3                 |3                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                4352      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48833 (190.75 KB)\n",
      "Trainable params: 47425 (185.25 KB)\n",
      "Non-trainable params: 1408 (5.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "538/538 [==============================] - 22s 30ms/step - loss: 84.6723 - auc: 0.5667 - auprc: 0.1517 - val_loss: 0.8674 - val_auc: 0.5000 - val_auprc: 0.1192\n",
      "Epoch 2/3\n",
      "538/538 [==============================] - 16s 30ms/step - loss: 1.3859 - auc: 0.6279 - auprc: 0.1862 - val_loss: 0.8618 - val_auc: 0.4104 - val_auprc: 0.1074\n",
      "Epoch 3/3\n",
      "332/538 [=================>............] - ETA: 5s - loss: 1.3161 - auc: 0.6332 - auprc: 0.1827"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_numLayers()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"numLayers\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train.values, y_train, \n",
    "                 validation_data=(X_val.values, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1459a4-863a-4771-a59c-f939a017a573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae12a48-3966-4515-ba87-417297b97fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8165091-a69e-4252-9d73-5ee019001f58",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f79a24-21a8-4198-8fd4-5752297dbe37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = 50\n",
    "hp = keras_tuner.HyperParameters()\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y_train)/sum(x == 1 for x in y_train)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "# print(class_weight)\n",
    "\n",
    "class MyHyperModel_optimizer(keras_tuner.HyperModel):\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0,l2=0.98),\n",
    "                        input_shape = (X_train.shape[1],)\n",
    "                       )\n",
    "                )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"elu\"))\n",
    "        model.add(Dropout(rate=0.3))\n",
    "        \n",
    "        for i in range(2):\n",
    "            model.add(Dense(units=64, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0,l2=0.98),\n",
    "                        input_shape = (X_train.shape[1],)\n",
    "                       )\n",
    "                 )\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation(\"elu\"))\n",
    "            model.add(Dropout(rate=0.3))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        opt = hp.Choice(\"opt\", ['Adadelta', 'Adam', 'Adamax', 'RMSprop', 'SGD'])\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        ]\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    \n",
    "    # history = model.fit(X.values, y, validation_data=(X_val.values, y_val), epochs=200, batch_size=128, class_weight=class_weight,  callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6a3bc-9e13-4f61-8535-91cbf8e57baf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_optimizer()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"optimizer\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train.values, y_train, \n",
    "                 validation_data=(X_val.values, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a3ad3-3bcc-4972-b78e-4294229ff5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40111941-2f4f-4843-93db-f066f3fd45c1",
   "metadata": {},
   "source": [
    "### optimizer parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893ebfe-adad-415a-a7e7-c3ebfedc9022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = 50\n",
    "hp = keras_tuner.HyperParameters()\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y)/sum(x == 1 for x in y)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "# print(class_weight)\n",
    "\n",
    "class MyHyperModel_optimizerParameters(keras_tuner.HyperModel):\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, \n",
    "                        activation= \"selu\", \n",
    "                        kernel_initializer=\"lecun_normal\", \n",
    "                        kernel_regularizer=L1L2(l1=0,l2=0.2),\n",
    "                        input_shape = (X_train.shape[1],)\n",
    "                       )\n",
    "                )\n",
    "        model.add(Dropout(rate=0.3))\n",
    "        for i in range(2):\n",
    "            model.add(Dense(units=64, \n",
    "                        activation= \"selu\", \n",
    "                        kernel_initializer=\"lecun_normal\", \n",
    "                        kernel_regularizer=L1L2(l1=0,l2=0.2),\n",
    "                        input_shape = (X_train.shape[1],)\n",
    "                       )\n",
    "                 )\n",
    "            model.add(Dropout(rate=0.3))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        lr=hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]) \n",
    "        clipvalue=hp.Float(\"clipvalue\", min_value=0.1, max_value=0.7, step=0.2)\n",
    "        opt = Adam(learning_rate=lr, clipvalue=clipvalue)\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        ]\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    \n",
    "    # history = model.fit(X.values, y, validation_data=(X_val.values, y_val), epochs=200, batch_size=128, class_weight=class_weight,  callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0213298-273c-4a98-b108-74d8424b2a71",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_optimizerParameters()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"optimizerParameters\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train.values, y_train, \n",
    "                 validation_data=(X_val.values, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c983d7-e08d-410e-aa9c-86ebfe721182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d620419-27a1-427c-ada9-93af87ab488d",
   "metadata": {},
   "source": [
    "### search all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd77d3-5743-4ff7-86f6-85c027dc657a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = 50\n",
    "hp = keras_tuner.HyperParameters()\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y)/sum(x == 1 for x in y)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "# print(class_weight)\n",
    "\n",
    "class MyHyperModel_allParameters(keras_tuner.HyperModel):\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        neurons_layer1 = hp.Int(\"units_layer1\", min_value=32, max_value=128, step=32)\n",
    "        model.add(Dense(units=neurons_layer1, \n",
    "                        activation= hp.Choice(\"activation\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"]), \n",
    "                        kernel_initializer=hp.Choice(\"kernel_initializer_layer1\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "                        kernel_regularizer=L1L2(l1=hp.Float(\"l1_dense1\", min_value=0.0, max_value=0.3, step=0.02), \n",
    "                                                l2=hp.Float(\"l2_dense1\", min_value=0.0, max_value=0.3, step=0.02)\n",
    "                                               ),\n",
    "                        input_shape = (X_train.shape[1],)\n",
    "                       )\n",
    "                 )\n",
    "        model.add(Dropout(rate=hp.Float(\"rate\", min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        layers = hp.Int(\"num_layers\", min_value=0, max_value=10, step=1)\n",
    "        if layers > 0:\n",
    "            for i in range(layers):\n",
    "                neurons_layer1 = hp.Int(\"units_layer1\", min_value=32, max_value=128, step=32)\n",
    "                model.add(Dense(units=neurons_layer1, \n",
    "                                activation= hp.Choice(\"activation\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"]), \n",
    "                                kernel_initializer=hp.Choice(\"kernel_initializer_layer1\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "                                kernel_regularizer=L1L2(l1=hp.Float(\"l1_dense1\", min_value=0.0, max_value=0.3, step=0.02), \n",
    "                                                        l2=hp.Float(\"l2_dense1\", min_value=0.0, max_value=0.3, step=0.02)\n",
    "                                                       ),\n",
    "                               )\n",
    "                         )\n",
    "                model.add(Dropout(rate=hp.Float(\"rate\", min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        lr=hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]) \n",
    "        clipvalue=hp.Float(\"clipvalue\", min_value=0.1, max_value=0.7, step=0.2)\n",
    "        opt = Adam(learning_rate=lr, clipvalue=clipvalue)\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        ]\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    \n",
    "    # history = model.fit(X.values, y, validation_data=(X_val.values, y_val), epochs=200, batch_size=128, class_weight=class_weight,  callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013371a-0426-4ffd-902a-ee024da9c753",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_allParameters()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"allParameters\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train.values, y_train, \n",
    "                 validation_data=(X_val.values, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f463c4-ba84-4370-98eb-cb66f10aab70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e56e8-db46-4ea9-a108-0465bb38696b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "168e8d81-f09d-4cee-8302-11d387b308d1",
   "metadata": {},
   "source": [
    "## updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac979d6-fa6c-4549-89dc-c02cb937e8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = trainingData[features_columns]\n",
    "X_val = validationData[features_columns]\n",
    "# X = pd.concat([X, X_val])\n",
    "\n",
    "X_internaleval = internalEvaluationData[features_columns]\n",
    "X_eval = evaluationData[features_columns]\n",
    "X_eval_Wales = evaluationDataWales[features_columns]\n",
    "X_eval_Scotland = evaluationDataScotland[features_columns]\n",
    "\n",
    "print(X.shape)\n",
    "print(X_val.shape)\n",
    "print(X_eval.shape)\n",
    "print(X_eval_Wales.shape)\n",
    "print(X_eval_Scotland.shape)\n",
    "\n",
    "# target_outcomes = ['3months', '6months', '12months', '24months'] \n",
    "target_outcomes = ['12months'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e33cb-b043-41b5-8cd6-56f7d1d7fc41",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = 50\n",
    "y = trainingData[target_outcomes[0]].values\n",
    "y_val = validationData[target_outcomes[0]].values\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y)/sum(x == 1 for x in y)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "# print(class_weight)\n",
    "set\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, \n",
    "                    kernel_initializer=\"lecun_normal\", \n",
    "                    kernel_regularizer=L1L2(l1=0,l2=0.2),\n",
    "                    input_shape = (X_train.shape[1],)\n",
    "                   )\n",
    "            )\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    for i in range(1):\n",
    "        model.add(Dense(units=64, \n",
    "                    kernel_initializer=\"lecun_normal\", \n",
    "                    kernel_regularizer=L1L2(l1=0,l2=0.14),\n",
    "                   )\n",
    "             )\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('selu'))\n",
    "        model.add(Dropout(rate=0.3))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(learning_rate=5e-5, clipvalue=.1)\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=1000, name='auc'),\n",
    "        AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "    ]\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "    print(model.summary())\n",
    "    history = model.fit(X.values, y, validation_data=(X_val.values, y_val), epochs=200, batch_size=128, class_weight=class_weight,  callbacks = [earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494e910-7949-4780-9feb-47ad5187e0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auprc'])\n",
    "plt.plot(history.history['val_auprc'])\n",
    "plt.title('model auprc')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('auprc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16857ba-3770-42cd-93ec-b44012771561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_internaleval = internalEvaluationData[target_outcomes[0]]\n",
    "y_eval = evaluationData[target_outcomes[0]]\n",
    "y_eval_Wales = evaluationDataWales[target_outcomes[0]]\n",
    "y_eval_Scotland = evaluationDataScotland[target_outcomes[0]]\n",
    "print(model.evaluate(X_internaleval.values, y_internaleval))\n",
    "print(model.evaluate(X_eval.values, y_eval))\n",
    "print(model.evaluate(X_eval_Wales.values, y_eval_Wales))\n",
    "print(model.evaluate(X_eval_Scotland.values, y_eval_Scotland))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9897ff7-a1bc-4c5a-aec3-4deea9d8a289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaabe03-0654-40e4-846c-26c3b1ebb50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_dt = pickle.load(open('../Models_trainValEval/12months/DT0.sav', 'rb'))\n",
    "\n",
    "# pd.DataFrame([best_model3.feature_importances_], columns=X.columns).T.sort_values(0, ascending=False)\n",
    "sorted_idx = best_model_dt.feature_importances_.argsort()\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.barh(X.columns[sorted_idx][-10:], best_model_dt.feature_importances_[sorted_idx][-10:])\n",
    "plt.xlabel(\"Decision Tree Feature Importance\")\n",
    "plt.show()\n",
    "dttop10 = X.columns[sorted_idx][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c9b9a-76cf-45c8-a42d-ec9a120842cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_dt = pickle.load(open('../Models_trainValEval/12months/RF0.sav', 'rb'))\n",
    "\n",
    "# pd.DataFrame([best_model3.feature_importances_], columns=X.columns).T.sort_values(0, ascending=False)\n",
    "sorted_idx = best_model_dt.feature_importances_.argsort()\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.barh(X.columns[sorted_idx][-10:], best_model_dt.feature_importances_[sorted_idx][-10:])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")\n",
    "plt.show()\n",
    "rftop10 = X.columns[sorted_idx][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23258bb-f406-47ac-b970-2c68c6c9f392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_dt = pickle.load(open('../Models_trainValEval/12months/XGB0.sav', 'rb'))\n",
    "\n",
    "# pd.DataFrame([best_model3.feature_importances_], columns=X.columns).T.sort_values(0, ascending=False)\n",
    "sorted_idx = best_model_dt.feature_importances_.argsort()\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.barh(X.columns[sorted_idx][-10:], best_model_dt.feature_importances_[sorted_idx][-10:])\n",
    "plt.xlabel(\"XGBoost Feature Importance\")\n",
    "plt.show()\n",
    "xgbtop10 = X.columns[sorted_idx][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585071b8-244d-4f64-bd03-6cfcb038b749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_lasso = pickle.load(open('../Models_trainValEval/12months/LR0.sav', 'rb'))\n",
    "\n",
    "# pd.DataFrame([best_model3.feature_importances_], columns=X.columns).T.sort_values(0, ascending=False)\n",
    "sorted_idx = best_model_lasso.coef_[0].argsort()\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.barh(X.columns[sorted_idx][-10:], best_model_lasso.coef_[0][sorted_idx][-10:])\n",
    "plt.xlabel(\"LR Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7d254-18ba-4e6c-a70f-e232cef0d971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lassotop10 = X.columns[sorted_idx][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b58c1-a6cd-4616-b37e-8a838c4e694e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(set(dttop10).intersection(set(lassotop10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210cf71-819b-4110-a6da-a8476d3164f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prune_duplicate_leaves(best_model_dt)\n",
    "fig = plt.figure(figsize=(30,10))\n",
    "_ = tree.plot_tree(best_model_dt, \n",
    "                   feature_names=X.columns,  \n",
    "                   class_names=['no asthma attack','asthma attack'],\n",
    "                   filled=True,)\n",
    "plt.savefig('../FinalData/dt.png',format='png',bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cf88f-a1ea-4f04-82c1-e2ebcef6dea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData['BTS_step_2.0'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f8266-8e87-44d0-b10e-d3b1adb5163c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
