{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8032a7-260b-40a8-8cbe-e292ab403047",
   "metadata": {},
   "source": [
    "# combine tabular data with seq of clinical and therapy readcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1286de-3d50-4eb3-b3c7-a4404748f00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 14:46:08.138254: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-17 14:46:08.170723: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 14:46:08.170751: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 14:46:08.170768: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 14:46:08.176177: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Input, concatenate, Reshape, Activation, Flatten, Add, BatchNormalization, Multiply, LeakyReLU\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.metrics import AUC, SensitivityAtSpecificity\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, Adamax, SGD, Adadelta\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import L1L2, L1, L2\n",
    "from livelossplot import PlotLossesKeras\n",
    "#internal validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score, matthews_corrcoef, auc, average_precision_score, roc_auc_score, balanced_accuracy_score, roc_curve, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6e815-ba3e-471b-ba0e-efb907dfed14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5a964-b378-45ee-a3f1-30c4a35dda53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "# trainingData = pd.read_csv('../FinalData/trainingDataEncoded_08102023.csv')\n",
    "# validationData = pd.read_csv('../FinalData/validationDataEncoded_08102023.csv')\n",
    "# evaluationData = pd.read_csv('../FinalData/evaluationDataEncoded_08102023.csv')\n",
    "# evaluationDataWales = pd.read_csv('../FinalData/evaluationDataWalesEncoded_08102023.csv')\n",
    "# evaluationDataScotland = pd.read_csv('../FinalData/evaluationDataScotlandEncoded_08102023.csv')\n",
    "\n",
    "trainingData, validationData, internalEvaluationData, evaluationData, evaluationDataWales, evaluationDataScotland = pickle.load(open('../FinalData/dataset_scaled_2vs1_09122023.sav', 'rb'))\n",
    "\n",
    "trainingData = trainingData[(trainingData.age >=8) & (trainingData.age <=80)]\n",
    "validationData = validationData[(validationData.age >=8) & (validationData.age <=80)]\n",
    "internalEvaluationData = internalEvaluationData[(internalEvaluationData.age >=8) & (internalEvaluationData.age <=80)]\n",
    "evaluationData = evaluationData[(evaluationData.age >=8) & (evaluationData.age <=80)]\n",
    "evaluationDataWales = evaluationDataWales[(evaluationDataWales.age >=8) & (evaluationDataWales.age <=80)]\n",
    "evaluationDataScotland = evaluationDataScotland[(evaluationDataScotland.age >=8) & (evaluationDataScotland.age <=80)]\n",
    "\n",
    "\n",
    "trainingData = trainingData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "validationData = validationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "internalEvaluationData = internalEvaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationData = evaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataWales = evaluationDataWales.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataScotland = evaluationDataScotland.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "\n",
    "# trainingData = trainingData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# validationData = validationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# internalEvaluationData = internalEvaluationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationData = evaluationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationDataWales = evaluationDataWales.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationDataScotland = evaluationDataScotland.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e45cc-eb7f-4fee-89d8-a8081c0151d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define feature candidates\n",
    "\n",
    "features_columns = trainingData.columns.to_list()\n",
    "exclude_columns = ['patid', 'practice_id', #identifier\n",
    "                   'BMI', #use the categorical instead\n",
    "                   'ethnicity', #use ethnic_group instead\n",
    "                   'Spacer',  #all zero\n",
    "                   \n",
    "                   'outcome_3months', 'outcome_6months', 'outcome_9months', 'outcome_12months', 'outcome_15months', 'outcome_18months', \n",
    "                   'outcome_21months', 'outcome_24months', 'outcome_combined_6months', 'outcome_combined_9months', 'outcome_combined_12months', \n",
    "                   'outcome_combined_15months', 'outcome_combined_18months', 'outcome_combined_24months', '3months', '6months', '9months', '12months', '24months', #outcomes variable\n",
    "                   \n",
    "                   'postcode_district', 'County', 'LocalAuthority', 'OutputAreaClassification', #location related variables, use IMD decile only\n",
    "                   \n",
    "                   'cat_age', 'cat_average_daily_dose_ICS', 'cat_prescribed_daily_dose_ICS', 'cat_ICS_medication_possesion_ratio', 'cat_numOCS', 'cat_numOCSEvents', \n",
    "                   'cat_numOCSwithLRTI', 'cat_numAcuteRespEvents', 'cat_numAntibioticsEvents', 'cat_numAntibioticswithLRTI', 'cat_numAsthmaAttacks', 'cat_numHospEvents', \n",
    "                   'cat_numPCS', 'cat_numPCSAsthma', #use continous vars instead\n",
    "                   \n",
    "                   'count_rhinitis', 'count_cardiovascular', 'count_heartfailure',\n",
    "                   'count_psoriasis', 'count_anaphylaxis', 'count_diabetes', 'count_ihd',\n",
    "                   'count_anxiety', 'count_eczema', 'count_nasalpolyps',\n",
    "                   'count_paracetamol', 'count_nsaids', 'count_betablocker', #use binary ones\n",
    "                   \n",
    "                   'paracetamol', 'nsaids', 'betablocker', #no data in evaluation\n",
    "                   \n",
    "                   'numOCSEvents', #duplicate with numOCS\n",
    "                   \n",
    "                   'month_12', 'month_4', 'month_5', 'month_10', 'month_1', 'month_6', 'month_3', \n",
    "                   'month_11', 'month_8', 'month_9', 'month_7', 'month_2', #month of attacks\n",
    "                   \n",
    "                   # 'system_EMIS', 'system_SystemOne', 'system_Vision', #primary care system used\n",
    "                  ]\n",
    "exclude_columns = exclude_columns + [x for x in features_columns if '_count' in x] #filter out commorbid count variables\n",
    "features_columns = [x for x in features_columns if x not in exclude_columns]\n",
    "print('Features size: ', len(features_columns))\n",
    "print(features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1e4f0-1280-448d-8df8-55a2dd6648d9",
   "metadata": {},
   "source": [
    "# load sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cdd2d-5347-47e4-8435-5d80018fdba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clinical = pd.read_feather('../SeqModel/all_data_clinical.feather')\n",
    "therapy = pd.read_feather('../SeqModel/all_data_therapy.feather')\n",
    "seqCols = ['patid',\n",
    "       'read_code_seq_padded_end_idx_clin',\n",
    "       'month_padded_idx_end_clin',\n",
    "       'read_code_seq_padded_end_idx_ther',\n",
    "       'month_padded_idx_end_ther']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f21cb5-e88b-4f4e-b2ea-72ef9034ca48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_data = clinical.merge(therapy[['patid', 'read_code_seq_padded_idx', 'read_code_seq_padded_end_idx',\n",
    "       'month_padded_idx', 'month_padded_idx_end']], on='patid', suffixes=['_clin', '_ther'], how='inner')\n",
    "\n",
    "trainingData = trainingData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "validationData = validationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "internalEvaluationData = internalEvaluationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationData = evaluationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationDataWales = evaluationDataWales.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationDataScotland = evaluationDataScotland.merge(sequence_data[seqCols], on='patid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6b74c-ad65-4e0b-bc67-d8e9d3fd02a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trainingData.shape)\n",
    "print(validationData.shape)\n",
    "print(internalEvaluationData.shape)\n",
    "print(evaluationData.shape)\n",
    "print(evaluationDataWales.shape)\n",
    "print(evaluationDataScotland.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b20f0-4a38-45fe-a059-8d4b8aca7988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xt_train = np.array(trainingData[features_columns].values)\n",
    "Xt_val = np.array(validationData[features_columns].values)\n",
    "Xt_internaleval = np.array(internalEvaluationData[features_columns].values)\n",
    "Xt_eval = np.array(evaluationData[features_columns].values)\n",
    "Xt_eval_Wales = np.array(evaluationDataWales[features_columns].values)\n",
    "Xt_eval_Scotland = np.array(evaluationDataScotland[features_columns].values)\n",
    "\n",
    "#scalling tabular data\n",
    "scaler = StandardScaler().fit(Xt_train)\n",
    "Xt_train = scaler.transform(Xt_train)\n",
    "Xt_val = scaler.transform(Xt_val)\n",
    "Xt_internaleval = scaler.transform(Xt_internaleval)\n",
    "Xt_eval = scaler.transform(Xt_eval)\n",
    "Xt_eval_Wales = scaler.transform(Xt_eval_Wales)\n",
    "Xt_eval_Scotland = scaler.transform(Xt_eval_Scotland)\n",
    "\n",
    "Xclin_train = np.array(trainingData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_val = np.array(validationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_internaleval = np.array(internalEvaluationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval = np.array(evaluationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval_Wales = np.array(evaluationDataWales['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval_Scotland = np.array(evaluationDataScotland['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_train = np.array([x for x in Xclin_train])\n",
    "Xclin_val = np.array([x for x in Xclin_val])\n",
    "Xclin_internaleval = np.array([x for x in Xclin_internaleval])\n",
    "Xclin_eval = np.array([x for x in Xclin_eval])\n",
    "Xclin_eval_Wales = np.array([x for x in Xclin_eval_Wales])\n",
    "Xclin_eval_Scotland = np.array([x for x in Xclin_eval_Scotland])\n",
    "\n",
    "Xther_train = np.array(trainingData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_val = np.array(validationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_internaleval = np.array(internalEvaluationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval = np.array(evaluationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval_Wales = np.array(evaluationDataWales['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval_Scotland = np.array(evaluationDataScotland['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_train = np.array([x for x in Xther_train])\n",
    "Xther_val = np.array([x for x in Xther_val])\n",
    "Xther_internaleval = np.array([x for x in Xther_internaleval])\n",
    "Xther_eval = np.array([x for x in Xther_eval])\n",
    "Xther_eval_Wales = np.array([x for x in Xther_eval_Wales])\n",
    "Xther_eval_Scotland = np.array([x for x in Xther_eval_Scotland])\n",
    "\n",
    "\n",
    "print(Xt_train.shape)\n",
    "print(Xt_internaleval.shape)\n",
    "print(Xt_val.shape)\n",
    "print(Xt_eval.shape)\n",
    "print(Xt_eval_Wales.shape)\n",
    "print(Xt_eval_Scotland.shape)\n",
    "\n",
    "# target_outcomes = ['3months', '6months', '12months', '24months'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e26e6d-1fba-430e-a954-9676cc730947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vocab\n",
    "code2idx_clin = pickle.load(open('../SeqModel/all_vocab_clinical.sav', 'rb'))\n",
    "code2idx_ther = pickle.load(open('../SeqModel/all_vocab_therapy.sav', 'rb'))\n",
    "month2idx = pickle.load(open('../SeqModel/all_vocab_month.sav', 'rb'))\n",
    "vocab_size_clinical = len(code2idx_clin)+1\n",
    "vocab_size_therapy = len(code2idx_ther)+1\n",
    "month_size = len(month2idx)+1\n",
    "print(vocab_size_clinical)\n",
    "print(vocab_size_therapy)\n",
    "print(month_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95667606-62df-44a4-a3db-7a440d36947f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_outcome = '12months'\n",
    "max_codes_clin = Xclin_train.shape[1]\n",
    "max_codes_ther = Xther_train.shape[1]\n",
    "max_codes = 100\n",
    "tab_feature_size = Xt_train.shape[1]\n",
    "\n",
    "y_train = trainingData[target_outcome].values\n",
    "y_val = validationData[target_outcome].values\n",
    "y_internaleval = internalEvaluationData[target_outcome].values\n",
    "y_eval = evaluationData[target_outcome].values\n",
    "y_eval_Wales = evaluationDataWales[target_outcome].values\n",
    "y_eval_Scotland = evaluationDataScotland[target_outcome].values\n",
    "\n",
    "pos_weight = sum(x == 0 for x in y_train)/sum(x == 1 for x in y_train)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320eb9f8-d645-4f08-b995-9424c02e0423",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8c208-941b-4b2c-8170-c0c3e993028e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_codes_ther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45f82c-62f0-4eab-8a38-1f6519f8d2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hierarchical():\n",
    "    #tabular dara - demography   \n",
    "    inputs1 = Input(shape=tab_feature_size)\n",
    "    nn = Dense(32, kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.04))(inputs1)\n",
    "    nn = BatchNormalization()(nn)\n",
    "    nn = Activation('relu')(nn)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    \n",
    "    #clinical embedding for lstm\n",
    "    inputs2 = Input(shape=max_codes_clin)\n",
    "    embedding_clin = Embedding(vocab_size_clinical, int(np.sqrt(vocab_size_clinical)), input_length=max_codes_clin)(inputs2)\n",
    "    # lstmClinical = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.04)))(embedding_clin)\n",
    "        \n",
    "     \n",
    "    #therapy embedding for lstm\n",
    "    inputs3 = Input(shape=max_codes_clin)\n",
    "    embedding_ther = Embedding(vocab_size_therapy, int(np.sqrt(vocab_size_clinical)), input_length=max_codes_clin)(inputs3)\n",
    "    # lstmTherapy = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.04)))(embedding_ther)\n",
    "    \n",
    "    ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "    add = Add()([embedding_clin, embedding_ther])\n",
    "    \n",
    "    ###layer 3 - LSTM to the final product\n",
    "    lstm = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.04)))(add)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    #merge tabular and sequence layers\n",
    "    nn = Reshape((1, 32))(nn)\n",
    "    add = concatenate([nn, lstm], axis=1)\n",
    "    \n",
    "    ###layer 4 - FCN before classification layer\n",
    "    final = Dense(units=8, kernel_regularizer=L1L2(l1=0.0, l2=0.04))(add)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.5)(final)\n",
    "    final = Flatten()(final)\n",
    "    final = Dropout(0.5)(final)\n",
    "    \n",
    "    ###layer 5 - classification layer\n",
    "    output = Dense(1, activation='sigmoid')(final)\n",
    "    \n",
    "    opt = Adamax(learning_rate=1e-3, clipvalue=.5, weight_decay=0.01)\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "        AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        tf.keras.metrics.Precision(name='prec'),\n",
    "        tf.keras.metrics.Recall(name='rec'),\n",
    "        tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "        tf.keras.metrics.TruePositives(name='TP'),\n",
    "        tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "    ]\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer=opt, \n",
    "        metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1375ac8-eeac-420d-bb0b-3c62b2494cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# sklearn_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weight = dict(enumerate(sklearn_weights))\n",
    "\n",
    "#Hyperparameter\n",
    "lr = 1e-5\n",
    "clipvalue = 0.2\n",
    "epoch = 1000\n",
    "batch_size = 256\n",
    "embedding_vector_length = 50\n",
    "month_embedding_vector_length = 5\n",
    "# embedding_vector_length = int(np.sqrt(vocab_size))\n",
    "# embedding_vector_length = int(np.cbrt(vocab_size))\n",
    "print(embedding_vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0d784-620b-44e3-8aeb-c3d39b8dce0a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#visualise model\n",
    "model = hierarchical()\n",
    "# model = earlyFussion()\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcd4e8-9bc0-479d-b5ac-ec2488e953ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "#training\n",
    "with tf.device('/GPU:0'):\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=50, verbose=0, mode='max', restore_best_weights=True)\n",
    "    mcp_save = ModelCheckpoint('../SeqModel/seqModel_therapy_tabSeq.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "    history = model.fit([Xt_train, Xclin_train[:,:max_codes_clin], Xther_train[:,:max_codes_clin]], y_train, validation_data=([Xt_val, Xclin_val[:,:max_codes_clin], Xther_val[:,:max_codes_clin]], y_val), \n",
    "                            epochs=epoch, batch_size=128, \n",
    "                        class_weight = class_weight, \n",
    "                        callbacks = [earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a131e0-0c7e-463a-ac70-7aeae48915e0",
   "metadata": {},
   "source": [
    "# tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03baf622-f51d-4ffe-b399-661929a754e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use only 20% of training data for parameter search\n",
    "target_outcomes = ['12months']\n",
    "ignore, use = train_test_split(trainingData, stratify=trainingData[target_outcomes[0]], test_size=0.2, random_state=1234)\n",
    "search_train, search_val = train_test_split(use, stratify=use[target_outcomes[0]], test_size=0.2, random_state=1234)\n",
    "search_train.reset_index(inplace=True, drop=True)\n",
    "search_val.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "Xt_train_search = np.array(search_train[features_columns].values)\n",
    "Xt_val_search = np.array(search_val[features_columns].values)\n",
    "scaler = StandardScaler().fit(Xt_train)\n",
    "Xt_train_search = scaler.transform(Xt_train_search)\n",
    "Xt_val_search = scaler.transform(Xt_val_search)\n",
    "Xclin_train_search = np.array(search_train['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_val_search = np.array(search_val['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xther_train_search = np.array(search_train['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_val_search = np.array(search_val['read_code_seq_padded_end_idx_ther'].values)\n",
    "\n",
    "Xclin_train_search = np.array([x for x in Xclin_train_search])\n",
    "Xclin_val_search = np.array([x for x in Xclin_val_search])\n",
    "Xther_train_search = np.array([x for x in Xther_train_search])\n",
    "Xther_val_search = np.array([x for x in Xther_val_search])\n",
    "\n",
    "y_train_search = search_train[target_outcome].values\n",
    "y_val_search = search_val[target_outcome].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f9e1d-46dd-4f15-80f9-63101a8dd71b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Xt_train_search.shape)\n",
    "print(Xt_val_search.shape)\n",
    "\n",
    "print(search_train[target_outcomes[0]].value_counts()[0]/search_train[target_outcomes[0]].value_counts()[1])\n",
    "print(search_val[target_outcomes[0]].value_counts()[0]/search_val[target_outcomes[0]].value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eb1ad-698a-4da5-b7c5-a45ae1ba716c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## layer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075cd9b-e406-4975-b2f8-90e3a956693e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "class MyHyperModel_allParams(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #tabular dara - demography   \n",
    "        inputs1 = Input(shape=tab_feature_size)\n",
    "        neuron_units = hp.Int(\"neuron_units\", min_value=32, max_value=128, step=32)\n",
    "        nn = Dense(units=neuron_units, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0.04,l2=0.08),\n",
    "                        bias_regularizer=L1L2(l1=0.08,l2=0),\n",
    "                        activity_regularizer=L1L2(l1=0,l2=0),\n",
    "                        input_shape = (Xt_train_search.shape[1],)\n",
    "                       )(inputs1)\n",
    "        nn = BatchNormalization()(nn)\n",
    "        nn = Activation(hp.Choice(\"activation\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"]))(nn)\n",
    "        nn = Dropout(0.4)(nn)\n",
    "\n",
    "\n",
    "        #clinical embedding for lstm\n",
    "        inputs2 = Input(shape=max_codes_clin)\n",
    "        embedding_size = hp.Int(\"embedding_clinical\", min_value=int(np.cbrt(vocab_size_clinical)), max_value=int(np.sqrt(vocab_size_therapy)))\n",
    "        embedding_clin = Embedding(vocab_size_clinical, \n",
    "                                   output_dim = embedding_size, \n",
    "                                   input_length=max_codes_clin,\n",
    "                                   mask_zero=True,\n",
    "                                  )(inputs2)\n",
    "        \n",
    "\n",
    "\n",
    "        #therapy embedding for lstm\n",
    "        inputs3 = Input(shape=max_codes_clin)\n",
    "        embedding_ther = Embedding(vocab_size_therapy, \n",
    "                                   output_dim = embedding_size, \n",
    "                                   input_length=max_codes_clin,\n",
    "                                   mask_zero=True,\n",
    "                             )(inputs3)\n",
    "        \n",
    "\n",
    "        ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "        add = Multiply()([embedding_clin, embedding_ther])\n",
    "\n",
    "        ###layer 2 - LSTM to the final product\n",
    "        lstm = Bidirectional(LSTM(units=int(neuron_units/2), \n",
    "                            # return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"kernel_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"bias_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=hp.Float(\"act_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"act_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=hp.Float(\"rec_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"rec_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(add)\n",
    "        lstm = Dropout(rate=hp.Float(\"rate_lstm1\", min_value=0.1, max_value=0.5))(lstm)\n",
    "\n",
    "        #merge tabular and sequence layers\n",
    "        # nn = Reshape((1, neuron_units))(nn)\n",
    "        add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "        ##layer 4 - FCN before classification layer\n",
    "        neurons_final_layer = hp.Int(\"units_layer1\", min_value=32, max_value=128, step=32)\n",
    "        final = Dense(units=neurons_final_layer, \n",
    "                        kernel_initializer=hp.Choice(\"kernel_initializer_layer1\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "                        kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"kernel_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                        bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"bias_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                        activity_regularizer=L1L2(l1=hp.Float(\"act_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"act_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                     )(add)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation(hp.Choice(\"activation\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"]))(final)\n",
    "        final = Dropout(0.4)(final)     \n",
    "        \n",
    "        # for i in range(3):\n",
    "        #     final = Dense(units=neurons_final_layer, \n",
    "        #                     kernel_initializer=hp.Choice(\"kernel_initializer_layer1\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "        #                     kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "        #                                             l2=hp.Float(\"kernel_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "        #                                            ),\n",
    "        #                     bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "        #                                             l2=hp.Float(\"bias_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "        #                                            ),\n",
    "        #                     activity_regularizer=L1L2(l1=hp.Float(\"act_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "        #                                             l2=hp.Float(\"act_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "        #                                            ),\n",
    "        #                  )(final)\n",
    "        #     final = BatchNormalization()(final)\n",
    "        #     final = Activation(hp.Choice(\"activation\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"]))(final)\n",
    "        #     final = Dropout(0.4)(final)                                                        \n",
    "\n",
    "\n",
    "        ###layer 5 - classification layer\n",
    "        # final = Flatten()(add)\n",
    "        output = Dense(1, activation='sigmoid')(final)\n",
    "        \n",
    "        lr=hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]) \n",
    "        clipvalue=hp.Float(\"clipvalue\", min_value=0.1, max_value=0.7, step=0.2)\n",
    "        beta_1=hp.Float(\"beta_1\", min_value=0.8, max_value=0.95, step=0.05)\n",
    "        beta_2=hp.Float(\"beta_2\", min_value=0.900, max_value=0.999, step=0.011)\n",
    "        epsilon=hp.Choice(\"epsilon\", [1e-6, 1e-7, 1e-8]) \n",
    "        weight_decay=hp.Float(\"weight_decay\", min_value=0, max_value=0.1) \n",
    "\n",
    "        opt = Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "        # opt = RMSprop(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "            # tf.keras.metrics.Precision(name='prec'),\n",
    "            # tf.keras.metrics.Recall(name='rec'),\n",
    "            # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "            # tf.keras.metrics.TruePositives(name='TP'),\n",
    "            # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "        ]\n",
    "\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=opt, \n",
    "            metrics=metrics)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6467b6-978b-45d6-819b-e2b10dfa3967",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='max')\n",
    "pos_weight = sum(x == 0 for x in y_train_search)/sum(x == 1 for x in y_train_search)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_allParams()\n",
    "    # tuner = keras_tuner.Hyperband(\n",
    "    #     hypermodel= model,\n",
    "    #     objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    #     max_epochs=25,\n",
    "    #     hyperband_iterations=1,\n",
    "    #     overwrite=True,\n",
    "    #     directory='../SeqModel/tuner/',\n",
    "    #     project_name=\"lstmv2.0AllParams1\",\n",
    "    # )\n",
    "    tuner = keras_tuner.BayesianOptimization(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_trials=20,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"lstmv2.0AllParams1-bayesian\",\n",
    "    )\n",
    "    \n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search([Xt_train_search, Xclin_train_search, Xther_train_search[:,:max_codes_clin]], y_train_search, \n",
    "                 validation_data=([Xt_val_search, Xclin_val_search, Xther_val_search[:,:max_codes_clin]], y_val_search),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc36a7e-d709-4c72-ac7a-f98dfd75efed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MyHyperModel_allParams()\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel= model,\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=20,\n",
    "    overwrite=False,\n",
    "    directory='../SeqModel/tuner/',\n",
    "    project_name=\"lstmv2.0AllParams1-bayesian\",\n",
    ")\n",
    "best_models = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b6a73-6a6b-4082-9fc8-76f7a6d95ec7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lr= 1e-4\n",
    "# clipvalue= 0.5\n",
    "# beta_1= 0.9\n",
    "# beta_2= 0.9\n",
    "# epsilon= 1e-06\n",
    "# weight_decay= 0.06\n",
    "# # opt = Adamax(learning_rate=lr, clipvalue=clipvalue, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "# opt = Adadelta(learning_rate=lr, clipvalue=clipvalue, epsilon=epsilon, weight_decay=weight_decay)\n",
    "# metrics = [\n",
    "#     AUC(num_thresholds=1000, name='auc'),\n",
    "#     AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "# ]\n",
    "# best_models.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics )\n",
    "print(best_models.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_seq+tab.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='max')\n",
    "history = best_models.fit([Xt_train, Xclin_train, Xther_train[:,:max_codes_clin]], y_train, \n",
    "                 validation_data=([Xt_val, Xclin_val, Xther_val[:,:max_codes_clin]], y_val),\n",
    "                          epochs=200, batch_size=128, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4954e-27f9-4913-b35e-e255bf8e6628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auprc'])\n",
    "plt.plot(history.history['val_auprc'])\n",
    "plt.title('model auprc')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('auprc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1033982-4b4a-4d06-a31d-cca2d4a311ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summariseResultWithThreshold (Xt_test, Xs_clin_test, Xs_ther_test, testY, model):\n",
    "    preds = model.predict([Xt_test, Xs_clin_test, Xs_ther_test])\n",
    "    # tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    # specificity = tn / (tn+fp)\n",
    "    # sensitivity = tp / (tp+fn)\n",
    "    # ppv = 100*tp/(tp+fp)\n",
    "    # npv = 100*tn/(fn+tn)\n",
    "    # acc = accuracy_score(testY, preds)\n",
    "    # f1score = f1_score(testY, preds, average = 'binary')\n",
    "    # balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    # aucscore = auc(fpr, tpr)\n",
    "    aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(aucscore,4), np.round(auprc,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711b45b-2abd-4bdf-a966-c766d80828ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(summariseResultWithThreshold(Xt_eval, Xclin_eval[:,:max_codes_clin], Xther_eval[:,:max_codes_clin], y_eval, best_models))\n",
    "print(summariseResultWithThreshold(Xt_eval, Xclin_eval[:,:max_codes_clin], Xther_eval[:,:max_codes_clin], y_eval, best_models))\n",
    "print(summariseResultWithThreshold(Xt_testWales, Xclin_testWales[:,:max_codes_clin], Xther_testWales[:,:max_codes_clin], y_testWales, best_models))\n",
    "print(summariseResultWithThreshold(Xt_testScotland, Xclin_testScotland[:,:max_codes_clin], Xther_testScotland[:,:max_codes_clin], y_testScotland, best_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868746fb-2b3b-406c-a143-73baded1f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary_result_val = []\n",
    "summary_result_eval = []\n",
    "summary_result_Wales = []\n",
    "summary_result_Scotland = []\n",
    "cols = ['auc', 'auprc']\n",
    "\n",
    "y_internaleval = internalEvaluationData[target_outcomes[0]]\n",
    "y_eval = evaluationData[target_outcomes[0]]\n",
    "y_eval_Wales = evaluationDataWales[target_outcomes[0]]\n",
    "y_eval_Scotland = evaluationDataScotland[target_outcomes[0]]\n",
    "summary_result_eval.append(summariseResult (X_eval, y_eval, best_models) )\n",
    "summary_result_Wales.append(summariseResult (X_eval_Wales, y_eval_Wales, best_models) )       \n",
    "summary_result_Scotland.append(summariseResult (X_eval_Scotland, y_eval_Scotland, best_models) )       \n",
    "summary_result_val.append(summariseResult (X_internaleval, y_internaleval, best_models) )   \n",
    "\n",
    "    \n",
    "summary_result_eval = pd.DataFrame(summary_result_eval, columns=cols)\n",
    "summary_result_eval['model_num'] = summary_result_eval.index\n",
    "\n",
    "summary_result_Wales = pd.DataFrame(summary_result_Wales, columns=cols)\n",
    "summary_result_Wales['model_num'] = summary_result_Wales.index\n",
    "\n",
    "summary_result_Scotland = pd.DataFrame(summary_result_Scotland, columns=cols)\n",
    "summary_result_Scotland['model_num'] = summary_result_Scotland.index\n",
    "\n",
    "summary_result_internaleval = pd.DataFrame(summary_result_val, columns=cols)\n",
    "summary_result_internaleval['model_num'] = summary_result_val.index\n",
    "summary_result_internaleval['set'] = 'England'\n",
    "summary_result_eval['set'] = 'Wales & Scotland'\n",
    "summary_result_Wales['set'] = 'Wales'\n",
    "summary_result_Scotland['set'] = 'Scotland'\n",
    "\n",
    "combine = pd.concat([summary_result_internaleval, summary_result_eval, \n",
    "                     summary_result_Wales, summary_result_Scotland,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5b9aa-bd45-4101-977e-5c0f95c20c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    best_models.evaluate([Xt_eval, Xs_eval[:,:max_codes], Xm_eval[:,:max_codes]], y_eval)\n",
    "    best_models.evaluate([Xt_test, Xs_test[:,:max_codes], Xm_test[:,:max_codes]], y_test)\n",
    "    # model.evaluate(X_testWales, y_testWales)\n",
    "    # model.evaluate(X_testScotland, y_testScotland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de1947-ebe4-4025-a4cf-a003e089009e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda88d96-b8f0-4619-b881-31b8e531b2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5d616-6b94-49f0-ab11-a7a11c0b8c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087257e-17b4-4e0a-9ebe-41d1c4ef7174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0f5f2-dcbb-4e5a-a247-df856468d3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b98c2a-38d9-41d8-b997-d39931f18e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xclin_train_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559f565-f741-425d-982a-15985ed7edf6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61ebbd-4052-44ad-8af0-7c68f753746f",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34484c-c2a6-4e9b-9992-5be57241ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "class MyHyperModel_optimizer(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #tabular dara - demography   \n",
    "        inputs1 = Input(shape=tab_feature_size)\n",
    "                nn = Dense(units=128, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0.04,l2=0.08),\n",
    "                        bias_regularizer=L1L2(l1=0.08,l2=0),\n",
    "                        activity_regularizer=L1L2(l1=0,l2=0),\n",
    "                        input_shape = (Xt_train_search.shape[1],)\n",
    "                       )(inputs1)\n",
    "        nn = BatchNormalization()(nn)\n",
    "        nn = Activation(\"elu\")(nn)\n",
    "        nn = Dropout(0.4)(nn)\n",
    "\n",
    "\n",
    "        #clinical embedding for lstm\n",
    "        inputs2 = Input(shape=max_codes_clin)\n",
    "        embedding_clin = Embedding(vocab_size_clinical, \n",
    "                                   output_dim = 144, \n",
    "                                   input_length=max_codes_clin\n",
    "                                  )(inputs2)\n",
    "        lstmClinical = Bidirectional(LSTM(units=lstm_units0, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.04\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.08\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0, \n",
    "                                           l2=.06\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0, \n",
    "                                           l2=.04\n",
    "                                                      ),\n",
    "                                         )\n",
    "                                    )(embedding_clin)\n",
    "\n",
    "\n",
    "        #therapy embedding for lstm\n",
    "        inputs3 = Input(shape=max_codes_ther)\n",
    "        embedding_ther = Embedding(vocab_size_therapy, \n",
    "                              output_dim = 62, \n",
    "                              input_length=max_codes_ther\n",
    "                             )(inputs3)\n",
    "        lstmTherapy = Bidirectional(LSTM(units=lstm_units0, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(embedding_ther)\n",
    "\n",
    "        ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "        add = concatenate([lstmClinical, lstmTherapy], axis=1)\n",
    "\n",
    "        ###layer 2 - LSTM to the final product\n",
    "        lstm = Bidirectional(LSTM(units=64, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(add)\n",
    "        lstm = Dropout(rate=0.3403698716775575)(lstm)\n",
    "\n",
    "        #merge tabular and sequence layers\n",
    "        nn = Reshape((1, 128))(nn)\n",
    "        add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "        ###layer 4 - FCN before classification layer\n",
    "        neurons_final_layer = 128\n",
    "        final = Dense(units=neurons_final_layer, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0, \n",
    "                                                l2=.04\n",
    "                                               ),\n",
    "                        bias_regularizer=L1L2(l1=.04, \n",
    "                                                l2=.02\n",
    "                                               ),\n",
    "                        activity_regularizer=L1L2(l1=.06, \n",
    "                                                l2=.1\n",
    "                                               ),\n",
    "                     )(add)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation(\"elu\")(final)\n",
    "        final = Dropout(0.4)(final)                                                        \n",
    "\n",
    "\n",
    "        ###layer 5 - classification layer\n",
    "        final = Flatten()(final)\n",
    "        output = Dense(1, activation='sigmoid')(final)\n",
    "\n",
    "        opt = hp.Choice(\"opt\", ['Adadelta', 'Adam', 'Adamax', 'RMSprop', 'SGD'])\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "            # tf.keras.metrics.Precision(name='prec'),\n",
    "            # tf.keras.metrics.Recall(name='rec'),\n",
    "            # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "            # tf.keras.metrics.TruePositives(name='TP'),\n",
    "            # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "        ]\n",
    "\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=opt, \n",
    "            metrics=metrics)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6016b40-f998-445b-b383-062fcec177d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_optimizer()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"optimizerLSTM\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search([Xt_train_search, Xclin_train_search, Xther_train_search], y_train_search, \n",
    "                 validation_data=([Xt_val_search, Xclin_val_search, Xther_val_search], y_val_search),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a8ac8-1d0a-4767-81d5-18e7418f0e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load tuner, cimment it if not needed\n",
    "model = MyHyperModel_optimizer()\n",
    "tuner = keras_tuner.Hyperband(\n",
    "    hypermodel= model,\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_epochs=25,\n",
    "    hyperband_iterations=1,\n",
    "    overwrite=False, #make sure it is set to False\n",
    "    directory='../SeqModel/tuner/',\n",
    "    project_name=\"optimizerLSTM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec90876-01ef-48ba-ab57-e5663fb4e8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a9e53-433b-4fed-a116-7c7bc1e05c9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## opt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4346e3-cedf-49bd-b14f-ebaff2ff4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "class MyHyperModel_optParameter(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #tabular dara - demography   \n",
    "        inputs1 = Input(shape=tab_feature_size)\n",
    "        \n",
    "        nn = Dense(units=128, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0.04,l2=0.08),\n",
    "                        bias_regularizer=L1L2(l1=0.08,l2=0),\n",
    "                        activity_regularizer=L1L2(l1=0,l2=0),\n",
    "                        input_shape = (Xt_train_search.shape[1],)\n",
    "                       )(inputs1)\n",
    "        nn = BatchNormalization()(nn)\n",
    "        nn = Activation(\"elu\")(nn)\n",
    "        nn = Dropout(0.4)(nn)\n",
    "\n",
    "\n",
    "        #clinical embedding for lstm\n",
    "        inputs2 = Input(shape=max_codes_clin)\n",
    "        embedding_clin = Embedding(vocab_size_clinical, \n",
    "                                   output_dim = 144, \n",
    "                                   input_length=max_codes_clin\n",
    "                                  )(inputs2)\n",
    "        lstmClinical = Bidirectional(LSTM(units=48, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.04\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.08\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0, \n",
    "                                           l2=.06\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0, \n",
    "                                           l2=.04\n",
    "                                                      ),\n",
    "                                         )\n",
    "                                    )(embedding_clin)\n",
    "\n",
    "\n",
    "        #therapy embedding for lstm\n",
    "        inputs3 = Input(shape=max_codes_ther)\n",
    "        embedding_ther = Embedding(vocab_size_therapy, \n",
    "                              output_dim = 62, \n",
    "                              input_length=max_codes_ther\n",
    "                             )(inputs3)\n",
    "        lstmTherapy = Bidirectional(LSTM(units=48, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(embedding_ther)\n",
    "\n",
    "        ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "        add = concatenate([lstmClinical, lstmTherapy], axis=1)\n",
    "\n",
    "        ###layer 2 - LSTM to the final product\n",
    "        lstm = Bidirectional(LSTM(units=64, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(add)\n",
    "        lstm = Dropout(rate=0.3403698716775575)(lstm)\n",
    "\n",
    "        #merge tabular and sequence layers\n",
    "        nn = Reshape((1, 128))(nn)\n",
    "        add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "        ###layer 4 - FCN before classification layer\n",
    "        neurons_final_layer = 128\n",
    "        final = Dense(units=neurons_final_layer, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0, \n",
    "                                                l2=.04\n",
    "                                               ),\n",
    "                        bias_regularizer=L1L2(l1=.04, \n",
    "                                                l2=.02\n",
    "                                               ),\n",
    "                        activity_regularizer=L1L2(l1=.06, \n",
    "                                                l2=.1\n",
    "                                               ),\n",
    "                     )(add)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation(\"elu\")(final)\n",
    "        final = Dropout(0.4)(final)                                                        \n",
    "\n",
    "\n",
    "        ###layer 5 - classification layer\n",
    "        final = Flatten()(final)\n",
    "        output = Dense(1, activation='sigmoid')(final)\n",
    "        \n",
    "        lr=hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]) \n",
    "        clipvalue=hp.Float(\"clipvalue\", min_value=0.1, max_value=0.7, step=0.2)\n",
    "        beta_1=hp.Float(\"beta_1\", min_value=0.8, max_value=0.95, step=0.05)\n",
    "        beta_2=hp.Float(\"beta_2\", min_value=0.900, max_value=0.999, step=0.011)\n",
    "        epsilon=hp.Choice(\"epsilon\", [1e-6, 1e-7, 1e-8]) \n",
    "        weight_decay=hp.Float(\"weight_decay\", min_value=0, max_value=0.1) \n",
    "\n",
    "        opt = Adamax(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "            # tf.keras.metrics.Precision(name='prec'),\n",
    "            # tf.keras.metrics.Recall(name='rec'),\n",
    "            # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "            # tf.keras.metrics.TruePositives(name='TP'),\n",
    "            # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "        ]\n",
    "\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=opt, \n",
    "            metrics=metrics)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d057b-e162-4fb8-8315-80ad735ea880",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_optParameter()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"optimizerParameterLSTM\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search([Xt_train_search, Xclin_train_search, Xther_train_search], y_train_search, \n",
    "                 validation_data=([Xt_val_search, Xclin_val_search, Xther_val_search], y_val_search),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33da05-39e7-4cbc-9797-93f0cef43566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97c0ae-ed61-4f64-898c-263923b55e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae28316-d9e8-4680-a5c6-e8522b5e8ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12cf3d-74a5-4f75-8493-605a48ba742e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbddf77-c941-48d2-9bbd-b80b77d85e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auprc'])\n",
    "plt.plot(history.history['val_auprc'])\n",
    "plt.title('model auprc')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('auprc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5ec0a-dc15-4a6b-ab40-393708382e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5a989-8735-4b2e-955d-c4c1852626d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.evaluate([Xt_eval, Xs_clin_eval[:,:max_codes_clin], Xs_ther_eval[:,:max_codes_ther]], y_eval)\n",
    "    model.evaluate([Xt_test, Xs_clin_test[:,:max_codes_clin], Xs_ther_test[:,:max_codes_ther]], y_test)\n",
    "    model.evaluate([Xt_testWales, Xs_clin_testWales[:,:max_codes_clin], Xs_ther_testWales[:,:max_codes_ther]], y_testWales)\n",
    "    model.evaluate([Xt_testScotland, Xs_clin_testScotland[:,:max_codes_clin], Xs_ther_testScotland[:,:max_codes_ther]], y_testScotland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8ec53-d208-499f-8ff5-87d475065fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict([Xt_test, Xs_clin_test[:,:max_codes_clin], Xs_ther_test[:,:max_codes_ther]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50569eb6-1ac3-40f2-9c84-da785fdaff72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summariseResultWithThreshold (Xt_test, Xs_clin_test, Xs_ther_test, testY, model):\n",
    "    preds = model.predict([Xt_test, Xs_clin_test, Xs_ther_test])\n",
    "    # tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    # specificity = tn / (tn+fp)\n",
    "    # sensitivity = tp / (tp+fn)\n",
    "    # ppv = 100*tp/(tp+fp)\n",
    "    # npv = 100*tn/(fn+tn)\n",
    "    # acc = accuracy_score(testY, preds)\n",
    "    # f1score = f1_score(testY, preds, average = 'binary')\n",
    "    # balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    # aucscore = auc(fpr, tpr)\n",
    "    aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(aucscore,4), np.round(auprc,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a6da4-5e73-4a66-9597-c4239873e2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(summariseResultWithThreshold(Xt_eval, Xs_clin_eval[:,:max_codes_clin], Xs_ther_eval[:,:max_codes_ther], y_eval, model))\n",
    "print(summariseResultWithThreshold(Xt_test, Xs_clin_test[:,:max_codes_clin], Xs_ther_test[:,:max_codes_ther], y_test, model))\n",
    "print(summariseResultWithThreshold(Xt_testWales, Xs_clin_testWales[:,:max_codes_clin], Xs_ther_testWales[:,:max_codes_ther], y_testWales, model))\n",
    "print(summariseResultWithThreshold(Xt_testScotland, Xs_clin_testScotland[:,:max_codes_clin], Xs_ther_testScotland[:,:max_codes_ther], y_testScotland, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeed10f-72a8-457d-bfcb-f2662e74bfc6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # create the model\n",
    "# embedding_vector_length = 50\n",
    "# earlyStopping = EarlyStopping(monitor='val_auc', patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "# mcp_save = ModelCheckpoint('../SeqModel/seqModel_therapy.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_codes))\n",
    "#     model.add(LSTM(128, return_sequences=True, kernel_regularizer=L1L2(l1=0.02, l2=0.03)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(LSTM(64,  kernel_regularizer=L1L2(l1=0.02, l2=0.03)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(32, activation=LeakyReLU(alpha=.3), kernel_regularizer=L1L2(l1=0.02, l2=0.03)))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     opt = Adadelta(learning_rate=5e-3, clipvalue=0.3)\n",
    "#     metrics = [\n",
    "#         AUC(num_thresholds=3, name='auc'),\n",
    "#     ]\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "#     print(model.summary())\n",
    "#     history = model.fit(Xs_train, y_train, validation_data=(Xs_val, y_val), epochs=30, batch_size=128, class_weight = class_weight, callbacks = [earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943f759-cf30-4366-b7c8-bf33fbb73101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def earlyFussion():\n",
    "       \n",
    "    inputs1 = Input(shape=tab_feature_size)\n",
    "    inputs2 = Input(shape=max_codes)\n",
    "    inputs3 = Input(shape=max_codes)\n",
    "    \n",
    "    \n",
    "    #clinical embedding for lstm\n",
    "    embedding = Embedding(vocab_size, 50, input_length=max_codes)(inputs2)\n",
    "    \n",
    "    #month embedding for lstm\n",
    "    embedding_month = Embedding(month_size, 7, input_length=max_codes)(inputs3)\n",
    "    \n",
    "    nn = Dense(32, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1))(inputs1)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    lstmClinical = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(embedding)\n",
    "    lstmMonth = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(embedding_month)\n",
    "    # lstm = Add()([lstmClinical, lstmMonth])\n",
    "    lstm = lstmClinical\n",
    "    \n",
    "    # nn = Reshape((1, 32))(nn)\n",
    "    # add = concatenate([nn, lstm], axis=1)\n",
    "    nn = Dense(16, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1))(nn)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    lstm = Bidirectional(LSTM(units=8, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(lstm)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    # # nn = Reshape((301, 64))(nn)\n",
    "    # add = concatenate([nn, lstm], axis=1)\n",
    "    nn = Dense(16, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1))(nn)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    lstm = Bidirectional(LSTM(units=8, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(lstm)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    nn = Reshape((1, 16))(nn)\n",
    "    model_tot = concatenate([nn, lstm], axis=1)\n",
    "    # model_tot = BatchNormalization()(model_tot)\n",
    "\n",
    "    model_tot = Dense(units=8, activation=LeakyReLU())(model_tot)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    \n",
    "    model_tot = Flatten()(model_tot)\n",
    "    output = Dense(1, activation='sigmoid')(model_tot)\n",
    "    \n",
    "    opt = RMSprop(learning_rate=1e-4, clipvalue=.5)\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=3, name='auc', curve='ROC'),\n",
    "        AUC(num_thresholds=3, name='auprc', curve='PR'),\n",
    "        tf.keras.metrics.Precision(name='prec'),\n",
    "        tf.keras.metrics.Recall(name='rec'),\n",
    "        tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "        tf.keras.metrics.TruePositives(name='TP'),\n",
    "        tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "    ]\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer=opt, \n",
    "        metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c11344-dfa9-40e6-b2d2-63388d84ba09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.evaluate([Xt_eval, Xs_eval[:,:max_codes], Xm_eval[:,:max_codes]], y_eval)\n",
    "    model.evaluate([Xt_test, Xs_test[:,:max_codes], Xm_test[:,:max_codes]], y_test)\n",
    "    # model.evaluate(X_testWales, y_testWales)\n",
    "    # model.evaluate(X_testScotland, y_testScotland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fd35d-2903-48e4-a655-b55f16b99da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model evaluation function\n",
    "def summariseResult (testY, preds):\n",
    "    tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    ppv = 100*tp/(tp+fp)\n",
    "    npv = 100*tn/(fn+tn)\n",
    "    acc = accuracy_score(testY, preds)\n",
    "    f1score = f1_score(testY, preds, average = 'binary')\n",
    "    balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    aucscore = auc(fpr, tpr)\n",
    "    # aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(acc,4), np.round(specificity,4), np.round(sensitivity,4), np.round(aucscore,4), np.round(auprc,4), np.round(balanceacc,4), np.round(f1score,4), np.round(ppv,4), np.round(npv,4)\n",
    "\n",
    "data_test_Xs = [X_eval, X_test, X_testWales, X_testScotland]\n",
    "data_test_ys = [y_eval, y_test, y_testWales, y_testScotland]\n",
    "for data_test_X, data_test_y in zip(data_test_Xs, data_test_ys):\n",
    "    with tf.device('/CPU:0'):\n",
    "        preds = model.predict(data_test_X)\n",
    "    preds = [0 if pred <0.5 else 1 for pred in preds]\n",
    "    print(summariseResult(data_test_y, np.squeeze(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a50962-2f80-4931-8eb7-df84792130ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('../SeqModel/model_therapy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1adb28e-44a2-4303-8b79-25348bfcaad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# a = load_model('../SeqModel/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f42bb-89f7-4693-852f-ae329cc7afe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
