{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8032a7-260b-40a8-8cbe-e292ab403047",
   "metadata": {},
   "source": [
    "# combine tabular data with seq of clinical and therapy readcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1286de-3d50-4eb3-b3c7-a4404748f00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 09:34:20.203376: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-11 09:34:20.532886: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-11 09:34:20.532919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-11 09:34:20.534610: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-11 09:34:20.673334: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Input, concatenate, Reshape, Activation, Flatten, Add, BatchNormalization, Multiply, LeakyReLU\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.metrics import AUC, SensitivityAtSpecificity\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, Adamax, SGD, Adadelta\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import L1L2, L1, L2\n",
    "from livelossplot import PlotLossesKeras\n",
    "#internal validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score, matthews_corrcoef, auc, average_precision_score, roc_auc_score, balanced_accuracy_score, roc_curve, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6e815-ba3e-471b-ba0e-efb907dfed14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e5a964-b378-45ee-a3f1-30c4a35dda53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "# trainingData = pd.read_csv('../FinalData/trainingDataEncoded_08102023.csv')\n",
    "# validationData = pd.read_csv('../FinalData/validationDataEncoded_08102023.csv')\n",
    "# evaluationData = pd.read_csv('../FinalData/evaluationDataEncoded_08102023.csv')\n",
    "# evaluationDataWales = pd.read_csv('../FinalData/evaluationDataWalesEncoded_08102023.csv')\n",
    "# evaluationDataScotland = pd.read_csv('../FinalData/evaluationDataScotlandEncoded_08102023.csv')\n",
    "\n",
    "trainingData, validationData, internalEvaluationData, evaluationData, evaluationDataWales, evaluationDataScotland = pickle.load(open('../FinalData/dataset_scaled_2vs1_09122023.sav', 'rb'))\n",
    "\n",
    "trainingData = trainingData[(trainingData.age >=8) & (trainingData.age <=80)]\n",
    "validationData = validationData[(validationData.age >=8) & (validationData.age <=80)]\n",
    "internalEvaluationData = internalEvaluationData[(internalEvaluationData.age >=8) & (internalEvaluationData.age <=80)]\n",
    "evaluationData = evaluationData[(evaluationData.age >=8) & (evaluationData.age <=80)]\n",
    "evaluationDataWales = evaluationDataWales[(evaluationDataWales.age >=8) & (evaluationDataWales.age <=80)]\n",
    "evaluationDataScotland = evaluationDataScotland[(evaluationDataScotland.age >=8) & (evaluationDataScotland.age <=80)]\n",
    "\n",
    "\n",
    "trainingData = trainingData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "validationData = validationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "internalEvaluationData = internalEvaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationData = evaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataWales = evaluationDataWales.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataScotland = evaluationDataScotland.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "\n",
    "# trainingData = trainingData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# validationData = validationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# internalEvaluationData = internalEvaluationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationData = evaluationData.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationDataWales = evaluationDataWales.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)\n",
    "# evaluationDataScotland = evaluationDataScotland.rename({'outcome_3months': '3months', 'outcome_combined_6months': '6months','outcome_combined_12months': '12months','outcome_combined_24months': '24months',}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293e45cc-eb7f-4fee-89d8-a8081c0151d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features size:  67\n",
      "['sex', 'age', 'CharlsonScore', 'BTS_step', 'average_daily_dose_ICS', 'prescribed_daily_dose_ICS', 'ICS_medication_possesion_ratio', 'numOCS', 'PriorEducation', 'numPCS', 'numPCSAsthma', 'numAntibioticsEvents', 'numAntibioticswithLRTI', 'numOCSwithLRTI', 'numAsthmaAttacks', 'numAcuteRespEvents', 'numHospEvents', 'rhinitis', 'cardiovascular', 'heartfailure', 'psoriasis', 'anaphylaxis', 'diabetes', 'ihd', 'anxiety', 'eczema', 'nasalpolyps', 'ethnic_group_Asian - ethnic group', 'ethnic_group_Black - ethnic group', 'ethnic_group_Mixed ethnic census group', 'ethnic_group_Other ethnic group', 'ethnic_group_White - ethnic group', 'ethnic_group_not_recorded', 'smokingStatus_Active Smoker', 'smokingStatus_Former Smoker', 'smokingStatus_Non Smoker', 'DeviceType_BAI', 'DeviceType_DPI', 'DeviceType_NEB', 'DeviceType_pMDI', 'DeviceType_unknown', 'cat_BMI_normal', 'cat_BMI_not recorded', 'cat_BMI_obese', 'cat_BMI_overweight', 'cat_BMI_underweight', 'imd_decile_0', 'imd_decile_1', 'imd_decile_2', 'imd_decile_3', 'imd_decile_4', 'imd_decile_5', 'imd_decile_6', 'imd_decile_7', 'imd_decile_8', 'imd_decile_9', 'imd_decile_10', 'PEFStatus_60-80', 'PEFStatus_less than 60', 'PEFStatus_more than 80', 'PEFStatus_not_recorded', 'EosinophilLevel_high', 'EosinophilLevel_normal', 'EosinophilLevel_unknown', 'system_EMIS', 'system_SystemOne', 'system_Vision']\n"
     ]
    }
   ],
   "source": [
    "#Define feature candidates\n",
    "\n",
    "features_columns = trainingData.columns.to_list()\n",
    "exclude_columns = ['patid', 'practice_id', #identifier\n",
    "                   'BMI', #use the categorical instead\n",
    "                   'ethnicity', #use ethnic_group instead\n",
    "                   'Spacer',  #all zero\n",
    "                   \n",
    "                   'outcome_3months', 'outcome_6months', 'outcome_9months', 'outcome_12months', 'outcome_15months', 'outcome_18months', \n",
    "                   'outcome_21months', 'outcome_24months', 'outcome_combined_6months', 'outcome_combined_9months', 'outcome_combined_12months', \n",
    "                   'outcome_combined_15months', 'outcome_combined_18months', 'outcome_combined_24months', '3months', '6months', '9months', '12months', '24months', #outcomes variable\n",
    "                   \n",
    "                   'postcode_district', 'County', 'LocalAuthority', 'OutputAreaClassification', #location related variables, use IMD decile only\n",
    "                   \n",
    "                   'cat_age', 'cat_average_daily_dose_ICS', 'cat_prescribed_daily_dose_ICS', 'cat_ICS_medication_possesion_ratio', 'cat_numOCS', 'cat_numOCSEvents', \n",
    "                   'cat_numOCSwithLRTI', 'cat_numAcuteRespEvents', 'cat_numAntibioticsEvents', 'cat_numAntibioticswithLRTI', 'cat_numAsthmaAttacks', 'cat_numHospEvents', \n",
    "                   'cat_numPCS', 'cat_numPCSAsthma', #use continous vars instead\n",
    "                   \n",
    "                   'count_rhinitis', 'count_cardiovascular', 'count_heartfailure',\n",
    "                   'count_psoriasis', 'count_anaphylaxis', 'count_diabetes', 'count_ihd',\n",
    "                   'count_anxiety', 'count_eczema', 'count_nasalpolyps',\n",
    "                   'count_paracetamol', 'count_nsaids', 'count_betablocker', #use binary ones\n",
    "                   \n",
    "                   'paracetamol', 'nsaids', 'betablocker', #no data in evaluation\n",
    "                   \n",
    "                   'numOCSEvents', #duplicate with numOCS\n",
    "                   \n",
    "                   'month_12', 'month_4', 'month_5', 'month_10', 'month_1', 'month_6', 'month_3', \n",
    "                   'month_11', 'month_8', 'month_9', 'month_7', 'month_2', #month of attacks\n",
    "                   \n",
    "                   # 'system_EMIS', 'system_SystemOne', 'system_Vision', #primary care system used\n",
    "                  ]\n",
    "exclude_columns = exclude_columns + [x for x in features_columns if '_count' in x] #filter out commorbid count variables\n",
    "features_columns = [x for x in features_columns if x not in exclude_columns]\n",
    "print('Features size: ', len(features_columns))\n",
    "print(features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1e4f0-1280-448d-8df8-55a2dd6648d9",
   "metadata": {},
   "source": [
    "# load sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0cdd2d-5347-47e4-8435-5d80018fdba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clinical = pd.read_feather('../SeqModel/all_data_clinical.feather')\n",
    "therapy = pd.read_feather('../SeqModel/all_data_therapy.feather')\n",
    "seqCols = ['patid',\n",
    "       'read_code_seq_padded_end_idx_clin',\n",
    "       'month_padded_idx_end_clin',\n",
    "       'read_code_seq_padded_end_idx_ther',\n",
    "       'month_padded_idx_end_ther']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72b417e-3671-414a-ab47-dbe548316101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_data = clinical.merge(therapy[['patid', 'read_code_seq_padded_idx', 'read_code_seq_padded_end_idx',\n",
    "       'month_padded_idx', 'month_padded_idx_end']], on='patid', suffixes=['_clin', '_ther'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f21cb5-e88b-4f4e-b2ea-72ef9034ca48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData = trainingData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "validationData = validationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "internalEvaluationData = internalEvaluationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationData = evaluationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationDataWales = evaluationDataWales.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationDataScotland = evaluationDataScotland.merge(sequence_data[seqCols], on='patid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee6b74c-ad65-4e0b-bc67-d8e9d3fd02a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127450, 131)\n",
      "(39675, 131)\n",
      "(31980, 131)\n",
      "(8044, 131)\n",
      "(5359, 131)\n",
      "(2685, 131)\n"
     ]
    }
   ],
   "source": [
    "print(trainingData.shape)\n",
    "print(validationData.shape)\n",
    "print(internalEvaluationData.shape)\n",
    "print(evaluationData.shape)\n",
    "print(evaluationDataWales.shape)\n",
    "print(evaluationDataScotland.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6b20f0-4a38-45fe-a059-8d4b8aca7988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127450, 67)\n",
      "(31980, 67)\n",
      "(39675, 67)\n",
      "(8044, 67)\n",
      "(5359, 67)\n",
      "(2685, 67)\n"
     ]
    }
   ],
   "source": [
    "Xt_train = np.array(trainingData[features_columns].values)\n",
    "Xt_val = np.array(validationData[features_columns].values)\n",
    "Xt_internaleval = np.array(internalEvaluationData[features_columns].values)\n",
    "Xt_eval = np.array(evaluationData[features_columns].values)\n",
    "Xt_eval_Wales = np.array(evaluationDataWales[features_columns].values)\n",
    "Xt_eval_Scotland = np.array(evaluationDataScotland[features_columns].values)\n",
    "\n",
    "#scalling tabular data\n",
    "scaler = StandardScaler().fit(Xt_train)\n",
    "Xt_train = scaler.transform(Xt_train)\n",
    "Xt_val = scaler.transform(Xt_val)\n",
    "Xt_internaleval = scaler.transform(Xt_internaleval)\n",
    "Xt_eval = scaler.transform(Xt_eval)\n",
    "Xt_eval_Wales = scaler.transform(Xt_eval_Wales)\n",
    "Xt_eval_Scotland = scaler.transform(Xt_eval_Scotland)\n",
    "\n",
    "Xclin_train = np.array(trainingData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_val = np.array(validationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_internaleval = np.array(internalEvaluationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval = np.array(evaluationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval_Wales = np.array(evaluationDataWales['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval_Scotland = np.array(evaluationDataScotland['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_train = np.array([x for x in Xclin_train])\n",
    "Xclin_val = np.array([x for x in Xclin_val])\n",
    "Xclin_internaleval = np.array([x for x in Xclin_internaleval])\n",
    "Xclin_eval = np.array([x for x in Xclin_eval])\n",
    "Xclin_eval_Wales = np.array([x for x in Xclin_eval_Wales])\n",
    "Xclin_eval_Scotland = np.array([x for x in Xclin_eval_Scotland])\n",
    "\n",
    "Xther_train = np.array(trainingData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_val = np.array(validationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_internaleval = np.array(internalEvaluationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval = np.array(evaluationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval_Wales = np.array(evaluationDataWales['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval_Scotland = np.array(evaluationDataScotland['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_train = np.array([x for x in Xther_train])\n",
    "Xther_val = np.array([x for x in Xther_val])\n",
    "Xther_internaleval = np.array([x for x in Xther_internaleval])\n",
    "Xther_eval = np.array([x for x in Xther_eval])\n",
    "Xther_eval_Wales = np.array([x for x in Xther_eval_Wales])\n",
    "Xther_eval_Scotland = np.array([x for x in Xther_eval_Scotland])\n",
    "\n",
    "\n",
    "print(Xt_train.shape)\n",
    "print(Xt_internaleval.shape)\n",
    "print(Xt_val.shape)\n",
    "print(Xt_eval.shape)\n",
    "print(Xt_eval_Wales.shape)\n",
    "print(Xt_eval_Scotland.shape)\n",
    "\n",
    "# target_outcomes = ['3months', '6months', '12months', '24months'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2e26e6d-1fba-430e-a954-9676cc730947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51644\n",
      "10709\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#vocab\n",
    "code2idx_clin = pickle.load(open('../SeqModel/all_vocab_clinical.sav', 'rb'))\n",
    "code2idx_ther = pickle.load(open('../SeqModel/all_vocab_therapy.sav', 'rb'))\n",
    "month2idx = pickle.load(open('../SeqModel/all_vocab_month.sav', 'rb'))\n",
    "vocab_size_clinical = len(code2idx_clin)+1\n",
    "vocab_size_therapy = len(code2idx_ther)+1\n",
    "month_size = len(month2idx)+1\n",
    "print(vocab_size_clinical)\n",
    "print(vocab_size_therapy)\n",
    "print(month_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95667606-62df-44a4-a3db-7a440d36947f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 5.418068284822238}\n"
     ]
    }
   ],
   "source": [
    "target_outcome = '12months'\n",
    "max_codes_clin = Xclin_train.shape[1]\n",
    "max_codes_ther = Xther_train.shape[1]\n",
    "tab_feature_size = Xt_train.shape[1]\n",
    "\n",
    "y_train = trainingData[target_outcome].values\n",
    "y_val = validationData[target_outcome].values\n",
    "y_internaleval = internalEvaluationData[target_outcome].values\n",
    "y_eval = evaluationData[target_outcome].values\n",
    "y_eval_Wales = evaluationDataWales[target_outcome].values\n",
    "y_eval_Scotland = evaluationDataScotland[target_outcome].values\n",
    "\n",
    "pos_weight = sum(x == 0 for x in y_train)/sum(x == 1 for x in y_train)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320eb9f8-d645-4f08-b995-9424c02e0423",
   "metadata": {},
   "source": [
    "# initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45f82c-62f0-4eab-8a38-1f6519f8d2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hierarchical():\n",
    "    #tabular dara - demography   \n",
    "    inputs1 = Input(shape=tab_feature_size)\n",
    "    nn = Dense(32, kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.04))(inputs1)\n",
    "    nn = BatchNormalization()(nn)\n",
    "    nn = Activation('relu')(nn)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    \n",
    "    #clinical embedding for lstm\n",
    "    inputs2 = Input(shape=max_codes_clin)\n",
    "    embedding_clin = Embedding(vocab_size_clinical, int(np.cbrt(vocab_size_clinical)), input_length=max_codes_clin)(inputs2)\n",
    "    lstmClinical = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.04)))(embedding_clin)\n",
    "        \n",
    "     \n",
    "    #therapy embedding for lstm\n",
    "    inputs3 = Input(shape=max_codes_ther)\n",
    "    embedding_ther = Embedding(vocab_size_therapy, int(np.cbrt(vocab_size_therapy)), input_length=max_codes_ther)(inputs3)\n",
    "    lstmTherapy = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.04)))(embedding_ther)\n",
    "    \n",
    "    ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "    add = concatenate([lstmClinical, lstmTherapy], axis=1)\n",
    "    \n",
    "    ###layer 3 - LSTM to the final product\n",
    "    lstm = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.04)))(add)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    #merge tabular and sequence layers\n",
    "    nn = Reshape((1, 32))(nn)\n",
    "    add = concatenate([nn, lstm], axis=1)\n",
    "    \n",
    "    ###layer 4 - FCN before classification layer\n",
    "    final = Dense(units=8, kernel_regularizer=L1L2(l1=0.0, l2=0.04))(add)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.5)(final)\n",
    "    final = Flatten()(final)\n",
    "    final = Dropout(0.5)(final)\n",
    "    \n",
    "    ###layer 5 - classification layer\n",
    "    output = Dense(1, activation='sigmoid')(final)\n",
    "    \n",
    "    opt = RMSprop(learning_rate=1e-3, clipvalue=.5)\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "        AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        tf.keras.metrics.Precision(name='prec'),\n",
    "        tf.keras.metrics.Recall(name='rec'),\n",
    "        tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "        tf.keras.metrics.TruePositives(name='TP'),\n",
    "        tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "    ]\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer=opt, \n",
    "        metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1375ac8-eeac-420d-bb0b-3c62b2494cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# sklearn_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weight = dict(enumerate(sklearn_weights))\n",
    "\n",
    "#Hyperparameter\n",
    "lr = 1e-5\n",
    "clipvalue = 0.2\n",
    "epoch = 1000\n",
    "batch_size = 256\n",
    "embedding_vector_length = 50\n",
    "month_embedding_vector_length = 5\n",
    "# embedding_vector_length = int(np.sqrt(vocab_size))\n",
    "# embedding_vector_length = int(np.cbrt(vocab_size))\n",
    "print(embedding_vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0d784-620b-44e3-8aeb-c3d39b8dce0a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#visualise model\n",
    "model = hierarchical()\n",
    "# model = earlyFussion()\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcd4e8-9bc0-479d-b5ac-ec2488e953ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "#training\n",
    "with tf.device('/GPU:0'):\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=50, verbose=0, mode='max', restore_best_weights=True)\n",
    "    mcp_save = ModelCheckpoint('../SeqModel/seqModel_therapy_tabSeq.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "    history = model.fit([Xt_train, Xclin_train[:,:max_codes_clin], Xther_train[:,:max_codes_ther]], y_train, validation_data=([Xt_val, Xclin_val[:,:max_codes_clin], Xther_val[:,:max_codes_ther]], y_val), \n",
    "                            epochs=epoch, batch_size=128, \n",
    "                        class_weight = class_weight, \n",
    "                        callbacks = [earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a131e0-0c7e-463a-ac70-7aeae48915e0",
   "metadata": {},
   "source": [
    "# tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03baf622-f51d-4ffe-b399-661929a754e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use only 20% of training data for parameter search\n",
    "target_outcomes = ['12months']\n",
    "ignore, use = train_test_split(trainingData, stratify=trainingData[target_outcomes[0]], test_size=0.2, random_state=1234)\n",
    "search_train, search_val = train_test_split(use, stratify=use[target_outcomes[0]], test_size=0.2, random_state=1234)\n",
    "search_train.reset_index(inplace=True, drop=True)\n",
    "search_val.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "Xt_train_search = np.array(search_train[features_columns].values)\n",
    "Xt_val_search = np.array(search_val[features_columns].values)\n",
    "scaler = StandardScaler().fit(Xt_train)\n",
    "Xt_train_search = scaler.transform(Xt_train_search)\n",
    "Xt_val_search = scaler.transform(Xt_val_search)\n",
    "Xclin_train_search = np.array(search_train['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_val_search = np.array(search_val['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xther_train_search = np.array(search_train['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_val_search = np.array(search_val['read_code_seq_padded_end_idx_ther'].values)\n",
    "\n",
    "Xclin_train_search = np.array([x for x in Xclin_train_search])\n",
    "Xclin_val_search = np.array([x for x in Xclin_val_search])\n",
    "Xther_train_search = np.array([x for x in Xther_train_search])\n",
    "Xther_val_search = np.array([x for x in Xther_val_search])\n",
    "\n",
    "y_train_search = search_train[target_outcome].values\n",
    "y_val_search = search_val[target_outcome].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47f9e1d-46dd-4f15-80f9-63101a8dd71b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20392, 67)\n",
      "(5098, 67)\n",
      "5.416614222781623\n",
      "5.420654911838791\n"
     ]
    }
   ],
   "source": [
    "print(Xt_train_search.shape)\n",
    "print(Xt_val_search.shape)\n",
    "\n",
    "print(search_train[target_outcomes[0]].value_counts()[0]/search_train[target_outcomes[0]].value_counts()[1])\n",
    "print(search_val[target_outcomes[0]].value_counts()[0]/search_val[target_outcomes[0]].value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eb1ad-698a-4da5-b7c5-a45ae1ba716c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## layer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e075cd9b-e406-4975-b2f8-90e3a956693e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "class MyHyperModel_layerBlocks(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #tabular dara - demography   \n",
    "        inputs1 = Input(shape=tab_feature_size)\n",
    "        \n",
    "        nn = Dense(units=128, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0.04,l2=0.08),\n",
    "                        bias_regularizer=L1L2(l1=0.08,l2=0),\n",
    "                        activity_regularizer=L1L2(l1=0,l2=0),\n",
    "                        input_shape = (Xt_train_search.shape[1],)\n",
    "                       )(inputs1)\n",
    "        nn = BatchNormalization()(nn)\n",
    "        nn = Activation(\"elu\")(nn)\n",
    "        nn = Dropout(0.4)(nn)\n",
    "\n",
    "\n",
    "        #clinical embedding for lstm\n",
    "        inputs2 = Input(shape=max_codes_clin)\n",
    "        lstm_units0 = hp.Int(\"lstm_clinical_units0\", min_value=16, max_value=128, step=32)\n",
    "        embedding_clin = Embedding(vocab_size_clinical, \n",
    "                                   output_dim = hp.Int(\"embedding_clinical\", min_value=int(np.cbrt(vocab_size_clinical)), max_value=int(np.sqrt(vocab_size_clinical))), \n",
    "                                   input_length=max_codes_clin\n",
    "                                  )(inputs2)\n",
    "        lstmClinical = Bidirectional(LSTM(units=lstm_units0, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02), \n",
    "                                           l2=hp.Float(\"kernel_l2_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02)\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02), \n",
    "                                           l2=hp.Float(\"bias_l2_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02)\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=hp.Float(\"act_l1_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02), \n",
    "                                           l2=hp.Float(\"act_l2_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02)\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=hp.Float(\"rec_l1_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02), \n",
    "                                           l2=hp.Float(\"rec_l2_lstmClinical\", min_value=0.0, max_value=0.1, step=0.02)\n",
    "                                                       ),\n",
    "                                         )\n",
    "                                    )(embedding_clin)\n",
    "\n",
    "\n",
    "        #therapy embedding for lstm\n",
    "        inputs3 = Input(shape=max_codes_ther)\n",
    "        embedding_ther = Embedding(vocab_size_therapy, \n",
    "                              output_dim = hp.Int(\"embedding_therapy\", min_value=int(np.cbrt(vocab_size_therapy)), max_value=int(np.sqrt(vocab_size_therapy))), \n",
    "                              input_length=max_codes_ther\n",
    "                             )(inputs3)\n",
    "        lstmTherapy = Bidirectional(LSTM(units=lstm_units0, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"kernel_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"bias_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=hp.Float(\"act_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"act_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=hp.Float(\"rec_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"rec_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(embedding_ther)\n",
    "\n",
    "        ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "        add = concatenate([lstmClinical, lstmTherapy], axis=1)\n",
    "\n",
    "        ###layer 2 - LSTM to the final product\n",
    "        lstm = Bidirectional(LSTM(units=64, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"kernel_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"bias_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=hp.Float(\"act_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"act_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=hp.Float(\"rec_l1_lstmTherapy\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"rec_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(add)\n",
    "        lstm = Dropout(rate=hp.Float(\"rate_lstm1\", min_value=0.1, max_value=0.5))(lstm)\n",
    "\n",
    "        #merge tabular and sequence layers\n",
    "        nn = Reshape((1, 128))(nn)\n",
    "        add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "        ###layer 4 - FCN before classification layer\n",
    "        neurons_final_layer = hp.Int(\"units_layer1\", min_value=32, max_value=128, step=32)\n",
    "        final = Dense(units=neurons_final_layer, \n",
    "                        kernel_initializer=hp.Choice(\"kernel_initializer_layer1\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "                        kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"kernel_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                        bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"bias_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                        activity_regularizer=L1L2(l1=hp.Float(\"act_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"act_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                     )(add)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation(\"elu\")(final)\n",
    "        final = Dropout(0.4)(final)                                                        \n",
    "\n",
    "\n",
    "        ###layer 5 - classification layer\n",
    "        final = Flatten()(final)\n",
    "        output = Dense(1, activation='sigmoid')(final)\n",
    "\n",
    "        opt = Adam()\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "            # tf.keras.metrics.Precision(name='prec'),\n",
    "            # tf.keras.metrics.Recall(name='rec'),\n",
    "            # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "            # tf.keras.metrics.TruePositives(name='TP'),\n",
    "            # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "        ]\n",
    "\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=opt, \n",
    "            metrics=metrics)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec6467b6-978b-45d6-819b-e2b10dfa3967",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 02m 38s]\n",
      "val_auc: 0.7326650023460388\n",
      "\n",
      "Best val_auc So Far: 0.7914168834686279\n",
      "Total elapsed time: 00h 58m 53s\n",
      "--- Training time: 3534.147896051407 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_FCN.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "pos_weight = sum(x == 0 for x in y_train)/sum(x == 1 for x in y_train)\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_layerBlocks()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"lastmBlocks\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search([Xt_train_search, Xclin_train_search, Xther_train_search], y_train_search, \n",
    "                 validation_data=([Xt_val_search, Xclin_val_search, Xther_val_search], y_val_search),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1559f565-f741-425d-982a-15985ed7edf6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ../SeqModel/tuner/lastmBlocks\n",
      "Showing 3 best trials\n",
      "Objective(name=\"val_auc\", direction=\"max\")\n",
      "\n",
      "Trial 0025 summary\n",
      "Hyperparameters:\n",
      "lstm_clinical_units0: 48\n",
      "embedding_clinical: 144\n",
      "kernel_l1_lstmClinical: 0.1\n",
      "kernel_l2_lstmClinical: 0.04\n",
      "bias_l1_lstmClinical: 0.1\n",
      "bias_l2_lstmClinical: 0.08\n",
      "act_l1_lstmClinical: 0.0\n",
      "act_l2_lstmClinical: 0.06\n",
      "rec_l1_lstmClinical: 0.0\n",
      "rec_l2_lstmClinical: 0.04\n",
      "embedding_therapy: 62\n",
      "kernel_l1_lstmTherapy: 0.09236083288189484\n",
      "kernel_l2_lstmTherapy: 0.036755944592777104\n",
      "bias_l1_lstmTherapy: 0.03643811676858954\n",
      "bias_l2_lstmTherapy: 0.005029749805083916\n",
      "act_l1_lstmTherapy: 0.07146382396744337\n",
      "act_l2_lstmTherapy: 0.009106353056511385\n",
      "rec_l1_lstmTherapy: 0.0031326326705593656\n",
      "rec_l2_lstmTherapy: 0.08946777293518121\n",
      "rate_lstm1: 0.3403698716775575\n",
      "units_layer1: 128\n",
      "kernel_initializer_layer1: glorot_uniform\n",
      "kernel_l1_dense1: 0.0\n",
      "kernel_l2_dense1: 0.04\n",
      "bias_l1_dense1: 0.04\n",
      "bias_l2_dense1: 0.02\n",
      "act_l1_dense1: 0.06\n",
      "act_l2_dense1: 0.1\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 9\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0020\n",
      "Score: 0.7914168834686279\n",
      "\n",
      "Trial 0019 summary\n",
      "Hyperparameters:\n",
      "lstm_clinical_units0: 112\n",
      "embedding_clinical: 53\n",
      "kernel_l1_lstmClinical: 0.0\n",
      "kernel_l2_lstmClinical: 0.04\n",
      "bias_l1_lstmClinical: 0.1\n",
      "bias_l2_lstmClinical: 0.0\n",
      "act_l1_lstmClinical: 0.02\n",
      "act_l2_lstmClinical: 0.02\n",
      "rec_l1_lstmClinical: 0.04\n",
      "rec_l2_lstmClinical: 0.04\n",
      "embedding_therapy: 40\n",
      "kernel_l1_lstmTherapy: 0.004254705720113172\n",
      "kernel_l2_lstmTherapy: 0.05631328340297731\n",
      "bias_l1_lstmTherapy: 0.02016847186259321\n",
      "bias_l2_lstmTherapy: 0.021372341549443064\n",
      "act_l1_lstmTherapy: 0.06691089876823275\n",
      "act_l2_lstmTherapy: 0.05495058119067745\n",
      "rec_l1_lstmTherapy: 0.04328100678359394\n",
      "rec_l2_lstmTherapy: 0.051530314307130655\n",
      "rate_lstm1: 0.4230498324936346\n",
      "units_layer1: 64\n",
      "kernel_initializer_layer1: lecun_uniform\n",
      "kernel_l1_dense1: 0.04\n",
      "kernel_l2_dense1: 0.06\n",
      "bias_l1_dense1: 0.08\n",
      "bias_l2_dense1: 0.06\n",
      "act_l1_dense1: 0.08\n",
      "act_l2_dense1: 0.02\n",
      "tuner/epochs: 9\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.7877896428108215\n",
      "\n",
      "Trial 0020 summary\n",
      "Hyperparameters:\n",
      "lstm_clinical_units0: 48\n",
      "embedding_clinical: 144\n",
      "kernel_l1_lstmClinical: 0.1\n",
      "kernel_l2_lstmClinical: 0.04\n",
      "bias_l1_lstmClinical: 0.1\n",
      "bias_l2_lstmClinical: 0.08\n",
      "act_l1_lstmClinical: 0.0\n",
      "act_l2_lstmClinical: 0.06\n",
      "rec_l1_lstmClinical: 0.0\n",
      "rec_l2_lstmClinical: 0.04\n",
      "embedding_therapy: 62\n",
      "kernel_l1_lstmTherapy: 0.09236083288189484\n",
      "kernel_l2_lstmTherapy: 0.036755944592777104\n",
      "bias_l1_lstmTherapy: 0.03643811676858954\n",
      "bias_l2_lstmTherapy: 0.005029749805083916\n",
      "act_l1_lstmTherapy: 0.07146382396744337\n",
      "act_l2_lstmTherapy: 0.009106353056511385\n",
      "rec_l1_lstmTherapy: 0.0031326326705593656\n",
      "rec_l2_lstmTherapy: 0.08946777293518121\n",
      "rate_lstm1: 0.3403698716775575\n",
      "units_layer1: 128\n",
      "kernel_initializer_layer1: glorot_uniform\n",
      "kernel_l1_dense1: 0.0\n",
      "kernel_l2_dense1: 0.04\n",
      "bias_l1_dense1: 0.04\n",
      "bias_l2_dense1: 0.02\n",
      "act_l1_dense1: 0.06\n",
      "act_l2_dense1: 0.1\n",
      "tuner/epochs: 9\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.7867612838745117\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61ebbd-4052-44ad-8af0-7c68f753746f",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b34484c-c2a6-4e9b-9992-5be57241ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "class MyHyperModel_optimizer(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #tabular dara - demography   \n",
    "        inputs1 = Input(shape=tab_feature_size)\n",
    "                nn = Dense(units=128, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0.04,l2=0.08),\n",
    "                        bias_regularizer=L1L2(l1=0.08,l2=0),\n",
    "                        activity_regularizer=L1L2(l1=0,l2=0),\n",
    "                        input_shape = (Xt_train_search.shape[1],)\n",
    "                       )(inputs1)\n",
    "        nn = BatchNormalization()(nn)\n",
    "        nn = Activation(\"elu\")(nn)\n",
    "        nn = Dropout(0.4)(nn)\n",
    "\n",
    "\n",
    "        #clinical embedding for lstm\n",
    "        inputs2 = Input(shape=max_codes_clin)\n",
    "        embedding_clin = Embedding(vocab_size_clinical, \n",
    "                                   output_dim = 144, \n",
    "                                   input_length=max_codes_clin\n",
    "                                  )(inputs2)\n",
    "        lstmClinical = Bidirectional(LSTM(units=lstm_units0, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.04\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.08\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0, \n",
    "                                           l2=.06\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0, \n",
    "                                           l2=.04\n",
    "                                                      ),\n",
    "                                         )\n",
    "                                    )(embedding_clin)\n",
    "\n",
    "\n",
    "        #therapy embedding for lstm\n",
    "        inputs3 = Input(shape=max_codes_ther)\n",
    "        embedding_ther = Embedding(vocab_size_therapy, \n",
    "                              output_dim = 62, \n",
    "                              input_length=max_codes_ther\n",
    "                             )(inputs3)\n",
    "        lstmTherapy = Bidirectional(LSTM(units=lstm_units0, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(embedding_ther)\n",
    "\n",
    "        ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "        add = concatenate([lstmClinical, lstmTherapy], axis=1)\n",
    "\n",
    "        ###layer 2 - LSTM to the final product\n",
    "        lstm = Bidirectional(LSTM(units=64, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(add)\n",
    "        lstm = Dropout(rate=0.3403698716775575)(lstm)\n",
    "\n",
    "        #merge tabular and sequence layers\n",
    "        nn = Reshape((1, 128))(nn)\n",
    "        add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "        ###layer 4 - FCN before classification layer\n",
    "        neurons_final_layer = 128\n",
    "        final = Dense(units=neurons_final_layer, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0, \n",
    "                                                l2=.04\n",
    "                                               ),\n",
    "                        bias_regularizer=L1L2(l1=.04, \n",
    "                                                l2=.02\n",
    "                                               ),\n",
    "                        activity_regularizer=L1L2(l1=.06, \n",
    "                                                l2=.1\n",
    "                                               ),\n",
    "                     )(add)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation(\"elu\")(final)\n",
    "        final = Dropout(0.4)(final)                                                        \n",
    "\n",
    "\n",
    "        ###layer 5 - classification layer\n",
    "        final = Flatten()(final)\n",
    "        output = Dense(1, activation='sigmoid')(final)\n",
    "\n",
    "        opt = hp.Choice(\"opt\", ['Adadelta', 'Adam', 'Adamax', 'RMSprop', 'SGD'])\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "            # tf.keras.metrics.Precision(name='prec'),\n",
    "            # tf.keras.metrics.Recall(name='rec'),\n",
    "            # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "            # tf.keras.metrics.TruePositives(name='TP'),\n",
    "            # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "        ]\n",
    "\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=opt, \n",
    "            metrics=metrics)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6016b40-f998-445b-b383-062fcec177d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 26s]\n",
      "val_auc: 0.6849130988121033\n",
      "\n",
      "Best val_auc So Far: 0.7950500845909119\n",
      "Total elapsed time: 00h 29m 27s\n",
      "\n",
      "Search: Running Trial #21\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "48                |16                |48\n",
      "RMSprop           |Adamax            |opt\n",
      "9                 |25                |tuner/epochs\n",
      "0                 |9                 |tuner/initial_epoch\n",
      "1                 |2                 |tuner/bracket\n",
      "0                 |2                 |tuner/round\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 67)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 62)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 129)]                0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  8704      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 62, 144)              7436736   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 129, 62)              663958    ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 128)                  512       ['dense[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 62, 96)               74112     ['embedding[0][0]']           \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 129, 96)              42624     ['embedding_1[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 128)                  0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 191, 96)              0         ['bidirectional[0][0]',       \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 128)                  0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  (None, 191, 128)             82432     ['concatenate[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 128)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 191, 128)             0         ['bidirectional_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 192, 128)             0         ['reshape[0][0]',             \n",
      " )                                                                   'dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 192, 128)             16512     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 192, 128)             512       ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 192, 128)             0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 192, 128)             0         ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 24576)                0         ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    24577     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8350679 (31.86 MB)\n",
      "Trainable params: 8350167 (31.85 MB)\n",
      "Non-trainable params: 512 (2.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/9\n",
      "160/160 [==============================] - 69s 387ms/step - loss: 290.5819 - auc: 0.5910 - auprc: 0.2168 - val_loss: 40.8548 - val_auc: 0.4567 - val_auprc: 0.1658\n",
      "Epoch 2/9\n",
      " 43/160 [=======>......................] - ETA: 12:38 - loss: 37.6381 - auc: 0.5889 - auprc: 0.2226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 04:07:06.140754: E tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m36.346833526s\n",
      "\n",
      "********************************\n",
      "[Compiling module a_inference__update_step_xla_1613049__.41] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69/160 [===========>..................] - ETA: 30:16 - loss: 35.7276 - auc: 0.6044 - auprc: 0.2257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 04:27:46.482689: E tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 5m1.747462049s\n",
      "\n",
      "********************************\n",
      "[Compiling module a_inference__update_step_xla_1613021__.41] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92/160 [================>.............] - ETA: 29:40 - loss: 34.2060 - auc: 0.5977 - auprc: 0.2207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 04:58:04.700925: E tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 5m53.153657634s\n",
      "\n",
      "********************************\n",
      "[Compiling module a_inference__update_step_xla_1613049__.41] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - ETA: 0s - loss: 31.1626 - auc: 0.5916 - auprc: 0.2135 "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_optimizer()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"optimizerLSTM\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search([Xt_train_search, Xclin_train_search, Xther_train_search], y_train_search, \n",
    "                 validation_data=([Xt_val_search, Xclin_val_search, Xther_val_search], y_val_search),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a24a8ac8-1d0a-4767-81d5-18e7418f0e20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ../SeqModel/tuner/optimizerLSTM/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "#load tuner, cimment it if not needed\n",
    "model = MyHyperModel_optimizer()\n",
    "tuner = keras_tuner.Hyperband(\n",
    "    hypermodel= model,\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_epochs=25,\n",
    "    hyperband_iterations=1,\n",
    "    overwrite=False, #make sure it is set to False\n",
    "    directory='../SeqModel/tuner/',\n",
    "    project_name=\"optimizerLSTM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec90876-01ef-48ba-ab57-e5663fb4e8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ../SeqModel/tuner/optimizerLSTM\n",
      "Showing 3 best trials\n",
      "Objective(name=\"val_auc\", direction=\"max\")\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "48: 16\n",
      "opt: Adamax\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 9\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0012\n",
      "Score: 0.7950500845909119\n",
      "\n",
      "Trial 0012 summary\n",
      "Hyperparameters:\n",
      "48: 16\n",
      "opt: Adamax\n",
      "tuner/epochs: 9\n",
      "tuner/initial_epoch: 3\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0005\n",
      "Score: 0.7902669906616211\n",
      "\n",
      "Trial 0017 summary\n",
      "Hyperparameters:\n",
      "48: 112\n",
      "opt: Adam\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 9\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0014\n",
      "Score: 0.7863352298736572\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a9e53-433b-4fed-a116-7c7bc1e05c9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## opt parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac4346e3-cedf-49bd-b14f-ebaff2ff4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "class MyHyperModel_optParameter(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #tabular dara - demography   \n",
    "        inputs1 = Input(shape=tab_feature_size)\n",
    "        \n",
    "        nn = Dense(units=128, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0.04,l2=0.08),\n",
    "                        bias_regularizer=L1L2(l1=0.08,l2=0),\n",
    "                        activity_regularizer=L1L2(l1=0,l2=0),\n",
    "                        input_shape = (Xt_train_search.shape[1],)\n",
    "                       )(inputs1)\n",
    "        nn = BatchNormalization()(nn)\n",
    "        nn = Activation(\"elu\")(nn)\n",
    "        nn = Dropout(0.4)(nn)\n",
    "\n",
    "\n",
    "        #clinical embedding for lstm\n",
    "        inputs2 = Input(shape=max_codes_clin)\n",
    "        embedding_clin = Embedding(vocab_size_clinical, \n",
    "                                   output_dim = 144, \n",
    "                                   input_length=max_codes_clin\n",
    "                                  )(inputs2)\n",
    "        lstmClinical = Bidirectional(LSTM(units=48, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.04\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=.1, \n",
    "                                           l2=.08\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0, \n",
    "                                           l2=.06\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0, \n",
    "                                           l2=.04\n",
    "                                                      ),\n",
    "                                         )\n",
    "                                    )(embedding_clin)\n",
    "\n",
    "\n",
    "        #therapy embedding for lstm\n",
    "        inputs3 = Input(shape=max_codes_ther)\n",
    "        embedding_ther = Embedding(vocab_size_therapy, \n",
    "                              output_dim = 62, \n",
    "                              input_length=max_codes_ther\n",
    "                             )(inputs3)\n",
    "        lstmTherapy = Bidirectional(LSTM(units=48, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(embedding_ther)\n",
    "\n",
    "        ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "        add = concatenate([lstmClinical, lstmTherapy], axis=1)\n",
    "\n",
    "        ###layer 2 - LSTM to the final product\n",
    "        lstm = Bidirectional(LSTM(units=64, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.09236083288189484, \n",
    "                                           l2=0.036755944592777104\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.03643811676858954, \n",
    "                                           l2=0.005029749805083916\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.07146382396744337, \n",
    "                                           l2=0.009106353056511385\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.0031326326705593656, \n",
    "                                           l2=0.08946777293518121\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(add)\n",
    "        lstm = Dropout(rate=0.3403698716775575)(lstm)\n",
    "\n",
    "        #merge tabular and sequence layers\n",
    "        nn = Reshape((1, 128))(nn)\n",
    "        add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "        ###layer 4 - FCN before classification layer\n",
    "        neurons_final_layer = 128\n",
    "        final = Dense(units=neurons_final_layer, \n",
    "                        kernel_initializer=\"glorot_uniform\", \n",
    "                        kernel_regularizer=L1L2(l1=0, \n",
    "                                                l2=.04\n",
    "                                               ),\n",
    "                        bias_regularizer=L1L2(l1=.04, \n",
    "                                                l2=.02\n",
    "                                               ),\n",
    "                        activity_regularizer=L1L2(l1=.06, \n",
    "                                                l2=.1\n",
    "                                               ),\n",
    "                     )(add)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation(\"elu\")(final)\n",
    "        final = Dropout(0.4)(final)                                                        \n",
    "\n",
    "\n",
    "        ###layer 5 - classification layer\n",
    "        final = Flatten()(final)\n",
    "        output = Dense(1, activation='sigmoid')(final)\n",
    "        \n",
    "        lr=hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]) \n",
    "        clipvalue=hp.Float(\"clipvalue\", min_value=0.1, max_value=0.7, step=0.2)\n",
    "        beta_1=hp.Float(\"beta_1\", min_value=0.8, max_value=0.95, step=0.05)\n",
    "        beta_2=hp.Float(\"beta_2\", min_value=0.900, max_value=0.999, step=0.011)\n",
    "        epsilon=hp.Choice(\"epsilon\", [1e-6, 1e-7, 1e-8]) \n",
    "        weight_decay=hp.Float(\"weight_decay\", min_value=0, max_value=0.1) \n",
    "\n",
    "        opt = Adamax(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "            # tf.keras.metrics.Precision(name='prec'),\n",
    "            # tf.keras.metrics.Recall(name='rec'),\n",
    "            # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "            # tf.keras.metrics.TruePositives(name='TP'),\n",
    "            # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "        ]\n",
    "\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=opt, \n",
    "            metrics=metrics)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a1d057b-e162-4fb8-8315-80ad735ea880",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 02m 56s]\n",
      "val_auc: 0.7928877472877502\n",
      "\n",
      "Best val_auc So Far: 0.7972393035888672\n",
      "Total elapsed time: 01h 08m 16s\n",
      "--- Training time: 4096.979145526886 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = MyHyperModel_optParameter()\n",
    "    tuner = keras_tuner.Hyperband(\n",
    "        hypermodel= model,\n",
    "        objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "        max_epochs=25,\n",
    "        hyperband_iterations=1,\n",
    "        overwrite=True,\n",
    "        directory='../SeqModel/tuner/',\n",
    "        project_name=\"optimizerParameterLSTM\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=3, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "    tuner.search([Xt_train_search, Xclin_train_search, Xther_train_search], y_train_search, \n",
    "                 validation_data=([Xt_val_search, Xclin_val_search, Xther_val_search], y_val_search),\n",
    "                 epochs=10, \n",
    "                 batch_size = 128,\n",
    "                 class_weight = class_weight,\n",
    "                 callbacks = [earlyStopping])\n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33da05-39e7-4cbc-9797-93f0cef43566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ../SeqModel/tuner/optimizerParameterLSTM\n",
      "Showing 3 best trials\n",
      "Objective(name=\"val_auc\", direction=\"max\")\n",
      "\n",
      "Trial 0017 summary\n",
      "Hyperparameters:\n",
      "lr: 0.001\n",
      "clipvalue: 0.5\n",
      "beta_1: 0.9\n",
      "beta_2: 0.9440000000000001\n",
      "epsilon: 1e-08\n",
      "weight_decay: 0.010825981842160782\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 9\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0013\n",
      "Score: 0.7972393035888672\n",
      "\n",
      "Trial 0029 summary\n",
      "Hyperparameters:\n",
      "lr: 0.0005\n",
      "clipvalue: 0.1\n",
      "beta_1: 0.8\n",
      "beta_2: 0.9\n",
      "epsilon: 1e-07\n",
      "weight_decay: 0.03154803686642354\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.7928877472877502\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "lr: 0.0005\n",
      "clipvalue: 0.1\n",
      "beta_1: 0.8500000000000001\n",
      "beta_2: 0.966\n",
      "epsilon: 1e-07\n",
      "weight_decay: 0.047804048897018586\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 9\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0015\n",
      "Score: 0.792451798915863\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97c0ae-ed61-4f64-898c-263923b55e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae28316-d9e8-4680-a5c6-e8522b5e8ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12cf3d-74a5-4f75-8493-605a48ba742e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbddf77-c941-48d2-9bbd-b80b77d85e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auprc'])\n",
    "plt.plot(history.history['val_auprc'])\n",
    "plt.title('model auprc')\n",
    "# plt.ylim(0.3, 1)\n",
    "plt.ylabel('auprc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5ec0a-dc15-4a6b-ab40-393708382e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5a989-8735-4b2e-955d-c4c1852626d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.evaluate([Xt_eval, Xs_clin_eval[:,:max_codes_clin], Xs_ther_eval[:,:max_codes_ther]], y_eval)\n",
    "    model.evaluate([Xt_test, Xs_clin_test[:,:max_codes_clin], Xs_ther_test[:,:max_codes_ther]], y_test)\n",
    "    model.evaluate([Xt_testWales, Xs_clin_testWales[:,:max_codes_clin], Xs_ther_testWales[:,:max_codes_ther]], y_testWales)\n",
    "    model.evaluate([Xt_testScotland, Xs_clin_testScotland[:,:max_codes_clin], Xs_ther_testScotland[:,:max_codes_ther]], y_testScotland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8ec53-d208-499f-8ff5-87d475065fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict([Xt_test, Xs_clin_test[:,:max_codes_clin], Xs_ther_test[:,:max_codes_ther]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50569eb6-1ac3-40f2-9c84-da785fdaff72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summariseResultWithThreshold (Xt_test, Xs_clin_test, Xs_ther_test, testY, model):\n",
    "    preds = model.predict([Xt_test, Xs_clin_test, Xs_ther_test])\n",
    "    # tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    # specificity = tn / (tn+fp)\n",
    "    # sensitivity = tp / (tp+fn)\n",
    "    # ppv = 100*tp/(tp+fp)\n",
    "    # npv = 100*tn/(fn+tn)\n",
    "    # acc = accuracy_score(testY, preds)\n",
    "    # f1score = f1_score(testY, preds, average = 'binary')\n",
    "    # balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    # aucscore = auc(fpr, tpr)\n",
    "    aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(aucscore,4), np.round(auprc,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a6da4-5e73-4a66-9597-c4239873e2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(summariseResultWithThreshold(Xt_eval, Xs_clin_eval[:,:max_codes_clin], Xs_ther_eval[:,:max_codes_ther], y_eval, model))\n",
    "print(summariseResultWithThreshold(Xt_test, Xs_clin_test[:,:max_codes_clin], Xs_ther_test[:,:max_codes_ther], y_test, model))\n",
    "print(summariseResultWithThreshold(Xt_testWales, Xs_clin_testWales[:,:max_codes_clin], Xs_ther_testWales[:,:max_codes_ther], y_testWales, model))\n",
    "print(summariseResultWithThreshold(Xt_testScotland, Xs_clin_testScotland[:,:max_codes_clin], Xs_ther_testScotland[:,:max_codes_ther], y_testScotland, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeed10f-72a8-457d-bfcb-f2662e74bfc6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # create the model\n",
    "# embedding_vector_length = 50\n",
    "# earlyStopping = EarlyStopping(monitor='val_auc', patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "# mcp_save = ModelCheckpoint('../SeqModel/seqModel_therapy.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_codes))\n",
    "#     model.add(LSTM(128, return_sequences=True, kernel_regularizer=L1L2(l1=0.02, l2=0.03)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(LSTM(64,  kernel_regularizer=L1L2(l1=0.02, l2=0.03)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(32, activation=LeakyReLU(alpha=.3), kernel_regularizer=L1L2(l1=0.02, l2=0.03)))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     opt = Adadelta(learning_rate=5e-3, clipvalue=0.3)\n",
    "#     metrics = [\n",
    "#         AUC(num_thresholds=3, name='auc'),\n",
    "#     ]\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "#     print(model.summary())\n",
    "#     history = model.fit(Xs_train, y_train, validation_data=(Xs_val, y_val), epochs=30, batch_size=128, class_weight = class_weight, callbacks = [earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943f759-cf30-4366-b7c8-bf33fbb73101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def earlyFussion():\n",
    "       \n",
    "    inputs1 = Input(shape=tab_feature_size)\n",
    "    inputs2 = Input(shape=max_codes)\n",
    "    inputs3 = Input(shape=max_codes)\n",
    "    \n",
    "    \n",
    "    #clinical embedding for lstm\n",
    "    embedding = Embedding(vocab_size, 50, input_length=max_codes)(inputs2)\n",
    "    \n",
    "    #month embedding for lstm\n",
    "    embedding_month = Embedding(month_size, 7, input_length=max_codes)(inputs3)\n",
    "    \n",
    "    nn = Dense(32, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1))(inputs1)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    lstmClinical = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(embedding)\n",
    "    lstmMonth = Bidirectional(LSTM(units=16, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(embedding_month)\n",
    "    # lstm = Add()([lstmClinical, lstmMonth])\n",
    "    lstm = lstmClinical\n",
    "    \n",
    "    # nn = Reshape((1, 32))(nn)\n",
    "    # add = concatenate([nn, lstm], axis=1)\n",
    "    nn = Dense(16, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1))(nn)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    lstm = Bidirectional(LSTM(units=8, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(lstm)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    # # nn = Reshape((301, 64))(nn)\n",
    "    # add = concatenate([nn, lstm], axis=1)\n",
    "    nn = Dense(16, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=L1L2(l1=0.0, l2=0.1))(nn)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    lstm = Bidirectional(LSTM(units=8, return_sequences=True, kernel_regularizer=L1L2(l1=0.0, l2=0.1)))(lstm)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    nn = Reshape((1, 16))(nn)\n",
    "    model_tot = concatenate([nn, lstm], axis=1)\n",
    "    # model_tot = BatchNormalization()(model_tot)\n",
    "\n",
    "    model_tot = Dense(units=8, activation=LeakyReLU())(model_tot)\n",
    "    nn = Dropout(0.5)(nn)\n",
    "    \n",
    "    model_tot = Flatten()(model_tot)\n",
    "    output = Dense(1, activation='sigmoid')(model_tot)\n",
    "    \n",
    "    opt = RMSprop(learning_rate=1e-4, clipvalue=.5)\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=3, name='auc', curve='ROC'),\n",
    "        AUC(num_thresholds=3, name='auprc', curve='PR'),\n",
    "        tf.keras.metrics.Precision(name='prec'),\n",
    "        tf.keras.metrics.Recall(name='rec'),\n",
    "        tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "        tf.keras.metrics.TruePositives(name='TP'),\n",
    "        tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "    ]\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer=opt, \n",
    "        metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c11344-dfa9-40e6-b2d2-63388d84ba09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.evaluate([Xt_eval, Xs_eval[:,:max_codes], Xm_eval[:,:max_codes]], y_eval)\n",
    "    model.evaluate([Xt_test, Xs_test[:,:max_codes], Xm_test[:,:max_codes]], y_test)\n",
    "    # model.evaluate(X_testWales, y_testWales)\n",
    "    # model.evaluate(X_testScotland, y_testScotland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fd35d-2903-48e4-a655-b55f16b99da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model evaluation function\n",
    "def summariseResult (testY, preds):\n",
    "    tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    ppv = 100*tp/(tp+fp)\n",
    "    npv = 100*tn/(fn+tn)\n",
    "    acc = accuracy_score(testY, preds)\n",
    "    f1score = f1_score(testY, preds, average = 'binary')\n",
    "    balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    aucscore = auc(fpr, tpr)\n",
    "    # aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(acc,4), np.round(specificity,4), np.round(sensitivity,4), np.round(aucscore,4), np.round(auprc,4), np.round(balanceacc,4), np.round(f1score,4), np.round(ppv,4), np.round(npv,4)\n",
    "\n",
    "data_test_Xs = [X_eval, X_test, X_testWales, X_testScotland]\n",
    "data_test_ys = [y_eval, y_test, y_testWales, y_testScotland]\n",
    "for data_test_X, data_test_y in zip(data_test_Xs, data_test_ys):\n",
    "    with tf.device('/CPU:0'):\n",
    "        preds = model.predict(data_test_X)\n",
    "    preds = [0 if pred <0.5 else 1 for pred in preds]\n",
    "    print(summariseResult(data_test_y, np.squeeze(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a50962-2f80-4931-8eb7-df84792130ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('../SeqModel/model_therapy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1adb28e-44a2-4303-8b79-25348bfcaad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# a = load_model('../SeqModel/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f42bb-89f7-4693-852f-ae329cc7afe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
