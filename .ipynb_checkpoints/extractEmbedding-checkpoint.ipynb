{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cdacfb-c3ba-40e2-9706-cb6d6568f42a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 11:52:10.448932: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-27 11:52:10.775017: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-27 11:52:10.775052: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-27 11:52:10.776719: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-27 11:52:10.928597: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Input, concatenate, Reshape, Activation, Flatten, Add, BatchNormalization, Multiply, LeakyReLU\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.metrics import AUC, SensitivityAtSpecificity\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, Adamax, SGD, Adadelta\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import L1L2, L1, L2\n",
    "# from livelossplot import PlotLossesKeras\n",
    "#internal validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score, matthews_corrcoef, auc, average_precision_score, roc_auc_score, balanced_accuracy_score, roc_curve, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c0da5b-4ed0-4eec-9e47-7f742e1b8f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features size:  67\n",
      "['sex', 'age', 'CharlsonScore', 'BTS_step', 'average_daily_dose_ICS', 'prescribed_daily_dose_ICS', 'ICS_medication_possesion_ratio', 'numOCS', 'PriorEducation', 'numPCS', 'numPCSAsthma', 'numAntibioticsEvents', 'numAntibioticswithLRTI', 'numOCSwithLRTI', 'numAsthmaAttacks', 'numAcuteRespEvents', 'numHospEvents', 'rhinitis', 'cardiovascular', 'heartfailure', 'psoriasis', 'anaphylaxis', 'diabetes', 'ihd', 'anxiety', 'eczema', 'nasalpolyps', 'ethnic_group_Asian - ethnic group', 'ethnic_group_Black - ethnic group', 'ethnic_group_Mixed ethnic census group', 'ethnic_group_Other ethnic group', 'ethnic_group_White - ethnic group', 'ethnic_group_not_recorded', 'smokingStatus_Active Smoker', 'smokingStatus_Former Smoker', 'smokingStatus_Non Smoker', 'DeviceType_BAI', 'DeviceType_DPI', 'DeviceType_NEB', 'DeviceType_pMDI', 'DeviceType_unknown', 'cat_BMI_normal', 'cat_BMI_not recorded', 'cat_BMI_obese', 'cat_BMI_overweight', 'cat_BMI_underweight', 'imd_decile_0', 'imd_decile_1', 'imd_decile_2', 'imd_decile_3', 'imd_decile_4', 'imd_decile_5', 'imd_decile_6', 'imd_decile_7', 'imd_decile_8', 'imd_decile_9', 'imd_decile_10', 'PEFStatus_60-80', 'PEFStatus_less than 60', 'PEFStatus_more than 80', 'PEFStatus_not_recorded', 'EosinophilLevel_high', 'EosinophilLevel_normal', 'EosinophilLevel_unknown', 'system_EMIS', 'system_SystemOne', 'system_Vision']\n",
      "75810\n",
      "14564\n",
      "13\n",
      "(280911, 67)\n",
      "(70212, 67)\n",
      "(87777, 67)\n",
      "(21180, 67)\n",
      "(13343, 67)\n",
      "(7837, 67)\n"
     ]
    }
   ],
   "source": [
    "#################################### RETRAIN ################################################################################################################################################\n",
    "trainingData, validationData, internalEvaluationData, evaluationData, evaluationDataWales, evaluationDataScotland = pickle.load(open('../FinalData/dataset_scaled_2vs1_09122023.sav', 'rb'))\n",
    "\n",
    "trainingData = trainingData[(trainingData.age >=8) & (trainingData.age <=80)]\n",
    "validationData = validationData[(validationData.age >=8) & (validationData.age <=80)]\n",
    "internalEvaluationData = internalEvaluationData[(internalEvaluationData.age >=8) & (internalEvaluationData.age <=80)]\n",
    "evaluationData = evaluationData[(evaluationData.age >=8) & (evaluationData.age <=80)]\n",
    "evaluationDataWales = evaluationDataWales[(evaluationDataWales.age >=8) & (evaluationDataWales.age <=80)]\n",
    "evaluationDataScotland = evaluationDataScotland[(evaluationDataScotland.age >=8) & (evaluationDataScotland.age <=80)]\n",
    "\n",
    "\n",
    "trainingData = trainingData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "validationData = validationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "internalEvaluationData = internalEvaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationData = evaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataWales = evaluationDataWales.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataScotland = evaluationDataScotland.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "\n",
    "\n",
    "#Define feature candidates\n",
    "features_columns = trainingData.columns.to_list()\n",
    "exclude_columns = ['patid', 'practice_id', #identifier\n",
    "                   'BMI', #use the categorical instead\n",
    "                   'ethnicity', #use ethnic_group instead\n",
    "                   'Spacer',  #all zero\n",
    "                   \n",
    "                   'outcome_3months', 'outcome_6months', 'outcome_9months', 'outcome_12months', 'outcome_15months', 'outcome_18months', \n",
    "                   'outcome_21months', 'outcome_24months', 'outcome_combined_6months', 'outcome_combined_9months', 'outcome_combined_12months', \n",
    "                   'outcome_combined_15months', 'outcome_combined_18months', 'outcome_combined_24months', '3months', '6months', '9months', '12months', '24months', #outcomes variable\n",
    "                   \n",
    "                   'postcode_district', 'County', 'LocalAuthority', 'OutputAreaClassification', #location related variables, use IMD decile only\n",
    "                   \n",
    "                   'cat_age', 'cat_average_daily_dose_ICS', 'cat_prescribed_daily_dose_ICS', 'cat_ICS_medication_possesion_ratio', 'cat_numOCS', 'cat_numOCSEvents', \n",
    "                   'cat_numOCSwithLRTI', 'cat_numAcuteRespEvents', 'cat_numAntibioticsEvents', 'cat_numAntibioticswithLRTI', 'cat_numAsthmaAttacks', 'cat_numHospEvents', \n",
    "                   'cat_numPCS', 'cat_numPCSAsthma', #use continous vars instead\n",
    "                   \n",
    "                   'count_rhinitis', 'count_cardiovascular', 'count_heartfailure',\n",
    "                   'count_psoriasis', 'count_anaphylaxis', 'count_diabetes', 'count_ihd',\n",
    "                   'count_anxiety', 'count_eczema', 'count_nasalpolyps',\n",
    "                   'count_paracetamol', 'count_nsaids', 'count_betablocker', #use binary ones\n",
    "                   \n",
    "                   'paracetamol', 'nsaids', 'betablocker', #no data in evaluation\n",
    "                   \n",
    "                   'numOCSEvents', #duplicate with numOCS\n",
    "                   \n",
    "                   'month_12', 'month_4', 'month_5', 'month_10', 'month_1', 'month_6', 'month_3', \n",
    "                   'month_11', 'month_8', 'month_9', 'month_7', 'month_2', #month of attacks\n",
    "                   \n",
    "                   # 'system_EMIS', 'system_SystemOne', 'system_Vision', #primary care system used\n",
    "                  ]\n",
    "exclude_columns = exclude_columns + [x for x in features_columns if '_count' in x] #filter out commorbid count variables\n",
    "features_columns = [x for x in features_columns if x not in exclude_columns]\n",
    "print('Features size: ', len(features_columns))\n",
    "print(features_columns)\n",
    "\n",
    "\n",
    "#load sequence\n",
    "clinical = pd.read_feather('../SeqModel/all_data_clinical.feather')\n",
    "therapy = pd.read_feather('../SeqModel/all_data_therapy.feather')\n",
    "seqCols = ['patid',\n",
    "       'read_code_seq_padded_end_idx_clin',\n",
    "       'month_padded_idx_end_clin',\n",
    "       'read_code_seq_padded_end_idx_ther',\n",
    "       'month_padded_idx_end_ther']\n",
    "sequence_data = clinical.merge(therapy[['patid', 'read_code_seq_padded_idx', 'read_code_seq_padded_end_idx',\n",
    "       'month_padded_idx', 'month_padded_idx_end']], on='patid', suffixes=['_clin', '_ther'], how='inner')\n",
    "\n",
    "trainingData = trainingData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "validationData = validationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "internalEvaluationData = internalEvaluationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationData = evaluationData.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationDataWales = evaluationDataWales.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "evaluationDataScotland = evaluationDataScotland.merge(sequence_data[seqCols], on='patid', how='inner')\n",
    "\n",
    "#vocab\n",
    "code2idx_clin = pickle.load(open('../SeqModel/all_vocab_clinical.sav', 'rb'))\n",
    "code2idx_ther = pickle.load(open('../SeqModel/all_vocab_therapy.sav', 'rb'))\n",
    "month2idx = pickle.load(open('../SeqModel/all_vocab_month.sav', 'rb'))\n",
    "vocab_size_clinical = len(code2idx_clin)+1\n",
    "vocab_size_therapy = len(code2idx_ther)+1\n",
    "month_size = len(month2idx)+1\n",
    "print(vocab_size_clinical)\n",
    "print(vocab_size_therapy)\n",
    "print(month_size)\n",
    "\n",
    "Xt_train = np.array(trainingData[features_columns].values)\n",
    "Xt_val = np.array(validationData[features_columns].values)\n",
    "Xt_internaleval = np.array(internalEvaluationData[features_columns].values)\n",
    "Xt_eval = np.array(evaluationData[features_columns].values)\n",
    "Xt_eval_Wales = np.array(evaluationDataWales[features_columns].values)\n",
    "Xt_eval_Scotland = np.array(evaluationDataScotland[features_columns].values)\n",
    "\n",
    "#scalling tabular data\n",
    "scaler = StandardScaler().fit(Xt_train)\n",
    "Xt_train = scaler.transform(Xt_train)\n",
    "Xt_val = scaler.transform(Xt_val)\n",
    "Xt_internaleval = scaler.transform(Xt_internaleval)\n",
    "Xt_eval = scaler.transform(Xt_eval)\n",
    "Xt_eval_Wales = scaler.transform(Xt_eval_Wales)\n",
    "Xt_eval_Scotland = scaler.transform(Xt_eval_Scotland)\n",
    "\n",
    "Xclin_train = np.array(trainingData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_val = np.array(validationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_internaleval = np.array(internalEvaluationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval = np.array(evaluationData['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval_Wales = np.array(evaluationDataWales['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_eval_Scotland = np.array(evaluationDataScotland['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_train = np.array([x for x in Xclin_train])\n",
    "Xclin_val = np.array([x for x in Xclin_val])\n",
    "Xclin_internaleval = np.array([x for x in Xclin_internaleval])\n",
    "Xclin_eval = np.array([x for x in Xclin_eval])\n",
    "Xclin_eval_Wales = np.array([x for x in Xclin_eval_Wales])\n",
    "Xclin_eval_Scotland = np.array([x for x in Xclin_eval_Scotland])\n",
    "\n",
    "Xther_train = np.array(trainingData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_val = np.array(validationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_internaleval = np.array(internalEvaluationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval = np.array(evaluationData['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval_Wales = np.array(evaluationDataWales['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_eval_Scotland = np.array(evaluationDataScotland['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_train = np.array([x for x in Xther_train])\n",
    "Xther_val = np.array([x for x in Xther_val])\n",
    "Xther_internaleval = np.array([x for x in Xther_internaleval])\n",
    "Xther_eval = np.array([x for x in Xther_eval])\n",
    "Xther_eval_Wales = np.array([x for x in Xther_eval_Wales])\n",
    "Xther_eval_Scotland = np.array([x for x in Xther_eval_Scotland])\n",
    "\n",
    "\n",
    "print(Xt_train.shape)\n",
    "print(Xt_internaleval.shape)\n",
    "print(Xt_val.shape)\n",
    "print(Xt_eval.shape)\n",
    "print(Xt_eval_Wales.shape)\n",
    "print(Xt_eval_Scotland.shape)\n",
    "\n",
    "# target_outcomes = ['3months', '6months', '12months', '24months'] \n",
    "target_outcome = '12months'\n",
    "\n",
    "\n",
    "y_train = trainingData[target_outcome].values\n",
    "y_val = validationData[target_outcome].values\n",
    "y_internaleval = internalEvaluationData[target_outcome].values\n",
    "y_eval = evaluationData[target_outcome].values\n",
    "y_eval_Wales = evaluationDataWales[target_outcome].values\n",
    "y_eval_Scotland = evaluationDataScotland[target_outcome].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "868eb4b8-8888-4b48-83cf-e8640600b00d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neuron_units': 96,\n",
       " 'kernel_initializer_layer0': 'glorot_uniform',\n",
       " 'kernel_l1_dense0': 0.08,\n",
       " 'kernel_l2_dense0': 0.08,\n",
       " 'bias_l1_dense0': 0.02,\n",
       " 'bias_l2_dense0': 0.08,\n",
       " 'act_l1_dense0': 0.02,\n",
       " 'act_l2_dense0': 0.04,\n",
       " 'activation0': 'relu',\n",
       " 'rate0': 0.30000000000000004,\n",
       " 'embedding_size': 49,\n",
       " 'neuron_lstm_units': 96,\n",
       " 'kernel_l1_lstm1': 0.03539560759156795,\n",
       " 'kernel_l2_lstmTherapy': 0.02679857097199656,\n",
       " 'bias_l1_lstm1': 0.02964276553164017,\n",
       " 'bias_l2_lstmTherapy': 0.07197468824652294,\n",
       " 'act_l1_lstm1': 0.0543408274672415,\n",
       " 'act_l2_lstmTherapy': 0.04642310363956867,\n",
       " 'rec_l1_lstm1': 0.03409090790250019,\n",
       " 'rec_l2_lstmTherapy': 0.01270751815766289,\n",
       " 'rate_lstm1': 0.263933256026073,\n",
       " 'kernel_l1_lstm2': 0.06764991758989787,\n",
       " 'bias_l1_lstm2': 0.003912349530834402,\n",
       " 'act_l1_lstm2': 0.025818314921877986,\n",
       " 'rec_l1_lstm2': 0.064583858379006,\n",
       " 'rate_lstm2': 0.2278589083435948,\n",
       " 'units_layer1': 32,\n",
       " 'kernel_initializer_layer1': 'lecun_uniform',\n",
       " 'kernel_l1_dense1': 0.08,\n",
       " 'kernel_l2_dense1': 0.08,\n",
       " 'bias_l1_dense1': 0.0,\n",
       " 'bias_l2_dense1': 0.1,\n",
       " 'act_l1_dense1': 0.0,\n",
       " 'act_l2_dense1': 0.02,\n",
       " 'activation': 'relu',\n",
       " 'lr': 0.001,\n",
       " 'clipvalue': 0.1,\n",
       " 'beta_1': 0.8500000000000001,\n",
       " 'beta_2': 0.966,\n",
       " 'epsilon': 1e-07,\n",
       " 'weight_decay': 0.004949474929145093}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5caf9a0a-9846-40bf-b987-cc973e795971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_codes_clin = Xclin_train.shape[1]\n",
    "max_codes_ther = Xther_train.shape[1]\n",
    "max_codes = 100\n",
    "tab_feature_size = Xt_train.shape[1]\n",
    "top_vocabs_portion = .1\n",
    "\n",
    "def buildmodel():\n",
    "    #tabular dara - demography   \n",
    "    inputs1 = Input(shape=tab_feature_size)\n",
    "    neurons_layer0 = hp.Int('neuron_units', min_value=32, max_value=128, step=32)\n",
    "    neurons_layer1 = hp.Int('neuron_units', min_value=32, max_value=128, step=32)\n",
    "    nn = Dense(units=96, \n",
    "                kernel_initializer= \"glorot_uniform\", \n",
    "                kernel_regularizer=L1L2(l1=0.08, \n",
    "                                        l2=0.08\n",
    "                                       ),\n",
    "                bias_regularizer=L1L2(l1=.02, \n",
    "                                        l2=.08\n",
    "                                       ),\n",
    "                activity_regularizer=L1L2(l1=.02, \n",
    "                                        l2=.04\n",
    "                                       ),\n",
    "                input_shape = (tab_feature_size,)\n",
    "               )(inputs1)\n",
    "    nn = BatchNormalization()(nn)\n",
    "    nn = Activation(\"relu\")(nn)\n",
    "    nn = Dropout(rate=.3)(nn)\n",
    "\n",
    "\n",
    "    #==================================================================================================================================================#\n",
    "\n",
    "    #clinical embedding for lstm\n",
    "    inputs2 = Input(shape=max_codes)\n",
    "    embedding_clin = Embedding(int(top_vocabs_portion*vocab_size_clinical), \n",
    "                               output_dim = 49, \n",
    "                               input_length=max_codes,\n",
    "                               mask_zero=True,\n",
    "                              )(inputs2)\n",
    "\n",
    "\n",
    "\n",
    "    #therapy embedding for lstm\n",
    "    inputs3 = Input(shape=max_codes)\n",
    "    embedding_ther = Embedding(int(top_vocabs_portion*vocab_size_therapy), \n",
    "                               output_dim = 49, \n",
    "                               input_length=max_codes,\n",
    "                               mask_zero=True,\n",
    "                         )(inputs3)\n",
    "\n",
    "\n",
    "    ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "    allEmbedding = Add()([embedding_clin, embedding_ther])\n",
    "    # allEmbedding = concatenate([embedding_clin, embedding_ther], axis=1)\n",
    "#######################################################################################################################################################################\n",
    "\n",
    "    ###layer 2 - LSTM to the final product\n",
    "    for i in range (2):\n",
    "        lstm = Bidirectional(LSTM(units=96, \n",
    "                            return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=0.03539560759156795, \n",
    "                                           l2=0.02679857097199656\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=0.02964276553164017, \n",
    "                                           l2=0.07197468824652294\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=0.0543408274672415, \n",
    "                                           l2=0.04642310363956867\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=0.03409090790250019, \n",
    "                                           l2=0.01270751815766289\n",
    "                                          ),\n",
    "                                 )\n",
    "                            )(allEmbedding)\n",
    "        lstm = Dropout(rate=0.263933256026073)(lstm)\n",
    "\n",
    "\n",
    "    lstm = Bidirectional(LSTM(units=int(96/2), \n",
    "                        # return_sequences=True, \n",
    "                        kernel_regularizer=L1L2(l1=0.06764991758989787, \n",
    "                                       l2=0.02679857097199656\n",
    "                                      ),\n",
    "                        bias_regularizer=L1L2(l1=0.003912349530834402, \n",
    "                                       l2=0.07197468824652294\n",
    "                                      ),\n",
    "                        activity_regularizer=L1L2(l1=0.025818314921877986, \n",
    "                                       l2=0.04642310363956867\n",
    "                                      ),\n",
    "                        recurrent_regularizer=L1L2(l1=0.064583858379006, \n",
    "                                       l2=0.01270751815766289\n",
    "                                      ),\n",
    "                                     )\n",
    "                                )(lstm)\n",
    "    lstm = Dropout(rate=0.2278589083435948)(lstm)\n",
    "\n",
    "\n",
    "###################################################################################################################################################        \n",
    "\n",
    "    #merge tabular and sequence layers\n",
    "    # nn = Reshape((1, neurons_layer1))(nn)\n",
    "    add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "    ##layer 4 - FCN before classification layer\n",
    "    final = Dense(units=32, \n",
    "                    kernel_initializer=\"lecun_uniform\", \n",
    "                    kernel_regularizer=L1L2(l1=.08, \n",
    "                                            l2=.08\n",
    "                                           ),\n",
    "                    bias_regularizer=L1L2(l1=0, \n",
    "                                            l2=.1\n",
    "                                           ),\n",
    "                    activity_regularizer=L1L2(l1=0, \n",
    "                                            l2=.02\n",
    "                                           ),\n",
    "                 )(add)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation(\"relu\")(final)\n",
    "    final = Dropout(0.4)(final)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###layer 5 - classification layer\n",
    "    # final = Flatten()(add)\n",
    "    output = Dense(1, activation='sigmoid')(final)\n",
    "\n",
    "    lr= 0.001\n",
    "    clipvalue= 0.1\n",
    "    beta_1= 0.8500000000000001\n",
    "    beta_2= 0.966\n",
    "    epsilon= 1e-07\n",
    "    weight_decay= 0.004949474929145093 \n",
    "\n",
    "    opt = Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "    # opt = RMSprop(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "        AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "        # tf.keras.metrics.Precision(name='prec'),\n",
    "        # tf.keras.metrics.Recall(name='rec'),\n",
    "        # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "        # tf.keras.metrics.TruePositives(name='TP'),\n",
    "        # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "    ]\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer=opt, \n",
    "        metrics=metrics)\n",
    "    print(model.summary())\n",
    "    earlyStopping = EarlyStopping(monitor='val_auc', patience=20, verbose=0, mode='max', restore_best_weights=True)\n",
    "    pos_weight = trainingData[target_outcomes].value_counts()[0]/trainingData[target_outcomes].value_counts()[1]\n",
    "    class_weight = {0:1, 1:pos_weight}\n",
    "    history = model.fit([Xt_train, Xclin_train[:,:max_codes], Xther_train[:,:max_codes]], y_train, \n",
    "                 validation_data=([Xt_val, Xclin_val[:,:max_codes], Xther_val[:,:max_codes]], y_val),\n",
    "                          epochs=200, batch_size=128, class_weight=class_weight, callbacks = [earlyStopping])\n",
    "    \n",
    "    return model, embedding_clin, embedding_ther, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30393ed9-13c7-4fd7-b735-3d9578abad6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 100, 49)              371469    ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)     (None, 100, 49)              71344     ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 67)]                 0         []                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 100, 49)              0         ['embedding_4[0][0]',         \n",
      "                                                                     'embedding_5[0][0]']         \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 96)                   6528      ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_7 (Bidirecti  (None, 100, 192)             112128    ['add_2[0][0]']               \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 96)                   384       ['dense_6[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 100, 192)             0         ['bidirectional_7[0][0]']     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 96)                   0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " bidirectional_8 (Bidirecti  (None, 96)                   92544     ['dropout_12[0][0]']          \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 96)                   0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 96)                   0         ['bidirectional_8[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 192)                  0         ['dropout_10[0][0]',          \n",
      " )                                                                   'dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 32)                   6176      ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 32)                   128       ['dense_7[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 32)                   0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 32)                   0         ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    33        ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 660734 (2.52 MB)\n",
      "Trainable params: 660478 (2.52 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 225, in _update_step_xla  *\n        return self._update_step(gradient, variable)\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 241, in _update_step  **\n        self.update_step(gradient, variable)\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/optimizers/adam.py\", line 178, in update_step\n        m.assign_add(-m * (1 - self.beta_1))\n\n    TypeError: unsupported operand type(s) for -: 'int' and 'tuple'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_533/2248862012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_clin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_ther\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_533/898600599.py\u001b[0m in \u001b[0;36mbuildmodel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mpos_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_outcomes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_outcomes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     history = model.fit([Xt_train, Xclin_train[:,:max_codes], Xther_train[:,:max_codes]], y_train, \n\u001b[0m\u001b[1;32m    158\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXt_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXclin_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_codes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXther_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_codes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                           epochs=200, batch_size=128, class_weight=class_weight, callbacks = [earlyStopping])\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mtf___update_step_xla\u001b[0;34m(self, gradient, variable, key)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 225, in _update_step_xla  *\n        return self._update_step(gradient, variable)\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 241, in _update_step  **\n        self.update_step(gradient, variable)\n    File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/keras/src/optimizers/adam.py\", line 178, in update_step\n        m.assign_add(-m * (1 - self.beta_1))\n\n    TypeError: unsupported operand type(s) for -: 'int' and 'tuple'\n"
     ]
    }
   ],
   "source": [
    "model, embedding_clin, embedding_ther, history = buildmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7da65f-55d6-4294-85f3-36639e3d5321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb93269e-00d6-4a39-b80e-a5ca921f4880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LOAD TUNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e303929-91ce-47f3-899b-8243b5904330",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22473, 67)\n",
      "(5619, 67)\n"
     ]
    }
   ],
   "source": [
    "# use only 20% of training data for parameter search\n",
    "target_outcomes = '12months'\n",
    "ignore, use = train_test_split(trainingData, stratify=trainingData[target_outcomes], test_size=0.10, random_state=1234)\n",
    "search_train, search_val = train_test_split(use, stratify=use[target_outcomes], test_size=0.2, random_state=1234)\n",
    "search_train.reset_index(inplace=True, drop=True)\n",
    "search_val.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "Xt_train_search = np.array(search_train[features_columns].values)\n",
    "Xt_val_search = np.array(search_val[features_columns].values)\n",
    "scaler = StandardScaler().fit(Xt_train_search)\n",
    "Xt_train_search = scaler.transform(Xt_train_search)\n",
    "Xt_val_search = scaler.transform(Xt_val_search)\n",
    "Xclin_train_search = np.array(search_train['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xclin_val_search = np.array(search_val['read_code_seq_padded_end_idx_clin'].values)\n",
    "Xther_train_search = np.array(search_train['read_code_seq_padded_end_idx_ther'].values)\n",
    "Xther_val_search = np.array(search_val['read_code_seq_padded_end_idx_ther'].values)\n",
    "\n",
    "Xclin_train_search = np.array([x for x in Xclin_train_search])\n",
    "Xclin_val_search = np.array([x for x in Xclin_val_search])\n",
    "Xther_train_search = np.array([x for x in Xther_train_search])\n",
    "Xther_val_search = np.array([x for x in Xther_val_search])\n",
    "\n",
    "y_train_search = search_train[target_outcomes].values\n",
    "y_val_search = search_val[target_outcomes].values\n",
    "\n",
    "print(Xt_train_search.shape)\n",
    "print(Xt_val_search.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7e32f3-6d85-42d6-8f9c-02d640167791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "max_codes_clin = Xclin_train_search.shape[1]\n",
    "max_codes_ther = Xther_train_search.shape[1]\n",
    "max_codes = 100\n",
    "tab_feature_size = Xt_train_search.shape[1]\n",
    "top_vocabs_portion = .05\n",
    "\n",
    "print(max_codes_clin)\n",
    "print(max_codes_ther)\n",
    "\n",
    "hp = keras_tuner.HyperParameters()\n",
    "class MyHyperModel_allParams(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        #tabular dara - demography   \n",
    "        inputs1 = Input(shape=tab_feature_size)\n",
    "        neurons_layer0 = hp.Int('neuron_units', min_value=32, max_value=128, step=32)\n",
    "        neurons_layer1 = hp.Int('neuron_units', min_value=32, max_value=128, step=32)\n",
    "        nn = Dense(units=neurons_layer1, \n",
    "                    kernel_initializer=hp.Choice(\"kernel_initializer_layer0\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "                    kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_dense0\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                            l2=hp.Float(\"kernel_l2_dense0\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                           ),\n",
    "                    bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_dense0\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                            l2=hp.Float(\"bias_l2_dense0\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                           ),\n",
    "                    activity_regularizer=L1L2(l1=hp.Float(\"act_l1_dense0\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                            l2=hp.Float(\"act_l2_dense0\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                           ),\n",
    "                    input_shape = (tab_feature_size,)\n",
    "                   )(inputs1)\n",
    "        nn = BatchNormalization()(nn)\n",
    "        nn = Activation(hp.Choice(\"activation0\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"]))(nn)\n",
    "        nn = Dropout(rate=hp.Float(\"rate0\", min_value=0.1, max_value=0.5, step=0.1))(nn)\n",
    "\n",
    "        \n",
    "        #==================================================================================================================================================#\n",
    "\n",
    "        #clinical embedding for lstm\n",
    "        inputs2 = Input(shape=max_codes)\n",
    "        embedding_size = hp.Int(\"embedding_size\", min_value=int(np.cbrt(vocab_size_clinical)), max_value=int(np.sqrt(vocab_size_therapy)))\n",
    "        embedding_clin = Embedding(int(top_vocabs_portion*vocab_size_clinical), \n",
    "                                   output_dim = embedding_size, \n",
    "                                   input_length=max_codes,\n",
    "                                   mask_zero=True,\n",
    "                                  )(inputs2)\n",
    "        \n",
    "\n",
    "\n",
    "        #therapy embedding for lstm\n",
    "        inputs3 = Input(shape=max_codes)\n",
    "        embedding_ther = Embedding(int(top_vocabs_portion*vocab_size_therapy), \n",
    "                                   output_dim = embedding_size, \n",
    "                                   input_length=max_codes,\n",
    "                                   mask_zero=True,\n",
    "                             )(inputs3)\n",
    "        \n",
    "\n",
    "        ###Layer 1 - merge add (clin+ther) and lstm ther\n",
    "        allEmbedding = Add()([embedding_clin, embedding_ther])\n",
    "        # allEmbedding = concatenate([embedding_clin, embedding_ther], axis=1)\n",
    "#######################################################################################################################################################################\n",
    "\n",
    "        ###layer 2 - LSTM to the final product\n",
    "        neuron_lstm_units = hp.Int('neuron_lstm_units', min_value=32, max_value=128, step=32)\n",
    "        \n",
    "        for i in range (2):\n",
    "            lstm = Bidirectional(LSTM(units=neuron_lstm_units, \n",
    "                                return_sequences=True, \n",
    "                                kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_lstm1\", min_value=0.0, max_value=0.1), \n",
    "                                               l2=hp.Float(\"kernel_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                              ),\n",
    "                                bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_lstm1\", min_value=0.0, max_value=0.1), \n",
    "                                               l2=hp.Float(\"bias_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                              ),\n",
    "                                activity_regularizer=L1L2(l1=hp.Float(\"act_l1_lstm1\", min_value=0.0, max_value=0.1), \n",
    "                                               l2=hp.Float(\"act_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                              ),\n",
    "                                recurrent_regularizer=L1L2(l1=hp.Float(\"rec_l1_lstm1\", min_value=0.0, max_value=0.1), \n",
    "                                               l2=hp.Float(\"rec_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                              ),\n",
    "                                     )\n",
    "                                )(allEmbedding)\n",
    "            lstm = Dropout(rate=hp.Float(\"rate_lstm1\", min_value=0.1, max_value=0.5))(lstm)\n",
    "        \n",
    "            \n",
    "        lstm = Bidirectional(LSTM(units=int(neurons_layer1/2), \n",
    "                            # return_sequences=True, \n",
    "                            kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_lstm2\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"kernel_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_lstm2\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"bias_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            activity_regularizer=L1L2(l1=hp.Float(\"act_l1_lstm2\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"act_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                            recurrent_regularizer=L1L2(l1=hp.Float(\"rec_l1_lstm2\", min_value=0.0, max_value=0.1), \n",
    "                                           l2=hp.Float(\"rec_l2_lstmTherapy\", min_value=0.0, max_value=0.1)\n",
    "                                          ),\n",
    "                                         )\n",
    "                                    )(lstm)\n",
    "        lstm = Dropout(rate=hp.Float(\"rate_lstm2\", min_value=0.1, max_value=0.5))(lstm)\n",
    "        \n",
    "        \n",
    "###################################################################################################################################################        \n",
    "\n",
    "        #merge tabular and sequence layers\n",
    "        # nn = Reshape((1, neurons_layer1))(nn)\n",
    "        add = concatenate([nn, lstm], axis=1)\n",
    "\n",
    "        ##layer 4 - FCN before classification layer\n",
    "        neurons_final_layer = hp.Int(\"units_layer1\", min_value=32, max_value=128, step=32)\n",
    "        final = Dense(units=neurons_final_layer, \n",
    "                        kernel_initializer=hp.Choice(\"kernel_initializer_layer1\", [\"glorot_uniform\", \"glorot_normal\", \"lecun_uniform\", \"lecun_normal\"]), \n",
    "                        kernel_regularizer=L1L2(l1=hp.Float(\"kernel_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"kernel_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                        bias_regularizer=L1L2(l1=hp.Float(\"bias_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"bias_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                        activity_regularizer=L1L2(l1=hp.Float(\"act_l1_dense1\", min_value=0.0, max_value=.1, step=0.02), \n",
    "                                                l2=hp.Float(\"act_l2_dense1\", min_value=0.0, max_value=.1, step=0.02)\n",
    "                                               ),\n",
    "                     )(add)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation(hp.Choice(\"activation\", [\"relu\", \"elu\", \"gelu\", \"silu\", \"selu\"]))(final)\n",
    "        final = Dropout(0.4)(final)     \n",
    "        \n",
    "                                                      \n",
    "\n",
    "\n",
    "        ###layer 5 - classification layer\n",
    "        # final = Flatten()(add)\n",
    "        output = Dense(1, activation='sigmoid')(final)\n",
    "        \n",
    "        lr=hp.Choice(\"lr\", [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]) \n",
    "        clipvalue=hp.Float(\"clipvalue\", min_value=0.1, max_value=0.7, step=0.2)\n",
    "        beta_1=hp.Float(\"beta_1\", min_value=0.8, max_value=0.95, step=0.05)\n",
    "        beta_2=hp.Float(\"beta_2\", min_value=0.900, max_value=0.999, step=0.011)\n",
    "        epsilon=hp.Choice(\"epsilon\", [1e-6, 1e-7, 1e-8]) \n",
    "        weight_decay=hp.Float(\"weight_decay\", min_value=0, max_value=0.1) \n",
    "\n",
    "        opt = Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "        # opt = RMSprop(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay)\n",
    "\n",
    "        metrics = [\n",
    "            AUC(num_thresholds=1000, name='auc', curve='ROC'),\n",
    "            AUC(num_thresholds=1000, name='auprc', curve='PR'),\n",
    "            # tf.keras.metrics.Precision(name='prec'),\n",
    "            # tf.keras.metrics.Recall(name='rec'),\n",
    "            # tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "            # tf.keras.metrics.TruePositives(name='TP'),\n",
    "            # tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "        ]\n",
    "\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=opt, \n",
    "            metrics=metrics)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43442c07-964d-42c9-821f-2ef1464ae5d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ../SeqModel/tuner/lstmv2.0AllParams1-bayesian-deeper-5%/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 11:57:48.713150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:48.749099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:48.749132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:48.750349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:48.750377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:48.750389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:49.929373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:49.929421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:49.929449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-27 11:57:49.929481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 11:57:49.929501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3413 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-12-27 11:57:50.312447: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 100, 49)              185710    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 100, 49)              35672     ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 67)]                 0         []                            \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 100, 49)              0         ['embedding[0][0]',           \n",
      "                                                                     'embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 96)                   6528      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 100, 192)             112128    ['add[0][0]']                 \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 96)                   384       ['dense[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 100, 192)             0         ['bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 96)                   0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  (None, 96)                   92544     ['dropout_2[0][0]']           \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 96)                   0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 96)                   0         ['bidirectional_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 192)                  0         ['dropout[0][0]',             \n",
      "                                                                     'dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   6176      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 32)                   128       ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 32)                   0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 32)                   0         ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    33        ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 439303 (1.68 MB)\n",
      "Trainable params: 439047 (1.67 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#LOAD TUNER\n",
    "model = MyHyperModel_allParams()\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel= model,\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=20,\n",
    "    overwrite=False,\n",
    "    seed = 1234,\n",
    "    directory='../SeqModel/tuner/',\n",
    "    project_name=\"lstmv2.0AllParams1-bayesian-deeper-5%\",\n",
    ")\n",
    "best_models = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f12afc-eab5-4ebc-ac79-e1d7b7033ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c06038d4-38d9-4a7e-8499-11ed66af8084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neuron_units': 96,\n",
       " 'kernel_initializer_layer0': 'glorot_uniform',\n",
       " 'kernel_l1_dense0': 0.08,\n",
       " 'kernel_l2_dense0': 0.08,\n",
       " 'bias_l1_dense0': 0.02,\n",
       " 'bias_l2_dense0': 0.08,\n",
       " 'act_l1_dense0': 0.02,\n",
       " 'act_l2_dense0': 0.04,\n",
       " 'activation0': 'relu',\n",
       " 'rate0': 0.30000000000000004,\n",
       " 'embedding_size': 49,\n",
       " 'neuron_lstm_units': 96,\n",
       " 'kernel_l1_lstm1': 0.03539560759156795,\n",
       " 'kernel_l2_lstmTherapy': 0.02679857097199656,\n",
       " 'bias_l1_lstm1': 0.02964276553164017,\n",
       " 'bias_l2_lstmTherapy': 0.07197468824652294,\n",
       " 'act_l1_lstm1': 0.0543408274672415,\n",
       " 'act_l2_lstmTherapy': 0.04642310363956867,\n",
       " 'rec_l1_lstm1': 0.03409090790250019,\n",
       " 'rec_l2_lstmTherapy': 0.01270751815766289,\n",
       " 'rate_lstm1': 0.263933256026073,\n",
       " 'kernel_l1_lstm2': 0.06764991758989787,\n",
       " 'bias_l1_lstm2': 0.003912349530834402,\n",
       " 'act_l1_lstm2': 0.025818314921877986,\n",
       " 'rec_l1_lstm2': 0.064583858379006,\n",
       " 'rate_lstm2': 0.2278589083435948,\n",
       " 'units_layer1': 32,\n",
       " 'kernel_initializer_layer1': 'lecun_uniform',\n",
       " 'kernel_l1_dense1': 0.08,\n",
       " 'kernel_l2_dense1': 0.08,\n",
       " 'bias_l1_dense1': 0.0,\n",
       " 'bias_l2_dense1': 0.1,\n",
       " 'act_l1_dense1': 0.0,\n",
       " 'act_l2_dense1': 0.02,\n",
       " 'activation': 'relu',\n",
       " 'lr': 0.001,\n",
       " 'clipvalue': 0.1,\n",
       " 'beta_1': 0.8500000000000001,\n",
       " 'beta_2': 0.966,\n",
       " 'epsilon': 1e-07,\n",
       " 'weight_decay': 0.004949474929145093}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
