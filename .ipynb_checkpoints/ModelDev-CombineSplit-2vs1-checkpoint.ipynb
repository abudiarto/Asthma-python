{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf617ae-7cd6-4168-9b2a-c61a35fcaf7c",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7287c-5c7e-4d25-9085-f3971b3d5784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#uncomment this below code to install imblearn package\n",
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95c2128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pyreadr\n",
    "\n",
    "#statistics\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "\n",
    "import cudf #gpu-powered DataFrame (Pandas alternative)\n",
    "\n",
    "#imbalance handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, RepeatedEditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "#internal validation\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV, PredefinedSplit, train_test_split\n",
    "\n",
    "#performance metrices\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score, matthews_corrcoef, auc, average_precision_score, roc_auc_score, balanced_accuracy_score, roc_curve, accuracy_score\n",
    "\n",
    "#Models selection\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from cuml.svm import SVC #gpu-powered SVM\n",
    "\n",
    "#Tree pruning\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "\n",
    "#save and load trained model\n",
    "import pickle\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2fa869-bb96-4ebc-9d51-2b542e9486e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "# trainingData = pd.read_csv('../FinalData/trainingDataEncoded_08102023.csv')\n",
    "# validationData = pd.read_csv('../FinalData/validationDataEncoded_08102023.csv')\n",
    "# evaluationData = pd.read_csv('../FinalData/evaluationDataEncoded_08102023.csv')\n",
    "# evaluationDataWales = pd.read_csv('../FinalData/evaluationDataWalesEncoded_08102023.csv')\n",
    "# evaluationDataScotland = pd.read_csv('../FinalData/evaluationDataScotlandEncoded_08102023.csv')\n",
    "\n",
    "trainingData, validationData, internalEvaluationData, evaluationData, evaluationDataWales, evaluationDataScotland = pickle.load(open('../FinalData/dataset_scaled_2vs1_09122023.sav', 'rb'))\n",
    "\n",
    "trainingData = trainingData[(trainingData.age >=8) & (trainingData.age <=80)]\n",
    "validationData = validationData[(validationData.age >=8) & (validationData.age <=80)]\n",
    "internalEvaluationData = internalEvaluationData[(internalEvaluationData.age >=8) & (internalEvaluationData.age <=80)]\n",
    "evaluationData = evaluationData[(evaluationData.age >=8) & (evaluationData.age <=80)]\n",
    "evaluationDataWales = evaluationDataWales[(evaluationDataWales.age >=8) & (evaluationDataWales.age <=80)]\n",
    "evaluationDataScotland = evaluationDataScotland[(evaluationDataScotland.age >=8) & (evaluationDataScotland.age <=80)]\n",
    "\n",
    "trainingData = trainingData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "validationData = validationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "internalEvaluationData = internalEvaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationData = evaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataWales = evaluationDataWales.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "evaluationDataScotland = evaluationDataScotland.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11624789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Data loader\n",
    "# trainingData = pd.read_csv('../FinalData/trainingDataEncoded_2vs1_16112023.csv')\n",
    "# validationData = pd.read_csv('../FinalData/validationDataEncoded_2vs1_16112023.csv')\n",
    "# internalEvaluationData = pd.read_csv('../FinalData/internalEvaluationDataEncoded_2vs1_16112023.csv')\n",
    "# evaluationData = pd.read_csv('../FinalData/evaluationDataEncoded_2vs1_16112023.csv')\n",
    "# evaluationDataWales = pd.read_csv('../FinalData/evaluationDataWalesEncoded_2vs1_16112023.csv')\n",
    "# evaluationDataScotland = pd.read_csv('../FinalData/evaluationDataScotlandEncoded_2vs1_16112023.csv')\n",
    "\n",
    "# trainingData = trainingData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "# validationData = validationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "# internalEvaluationData = internalEvaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "# evaluationData = evaluationData.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "# evaluationDataWales = evaluationDataWales.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)\n",
    "# evaluationDataScotland = evaluationDataScotland.rename({'3MonthsOutcome': '3months', '6MonthsOutcome': '6months','9MonthsOutcome': '9months','12MonthsOutcome': '12months',}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09186b49-193f-4c37-9430-4d8f01788601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "12months\n",
      "0    0.882398\n",
      "1    0.117602\n",
      "Name: 12months, dtype: float64\n",
      "0    0.882648\n",
      "1    0.117352\n",
      "Name: 12months, dtype: float64\n",
      "0    0.838714\n",
      "1    0.161286\n",
      "Name: 12months, dtype: float64\n",
      "0    0.843046\n",
      "1    0.156954\n",
      "Name: 12months, dtype: float64\n",
      "0    0.83103\n",
      "1    0.16897\n",
      "Name: 12months, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_outcomes = ['12months'] \n",
    "for target_outcome in target_outcomes:\n",
    "    print('#######################################################')\n",
    "    print(target_outcome)\n",
    "    print(trainingData[target_outcome].value_counts(normalize=True))\n",
    "    print(validationData[target_outcome].value_counts(normalize=True))\n",
    "    # print(internalEvaluationData[target_outcome].value_counts(normalize=True))\n",
    "    print(evaluationData[target_outcome].value_counts(normalize=True))\n",
    "    print(evaluationDataWales[target_outcome].value_counts(normalize=True))\n",
    "    print(evaluationDataScotland[target_outcome].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f7e171-4ac8-4a82-9aac-5c9e2128484c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features size:  67\n",
      "['sex', 'age', 'CharlsonScore', 'BTS_step', 'average_daily_dose_ICS', 'prescribed_daily_dose_ICS', 'ICS_medication_possesion_ratio', 'numOCS', 'PriorEducation', 'numPCS', 'numPCSAsthma', 'numAntibioticsEvents', 'numAntibioticswithLRTI', 'numOCSwithLRTI', 'numAsthmaAttacks', 'numAcuteRespEvents', 'numHospEvents', 'rhinitis', 'cardiovascular', 'heartfailure', 'psoriasis', 'anaphylaxis', 'diabetes', 'ihd', 'anxiety', 'eczema', 'nasalpolyps', 'ethnic_group_Asian - ethnic group', 'ethnic_group_Black - ethnic group', 'ethnic_group_Mixed ethnic census group', 'ethnic_group_Other ethnic group', 'ethnic_group_White - ethnic group', 'ethnic_group_not_recorded', 'smokingStatus_Active Smoker', 'smokingStatus_Former Smoker', 'smokingStatus_Non Smoker', 'DeviceType_BAI', 'DeviceType_DPI', 'DeviceType_NEB', 'DeviceType_pMDI', 'DeviceType_unknown', 'cat_BMI_normal', 'cat_BMI_not recorded', 'cat_BMI_obese', 'cat_BMI_overweight', 'cat_BMI_underweight', 'imd_decile_0', 'imd_decile_1', 'imd_decile_2', 'imd_decile_3', 'imd_decile_4', 'imd_decile_5', 'imd_decile_6', 'imd_decile_7', 'imd_decile_8', 'imd_decile_9', 'imd_decile_10', 'PEFStatus_60-80', 'PEFStatus_less than 60', 'PEFStatus_more than 80', 'PEFStatus_not_recorded', 'EosinophilLevel_high', 'EosinophilLevel_normal', 'EosinophilLevel_unknown', 'system_EMIS', 'system_SystemOne', 'system_Vision']\n"
     ]
    }
   ],
   "source": [
    "#Define feature candidates\n",
    "\n",
    "features_columns = trainingData.columns.to_list()\n",
    "exclude_columns = ['patid', 'practice_id', #identifier\n",
    "                   'BMI', #use the categorical instead\n",
    "                   'ethnicity', #use ethnic_group instead\n",
    "                   'Spacer',  #all zero\n",
    "                   \n",
    "                   'outcome_3months', 'outcome_6months', 'outcome_9months', 'outcome_12months', 'outcome_15months', 'outcome_18months', \n",
    "                   'outcome_21months', 'outcome_24months', 'outcome_combined_6months', 'outcome_combined_9months', 'outcome_combined_12months', \n",
    "                   'outcome_combined_15months', 'outcome_combined_18months', 'outcome_combined_24months', '3months', '6months', '9months', '12months', '24months', #outcomes variable\n",
    "                   \n",
    "                   'postcode_district', 'County', 'LocalAuthority', 'OutputAreaClassification', #location related variables, use IMD decile only\n",
    "                   \n",
    "                   'cat_age', 'cat_average_daily_dose_ICS', 'cat_prescribed_daily_dose_ICS', 'cat_ICS_medication_possesion_ratio', 'cat_numOCS', 'cat_numOCSEvents', \n",
    "                   'cat_numOCSwithLRTI', 'cat_numAcuteRespEvents', 'cat_numAntibioticsEvents', 'cat_numAntibioticswithLRTI', 'cat_numAsthmaAttacks', 'cat_numHospEvents', \n",
    "                   'cat_numPCS', 'cat_numPCSAsthma', #use continous vars instead\n",
    "                   \n",
    "                   'count_rhinitis', 'count_cardiovascular', 'count_heartfailure',\n",
    "                   'count_psoriasis', 'count_anaphylaxis', 'count_diabetes', 'count_ihd',\n",
    "                   'count_anxiety', 'count_eczema', 'count_nasalpolyps',\n",
    "                   'count_paracetamol', 'count_nsaids', 'count_betablocker', #use binary ones\n",
    "                   \n",
    "                   'paracetamol', 'nsaids', 'betablocker', #no data in evaluation\n",
    "                   \n",
    "                   'numOCSEvents', #duplicate with numOCS\n",
    "                   \n",
    "                   'month_12', 'month_4', 'month_5', 'month_10', 'month_1', 'month_6', 'month_3', \n",
    "                   'month_11', 'month_8', 'month_9', 'month_7', 'month_2', #month of attacks\n",
    "                   \n",
    "                   # 'system_EMIS', 'system_SystemOne', 'system_Vision', #primary care system used\n",
    "                  ]\n",
    "exclude_columns = exclude_columns + [x for x in features_columns if '_count' in x] #filter out commorbid count variables\n",
    "features_columns = [x for x in features_columns if x not in exclude_columns]\n",
    "print('Features size: ', len(features_columns))\n",
    "print(features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037f720-0ed4-4e4a-84c8-f511fadac736",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4db59da-1f72-428e-9203-08d62c6d9090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model evaluation function\n",
    "\n",
    "def summariseResult (testX, testY, model):\n",
    "    preds = model.predict_proba(testX)\n",
    "    preds = [x[1] for x in preds]\n",
    "    # tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    # specificity = tn / (tn+fp)\n",
    "    # sensitivity = tp / (tp+fn)\n",
    "    # ppv = 100*tp/(tp+fp)\n",
    "    # npv = 100*tn/(fn+tn)\n",
    "    # acc = accuracy_score(testY, preds)\n",
    "    # f1score = f1_score(testY, preds, average = 'binary')\n",
    "    # balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    # aucscore = auc(fpr, tpr)\n",
    "    aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(aucscore,4), np.round(auprc,4)\n",
    "    # return np.round(acc,4), np.round(specificity,4), np.round(sensitivity,4), np.round(aucscore,4), np.round(auprc,4), np.round(balanceacc,4), np.round(f1score,4), np.round(ppv,4), np.round(npv,4)\n",
    "\n",
    "#Fix model name for visualisation\n",
    "\n",
    "def modelNameFixer(x):\n",
    "    if 'liblinear' in x:\n",
    "        return 'Lasso'\n",
    "    elif 'GaussianNB' in x:\n",
    "        return 'GNB'\n",
    "    elif 'SVC' in x:\n",
    "        return 'SVC'\n",
    "    elif 'RandomForest' in x:\n",
    "        return 'RF'\n",
    "    elif 'XGB' in x:\n",
    "        return 'XGBoost'\n",
    "    elif 'DecisionTree' in x:\n",
    "        return 'DT'\n",
    "    else:\n",
    "        return 'LR'\n",
    "    \n",
    "    \n",
    "# instantiate the model (using the default parameters)\n",
    "def build_models (X_train, y_train, target_outcome, params_dict, model_fodler, fold):\n",
    "    models = [] #list to store all the models\n",
    "    print(\"Building models . . . .\")\n",
    "\n",
    "    #LR\n",
    "    model = 'LR'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    lr_model = LogisticRegression(class_weight='balanced', C = params['C'], max_iter=params['max_iter'], solver=params['solver'], random_state=1234)\n",
    "    lr_model.fit(X_train,y_train)\n",
    "    pickle.dump(lr_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '_2vs1.sav', 'wb'))\n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]]) \n",
    "    print(\"LR done\")\n",
    "\n",
    "    #Lasso\n",
    "    model = 'Lasso'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    lasso_model = LogisticRegression(class_weight='balanced',  C = params['C'], max_iter=params['max_iter'], penalty='l1', solver=params['solver'], random_state=1234) #only the LIBLINEAR and SAGA (added in v0.19) solvers handle the L1 penalty\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    pickle.dump(lasso_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '_2vs1.sav', 'wb'))\n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"Lasso done\")\n",
    "    \n",
    "    #Elastics\n",
    "    model = 'Elastics'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    elastics_model = LogisticRegression(class_weight='balanced', solver='saga', l1_ratio=params['l1_ratio'], max_iter=params['max_iter'],  penalty = 'elasticnet', random_state=1234)\n",
    "    elastics_model.fit(X_train, y_train)\n",
    "    pickle.dump(elastics_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '_2vs1.sav', 'wb'))\n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"Elastics done\")\n",
    "\n",
    "    # #GNB\n",
    "    # model = 'NB'\n",
    "    # params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # # params = eval(params)\n",
    "    # print(params)\n",
    "    # gnb_model = GaussianNB(var_smoothing = params['var_smoothing'])\n",
    "    # gnb_model.fit(X_train, y_train)\n",
    "    # pickle.dump(gnb_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb'))  \n",
    "    # models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    # print(\"GNB done\")\n",
    "\n",
    "    # # #SVM\n",
    "    # model = 'SVM'\n",
    "    # params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # # params = eval(params)\n",
    "    # print(params)\n",
    "    # svc_model = SVC(class_weight='balanced', C = params['C'], gamma=params['gamma'], kernel='rbf', random_state=1234, cache_size=1000)\n",
    "    # svc_model.fit(X_train,y_train)\n",
    "    # pickle.dump(svc_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '.sav', 'wb')) \n",
    "    # models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    # print(\"SVM done\")\n",
    "\n",
    "    #DT\n",
    "    model = 'DT'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    dt_model = DecisionTreeClassifier(class_weight='balanced', max_depth=params['max_depth'], criterion=params['criterion'], splitter=params['splitter'], random_state=1234)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    pickle.dump(dt_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '_2vs1.sav', 'wb'))    \n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"DT done\")\n",
    "\n",
    "    #RF\n",
    "    model = 'RF'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    rf_model = RandomForestClassifier(class_weight='balanced', max_depth=params['max_depth'], criterion=params['criterion'], n_estimators=params['n_estimators'], random_state=1234)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    pickle.dump(rf_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '_2vs1.sav', 'wb'))     \n",
    "    models.append([model + str(fold), target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"RF done\")\n",
    "\n",
    "    #XGB\n",
    "    model = 'XGB'\n",
    "    params = params_dict[(params_dict['outcome']==target_outcome)&(params_dict['model']==model)]['params'].tolist()[0]\n",
    "    # params = eval(params)\n",
    "    print(params)\n",
    "    scale_pos_ratio = y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "    xgb_model = xgb.XGBClassifier(objective ='binary:logistic', max_depth = params['max_depth'], n_estimators = params['n_estimators'],   \n",
    "                                  learning_rate=params['learning_rate'], reg_alpha = params['reg_alpha'], reg_lambda = params['reg_lambda'],\n",
    "                                  importance_type = 'gain', scale_pos_weight = scale_pos_ratio, use_label_encoder=False, tree_method='gpu_hist', \n",
    "                                  gpu_id=0, verbosity = 0, random_state = 1234,)\n",
    "    # xgb_model = xgb.XGBClassifier(objective ='binary:logistic', learning_rate = 0.001, tree_method='gpu_hist', gpu_id=0,  verbosity = 0, random_state = 1234)\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    pickle.dump(xgb_model, open(model_folder+ target_outcome + '/'+ model + str(fold) + '_2vs1.sav', 'wb')) \n",
    "    models.append([model + str(fold),  target_outcome, y_train.value_counts()[1]/y_train.value_counts()[0]])\n",
    "    print(\"XGB done\")\n",
    "    \n",
    "    return models\n",
    "    # return [xgb_model]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def is_leaf(inner_tree, index):\n",
    "    # Check whether node is leaf node\n",
    "    return (inner_tree.children_left[index] == TREE_LEAF and \n",
    "            inner_tree.children_right[index] == TREE_LEAF)\n",
    "\n",
    "def prune_index(inner_tree, decisions, index=0):\n",
    "    # Start pruning from the bottom - if we start from the top, we might miss\n",
    "    # nodes that become leaves during pruning.\n",
    "    # Do not use this directly - use prune_duplicate_leaves instead.\n",
    "    if not is_leaf(inner_tree, inner_tree.children_left[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_left[index])\n",
    "    if not is_leaf(inner_tree, inner_tree.children_right[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_right[index])\n",
    "\n",
    "    # Prune children if both children are leaves now and make the same decision:     \n",
    "    if (is_leaf(inner_tree, inner_tree.children_left[index]) and\n",
    "        is_leaf(inner_tree, inner_tree.children_right[index]) and\n",
    "        (decisions[index] == decisions[inner_tree.children_left[index]]) and \n",
    "        (decisions[index] == decisions[inner_tree.children_right[index]])):\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "        ##print(\"Pruned {}\".format(index))\n",
    "\n",
    "def prune_duplicate_leaves(mdl):\n",
    "    # Remove leaves if both \n",
    "    decisions = mdl.tree_.value.argmax(axis=2).flatten().tolist() # Decision for each node\n",
    "    prune_index(mdl.tree_, decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba329484-ef88-4bba-a169-70f029879bf0",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977620af-cffb-4e5d-a21f-8f5ff3df530d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512062, 67)\n",
      "(121975, 67)\n",
      "(23883, 67)\n",
      "(15272, 67)\n",
      "(8611, 67)\n"
     ]
    }
   ],
   "source": [
    "X = trainingData[features_columns]\n",
    "X_val = validationData[features_columns]\n",
    "X = pd.concat([X, X_val])\n",
    "\n",
    "X_internaleval = internalEvaluationData[features_columns]\n",
    "X_eval = evaluationData[features_columns]\n",
    "X_eval_Wales = evaluationDataWales[features_columns]\n",
    "X_eval_Scotland = evaluationDataScotland[features_columns]\n",
    "\n",
    "print(X.shape)\n",
    "print(X_val.shape)\n",
    "print(X_eval.shape)\n",
    "print(X_eval_Wales.shape)\n",
    "print(X_eval_Scotland.shape)\n",
    "\n",
    "target_outcomes = ['3months', '6months', '9months', '12months'] \n",
    "# target_outcomes = ['12months'] \n",
    "model_names = ['LR', 'Lasso', 'NB', 'DT', 'RF', 'XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f225c66-5f1f-4331-a3c1-45e22ab444c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#params\n",
    "params_dict = pd.read_csv('../Models/2vs1_BS_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80821b01-8f91-4da1-ab92-c7fa3891eb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_params(param_items, best_param):\n",
    "    a = eval(param_items)\n",
    "    b = eval(best_param)\n",
    "    c = {}\n",
    "    for key, value in zip(a,b):\n",
    "        c[key] = value\n",
    "    return c\n",
    "\n",
    "params_dict['params'] = params_dict.apply(lambda x: process_params(x.param_items, x.best_param), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd97da62-733f-46ac-aeec-b9ced5aafe30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_param</th>\n",
       "      <th>param_items</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3months</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.710699</td>\n",
       "      <td>['liblinear', 0.10612435463932542, 123]</td>\n",
       "      <td>['solver', 'C', 'max_iter']</td>\n",
       "      <td>{'solver': 'liblinear', 'C': 0.106124354639325...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3months</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.710713</td>\n",
       "      <td>['liblinear', 0.3110204078235362, 161]</td>\n",
       "      <td>['solver', 'C', 'max_iter']</td>\n",
       "      <td>{'solver': 'liblinear', 'C': 0.311020407823536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3months</td>\n",
       "      <td>Elastics</td>\n",
       "      <td>0.701346</td>\n",
       "      <td>[0.3145311526395745, 709]</td>\n",
       "      <td>['l1_ratio', 'max_iter']</td>\n",
       "      <td>{'l1_ratio': 0.3145311526395745, 'max_iter': 709}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3months</td>\n",
       "      <td>NB</td>\n",
       "      <td>0.683118</td>\n",
       "      <td>[6.224078136444961e-09]</td>\n",
       "      <td>['var_smoothing']</td>\n",
       "      <td>{'var_smoothing': 6.224078136444961e-09}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3months</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.722408</td>\n",
       "      <td>['entropy', 'best', 3]</td>\n",
       "      <td>['criterion', 'splitter', 'max_depth']</td>\n",
       "      <td>{'criterion': 'entropy', 'splitter': 'best', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome     model  best_score                               best_param  \\\n",
       "0  3months        LR    0.710699  ['liblinear', 0.10612435463932542, 123]   \n",
       "1  3months     Lasso    0.710713   ['liblinear', 0.3110204078235362, 161]   \n",
       "2  3months  Elastics    0.701346                [0.3145311526395745, 709]   \n",
       "3  3months        NB    0.683118                  [6.224078136444961e-09]   \n",
       "4  3months        DT    0.722408                   ['entropy', 'best', 3]   \n",
       "\n",
       "                              param_items  \\\n",
       "0             ['solver', 'C', 'max_iter']   \n",
       "1             ['solver', 'C', 'max_iter']   \n",
       "2                ['l1_ratio', 'max_iter']   \n",
       "3                       ['var_smoothing']   \n",
       "4  ['criterion', 'splitter', 'max_depth']   \n",
       "\n",
       "                                              params  \n",
       "0  {'solver': 'liblinear', 'C': 0.106124354639325...  \n",
       "1  {'solver': 'liblinear', 'C': 0.311020407823536...  \n",
       "2  {'l1_ratio': 0.3145311526395745, 'max_iter': 709}  \n",
       "3           {'var_smoothing': 6.224078136444961e-09}  \n",
       "4  {'criterion': 'entropy', 'splitter': 'best', '...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf35146-ffe8-41e8-a831-a840906e440c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3months\n",
      "Building models . . . .\n",
      "{'solver': 'liblinear', 'C': 0.10612435463932542, 'max_iter': 123}\n",
      "LR done\n",
      "{'solver': 'liblinear', 'C': 0.3110204078235362, 'max_iter': 161}\n",
      "Lasso done\n",
      "{'l1_ratio': 0.3145311526395745, 'max_iter': 709}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastics done\n",
      "{'criterion': 'entropy', 'splitter': 'best', 'max_depth': 3}\n",
      "DT done\n",
      "{'criterion': 'log_loss', 'n_estimators': 286, 'max_depth': 6}\n",
      "RF done\n",
      "{'n_estimators': 160, 'max_depth': 4, 'learning_rate': 0.018224336938187292, 'reg_alpha': 4.3177889716795244e-05, 'reg_lambda': 2.914414442975993e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB done\n",
      "LR0\n",
      "Lasso0\n",
      "Elastics0\n",
      "DT0\n",
      "RF0\n",
      "XGB0\n",
      "6months\n",
      "Building models . . . .\n",
      "{'solver': 'liblinear', 'C': 0.3110204078235362, 'max_iter': 161}\n",
      "LR done\n",
      "{'solver': 'liblinear', 'C': 0.3110204078235362, 'max_iter': 161}\n",
      "Lasso done\n",
      "{'l1_ratio': 0.3145311526395745, 'max_iter': 709}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastics done\n",
      "{'criterion': 'entropy', 'splitter': 'best', 'max_depth': 3}\n",
      "DT done\n",
      "{'criterion': 'gini', 'n_estimators': 281, 'max_depth': 7}\n",
      "RF done\n",
      "{'n_estimators': 251, 'max_depth': 5, 'learning_rate': 0.017952118813409648, 'reg_alpha': 0.0038566417152755203, 'reg_lambda': 1.547768166339415e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB done\n",
      "LR0\n",
      "Lasso0\n",
      "Elastics0\n",
      "DT0\n",
      "RF0\n",
      "XGB0\n",
      "9months\n",
      "Building models . . . .\n",
      "{'solver': 'newton-cholesky', 'C': 9.424940225483164, 'max_iter': 68}\n",
      "LR done\n",
      "{'solver': 'liblinear', 'C': 4.268997181796597, 'max_iter': 74}\n",
      "Lasso done\n",
      "{'l1_ratio': 0.3145311526395745, 'max_iter': 709}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastics done\n",
      "{'criterion': 'entropy', 'splitter': 'best', 'max_depth': 3}\n",
      "DT done\n",
      "{'criterion': 'gini', 'n_estimators': 199, 'max_depth': 8}\n",
      "RF done\n",
      "{'n_estimators': 251, 'max_depth': 5, 'learning_rate': 0.017952118813409648, 'reg_alpha': 0.0038566417152755203, 'reg_lambda': 1.547768166339415e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB done\n",
      "LR0\n",
      "Lasso0\n",
      "Elastics0\n",
      "DT0\n",
      "RF0\n",
      "XGB0\n",
      "12months\n",
      "Building models . . . .\n",
      "{'solver': 'liblinear', 'C': 0.10612435463932542, 'max_iter': 123}\n",
      "LR done\n",
      "{'solver': 'liblinear', 'C': 0.3110204078235362, 'max_iter': 161}\n",
      "Lasso done\n",
      "{'l1_ratio': 0.15794968314524718, 'max_iter': 708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastics done\n",
      "{'criterion': 'entropy', 'splitter': 'best', 'max_depth': 6}\n",
      "DT done\n",
      "{'criterion': 'gini', 'n_estimators': 199, 'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#EXECUTE model training\n",
    "summary_result_val = []\n",
    "summary_result_eval = []\n",
    "summary_result_Wales = []\n",
    "summary_result_Scotland = []\n",
    "# cols = ['model_name', 'outcome', 'class_ratio', 'acc','spec','sens','auc', 'auprc', 'balance_accuracy', 'f1_score', 'ppv', 'npv']\n",
    "cols = ['model_name', 'outcome', 'class_ratio', 'auc', 'auprc']\n",
    "model_folder = '../Models_trainValEval/'\n",
    "fold = 0\n",
    "\n",
    "for target_outcome in target_outcomes:\n",
    "    models = pd.DataFrame(columns=['modelname', 'target_outcome', 'class_ratio'])\n",
    "    print(target_outcome)\n",
    "    y = trainingData[target_outcome]\n",
    "    y_val = validationData[target_outcome]\n",
    "    y = pd.concat([y, y_val])\n",
    "    y_internaleval = internalEvaluationData[target_outcome]\n",
    "    y_eval = evaluationData[target_outcome]\n",
    "    y_eval_Wales = evaluationDataWales[target_outcome]\n",
    "    y_eval_Scotland = evaluationDataScotland[target_outcome]\n",
    "    #Build models -> it can be commented if the models have been trained\n",
    "    models_temp = pd.DataFrame(build_models(X, y, target_outcome, params_dict, model_folder, fold), columns=['modelname', 'target_outcome', 'class_ratio'])\n",
    "    models = pd.concat([models,models_temp]).reset_index(drop=True)\n",
    "\n",
    "    #evaluate model\n",
    "    for modelname, target_outcome, classratio in models.values:\n",
    "        # print('======================================================================')\n",
    "        print(modelname)\n",
    "        model = pickle.load(open(model_folder+ target_outcome + '/'+ modelname + '_2vs1.sav', 'rb'))\n",
    "        summary_result_eval.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval, y_eval, model) )\n",
    "        summary_result_Wales.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval_Wales, y_eval_Wales, model) )       \n",
    "        summary_result_Scotland.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval_Scotland, y_eval_Scotland, model) )       \n",
    "        summary_result_val.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_internaleval, y_internaleval, model) )       \n",
    "\n",
    "\n",
    "summary_result_eval = pd.DataFrame(summary_result_eval, columns=cols)\n",
    "summary_result_eval['model_num'] = summary_result_eval.index\n",
    "\n",
    "summary_result_Wales = pd.DataFrame(summary_result_Wales, columns=cols)\n",
    "summary_result_Wales['model_num'] = summary_result_Wales.index\n",
    "\n",
    "summary_result_Scotland = pd.DataFrame(summary_result_Scotland, columns=cols)\n",
    "summary_result_Scotland['model_num'] = summary_result_Scotland.index\n",
    "\n",
    "summary_result_internaleval = pd.DataFrame(summary_result_val, columns=cols)\n",
    "summary_result_internaleval['model_num'] = summary_result_val.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df0bb2-a000-4236-a576-7d625e72205c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary_result_eval['model_name'] = summary_result_eval.apply(lambda x: modelNameFixer(x['model_name']), axis=1)\n",
    "# summary_result_val['model_name'] = summary_result_val.apply(lambda x: modelNameFixer(x['model_name']), axis=1)\n",
    "# summary_result_Wales['model_name'] = summary_result_Wales.apply(lambda x: modelNameFixer(x['model_name']), axis=1)\n",
    "# summary_result_Scotland['model_name'] = summary_result_Scotland.apply(lambda x: modelNameFixer(x['model_name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993827f6-2c42-4670-8d36-c1e016c7b46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary_result.to_csv('../Models/summary_result_test.csv', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d90777-2b7e-47bf-b71a-4830b584e880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary_result = pd.read_csv('../Models/summary_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3017ee-42a8-4f9f-83f2-af18bbc05fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summary_result_internaleval['set'] = 'England'\n",
    "# summary_result_eval['set'] = 'Wales & Scotland'\n",
    "# summary_result_Wales['set'] = 'Wales'\n",
    "# summary_result_Scotland['set'] = 'Scotland'\n",
    "\n",
    "# combine = pd.concat([summary_result_internaleval, summary_result_eval, \n",
    "#                      summary_result_Wales, summary_result_Scotland,\n",
    "#                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236a839-728a-46b6-b77a-730c94bf5c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = combine\n",
    "# # data = combine[(combine.outcome=='12months')]\n",
    "# bar = sns.catplot(x = 'model_name',       # x variable name\n",
    "#             y = 'auc',       # y variable name            \n",
    "#             data=data,\n",
    "#             kind = \"bar\",\n",
    "#             hue = 'set',\n",
    "#             # hue_order=['10-fold CV', 'Evaluation Set', 'Wales', 'Scotland'],\n",
    "#             height=8,\n",
    "#             row='outcome',\n",
    "#             aspect=1.8,\n",
    "#             errorbar = None,)\n",
    "\n",
    "# for items in bar.axes:\n",
    "#     for ax in items:\n",
    "#         for p in ax.patches:\n",
    "#             ax.text(p.get_x() + 0.01, \n",
    "#                 p.get_height() * 1.005, \n",
    "#                 '{0:.4f}'.format(p.get_height()), \n",
    "#                 color='black', rotation=20, fontsize=8)\n",
    "\n",
    "# ax.set_ylim(0.5, .9)\n",
    "# # ax.set_ylabel('AUC Score', fontsize=13)\n",
    "# ax.set_xlabel('Method', fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639ad8f-9414-4f8a-bf4e-8578993c973e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = ['LR', 'Lasso', 'Elastics', 'DT', 'RF', 'XGB']\n",
    "for model_name in model_names:\n",
    "    model = pickle.load(open('../Models_trainValEval/12months/'+model_name+'0_2vs1.sav', 'rb'))\n",
    "    preds = model.predict_proba(X_eval)\n",
    "    preds = [x[1] for x in preds]\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, preds, pos_label=1)\n",
    "    auc = np.round(roc_auc_score(y_eval, preds), 4)\n",
    "    plt.plot(fpr,tpr,label=model_name+\", auc=\"+str(auc))\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326ce41-3969-402d-982b-518b3a147c3f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_result_val = []\n",
    "summary_result_eval = []\n",
    "summary_result_Wales = []\n",
    "summary_result_Scotland = []\n",
    "cols = ['model_name', 'outcome', 'auc', 'auprc']\n",
    "for outcome in target_outcomes:\n",
    "    for model_name in model_names:\n",
    "        # print('======================================================================')\n",
    "        print(model_name)\n",
    "        model = pickle.load(open(model_folder+ outcome + '/'+ model_name + '0_2vs1.sav', 'rb'))\n",
    "        summary_result_eval.append((str(model_name),outcome,) + summariseResult (X_eval, y_eval, model) )\n",
    "        summary_result_Wales.append((str(model_name),outcome,) + summariseResult (X_eval_Wales, y_eval_Wales, model) )       \n",
    "        summary_result_Scotland.append((str(model_name),outcome,) + summariseResult (X_eval_Scotland, y_eval_Scotland, model) )       \n",
    "        summary_result_val.append((str(model_name),outcome,) + summariseResult (X_internaleval, y_internaleval, model) )   \n",
    "\n",
    "    \n",
    "summary_result_eval = pd.DataFrame(summary_result_eval, columns=cols)\n",
    "summary_result_eval['model_num'] = summary_result_eval.index\n",
    "\n",
    "summary_result_Wales = pd.DataFrame(summary_result_Wales, columns=cols)\n",
    "summary_result_Wales['model_num'] = summary_result_Wales.index\n",
    "\n",
    "summary_result_Scotland = pd.DataFrame(summary_result_Scotland, columns=cols)\n",
    "summary_result_Scotland['model_num'] = summary_result_Scotland.index\n",
    "\n",
    "summary_result_internaleval = pd.DataFrame(summary_result_val, columns=cols)\n",
    "summary_result_internaleval['model_num'] = summary_result_val.index\n",
    "summary_result_internaleval['set'] = 'England'\n",
    "summary_result_eval['set'] = 'Wales & Scotland'\n",
    "summary_result_Wales['set'] = 'Wales'\n",
    "summary_result_Scotland['set'] = 'Scotland'\n",
    "\n",
    "combine = pd.concat([summary_result_internaleval, summary_result_eval, \n",
    "                     summary_result_Wales, summary_result_Scotland,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea4acd-c883-46d7-bf09-7f701e8c9990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combine\n",
    "data = combine[(combine.outcome=='12months')]\n",
    "bar = sns.catplot(x = 'model_name',       # x variable name\n",
    "            y = 'auc',       # y variable name            \n",
    "            data=data,\n",
    "            kind = \"bar\",\n",
    "            hue = 'set',\n",
    "            # hue_order=['10-fold CV', 'Evaluation Set', 'Wales', 'Scotland'],\n",
    "            height=8,\n",
    "            row='outcome',\n",
    "            aspect=1.8,\n",
    "            errorbar = None,)\n",
    "\n",
    "for items in bar.axes:\n",
    "    for ax in items:\n",
    "        for p in ax.patches:\n",
    "            ax.text(p.get_x() + 0.01, \n",
    "                p.get_height() * 1.005, \n",
    "                '{0:.4f}'.format(p.get_height()), \n",
    "                color='black', rotation=20, fontsize=8)\n",
    "\n",
    "ax.set_ylim(0.6, .87)\n",
    "# ax.set_ylabel('AUC Score', fontsize=13)\n",
    "ax.set_xlabel('Method', fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5695b9a-ebba-450e-82e6-90b2389f85c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = ['LR', 'Lasso', 'Elastics', 'DT', 'RF', 'XGB']\n",
    "for model_name in model_names:\n",
    "    for outcome in target_outcomes:\n",
    "        model = pickle.load(open('../Models_trainValEval/'+outcome+'/'+model_name+'0_2vs1.sav', 'rb'))\n",
    "        preds = model.predict_proba(X_eval)\n",
    "        # preds = [0 if x[1] < 0.5 else 1 for x in preds]\n",
    "        preds = [x[1] for x in preds]\n",
    "        fpr, tpr, thresholds = roc_curve(y_eval, preds, pos_label=1)\n",
    "        auc = np.round(roc_auc_score(y_eval, preds), 4)\n",
    "        plt.plot(fpr,tpr,label=outcome+\", auc=\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(model_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c423b49-f3d5-4c99-9872-b67618571f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(combine, open('../Models/benchmarking_result_2vs1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ba3d7-086e-4c31-9ad6-9c0750152646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f588d006-eb87-4964-a9e8-7e01843fc5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e577b-243d-4b59-9520-aa4999e63b25",
   "metadata": {},
   "source": [
    "## calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87e7e6-6e3a-4b82-bf04-718fa3634f68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summariseResultCalibration (testX, testY, model, threshold):\n",
    "    preds = model.predict_proba(testX)\n",
    "    preds = [0 if x[1] < threshold else 1 for x in preds]\n",
    "    tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    ppv = 100*tp/(tp+fp)\n",
    "    npv = 100*tn/(fn+tn)\n",
    "    acc = accuracy_score(testY, preds)\n",
    "    f1score = f1_score(testY, preds, average = 'binary')\n",
    "    balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    aucscore = roc_auc_score(testY, preds)\n",
    "    return np.round(acc,4), np.round(specificity,4), np.round(sensitivity,4), np.round(balanceacc,4), np.round(f1score,4), np.round(ppv,4), np.round(npv,4), np.round(aucscore,4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05329630-e4f6-4df7-8827-c7a59324a904",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "calibrationResult = []\n",
    "model_names = ['LR', 'Lasso', 'Elastics', 'DT', 'RF', 'XGB']\n",
    "for model_name in model_names:\n",
    "    for outcome in target_outcomes:\n",
    "        model = pickle.load(open('../Models_trainValEval/'+outcome+'/'+model_name+'0_2vs1.sav', 'rb'))\n",
    "        preds = model.predict_proba(X_eval)\n",
    "        preds = [x[1] for x in preds]\n",
    "        fpr, tpr, thresholds = roc_curve(y_eval, preds)\n",
    "        optimal_idx = np.argmax(tpr - fpr)# Find optimal probability threshold\n",
    "        threshold = thresholds[optimal_idx]\n",
    "        calibrationResult.append(((outcome, model_name, threshold)+summariseResultCalibration(X_eval, y_eval, model, threshold)))\n",
    "\n",
    "cols = ['outcome', 'model', 'threshold', 'acc', 'sens', 'spec', 'balanced_acc', 'f1', 'ppv' , 'npv', 'auc']\n",
    "pd.DataFrame(calibrationResult, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c81677-6d74-40ce-a81c-ea773f316362",
   "metadata": {},
   "source": [
    "## DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23258bb-f406-47ac-b970-2c68c6c9f392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_dt = pickle.load(open('../Models_trainValEval/12months/XGB0.sav', 'rb'))\n",
    "\n",
    "# pd.DataFrame([best_model3.feature_importances_], columns=X.columns).T.sort_values(0, ascending=False)\n",
    "sorted_idx = best_model_dt.feature_importances_.argsort()\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.barh(X.columns[sorted_idx][-10:], best_model_dt.feature_importances_[sorted_idx][-10:])\n",
    "plt.xlabel(\"XGBoost Feature Importance\")\n",
    "plt.show()\n",
    "xgbtop10 = X.columns[sorted_idx][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585071b8-244d-4f64-bd03-6cfcb038b749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_lasso = pickle.load(open('../Models_trainValEval/12months/LR0.sav', 'rb'))\n",
    "\n",
    "# pd.DataFrame([best_model3.feature_importances_], columns=X.columns).T.sort_values(0, ascending=False)\n",
    "sorted_idx = best_model_lasso.coef_[0].argsort()\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.barh(X.columns[sorted_idx][-10:], best_model_lasso.coef_[0][sorted_idx][-10:])\n",
    "plt.xlabel(\"LR Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7d254-18ba-4e6c-a70f-e232cef0d971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lassotop10 = X.columns[sorted_idx][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b58c1-a6cd-4616-b37e-8a838c4e694e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(set(dttop10).intersection(set(lassotop10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210cf71-819b-4110-a6da-a8476d3164f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prune_duplicate_leaves(best_model_dt)\n",
    "fig = plt.figure(figsize=(30,10))\n",
    "_ = tree.plot_tree(best_model_dt, \n",
    "                   feature_names=X.columns,  \n",
    "                   class_names=['no asthma attack','asthma attack'],\n",
    "                   filled=True,)\n",
    "plt.savefig('../FinalData/dt.png',format='png',bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cf88f-a1ea-4f04-82c1-e2ebcef6dea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData['BTS_step_2.0'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc8cb2-0b7c-4068-8602-3f20ce245e3b",
   "metadata": {},
   "source": [
    "## Minimum depth DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f3421-1c24-4b2a-a9d4-883463ff6890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#DT with minimum max of depth\n",
    "\n",
    "#EXECUTE model training\n",
    "# X = trainingData[features_columns]\n",
    "# X_test = evaluationData[features_columns]\n",
    "target_outcome = '12months'\n",
    "\n",
    "print(target_outcome)\n",
    "y = trainingData[target_outcome]\n",
    "y_test = evaluationData[target_outcome]\n",
    "#Build models -> it can be commented if the models have been trained\n",
    "\n",
    "# if os.path.isfile('../Models_trainValEval/dt_minimum_depth.sav'):\n",
    "#     dt_model = pickle.load(open('../Models_trainValEval/dt_minimum_depth.sav', 'rb'))\n",
    "# else:\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', max_depth=4, splitter='random', random_state=1234)\n",
    "dt_model.fit(X, y)\n",
    "pickle.dump(dt_model, open('../Models_trainValEval/dt_minimum_depth.sav', 'wb'))    \n",
    "    \n",
    "\n",
    "print(summariseResult (X_val, y_val, dt_model))\n",
    "print(summariseResult (X_eval, y_eval, dt_model))\n",
    "print(summariseResult (X_eval_Wales, y_eval_Wales, dt_model))\n",
    "print(\"DT done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bca650-b488-4717-9563-093c43b119f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prune_duplicate_leaves(dt_model)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "_ = tree.plot_tree(dt_model, \n",
    "                   feature_names=X.columns,  \n",
    "                   class_names=['no asthma attack','asthma attack'],\n",
    "                   filled=True)\n",
    "plt.savefig('../FinalData/dt_minimumdepth.png',format='png',bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20cc936-fc3a-480e-ad1b-cccbbc8cd41b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = pickle.load(open('../Models_trainValEval/cont_scaler.pkl', 'rb'))\n",
    "continuous_vars = ['age', 'CharlsonScore', 'average_daily_dose_ICS', 'prescribed_daily_dose_ICS', 'ICS_medication_possesion_ratio', \n",
    "                   'numOCS', 'numPCS', 'numPCSAsthma', 'numAntibioticsEvents', 'numAntibioticswithLRTI', 'numOCSEvents', 'numOCSwithLRTI', \n",
    "                   'numAsthmaAttacks', 'numAcuteRespEvents', 'numHospEvents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f296b-25f4-4399-b254-48e940663b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataOfInterest_cont = pd.DataFrame(scaler.inverse_transform(trainingData[continuous_vars]), columns=scaler.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0eefc-26d5-4b8e-bf2f-6d7683f35940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VarOfInterest = ['numOCS', 'numPCS']\n",
    "additionalVars = ['prescribed_daily_dose_ICS', 'numAntibioticsEvents', 'ICS_medication_possesion_ratio', 'numAsthmaAttacks', 'numAntibioticswithLRTI', 'age']\n",
    "DataOfInterest = pd.concat((DataOfInterest_cont, trainingData[['BTS_step', 'imd_decile_0']+target_outcomes]), axis=1)\n",
    "#solve little problem with inverse scaling:\n",
    "DataOfInterest['numAsthmaAttacks'] = DataOfInterest.numAsthmaAttacks.apply(lambda x: 0 if x<1 else x)\n",
    "DataOfInterest['numOCSwithLRTI'] = DataOfInterest.numOCSwithLRTI.apply(lambda x: 0 if x<1 else x)\n",
    "DataOfInterest['numHospEvents'] = DataOfInterest.numHospEvents.apply(lambda x: 0 if x<1 else x)\n",
    "DataOfInterest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c4212-78a5-40ef-ac3c-faf16ee0c50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162ed5f-0822-4876-b7f8-905cefd4b556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depth3_1 = [[0, 0, 0,\n",
    "       0, 0,\n",
    "       0.041, -1.086, 0, 0,\n",
    "       0, 0, 0,\n",
    "       0, 0, 0]]\n",
    "\n",
    "depth3_2 = [[0, 0, 0,\n",
    "       0, 0,\n",
    "       0.583, -1.086, 0, 0,\n",
    "       0, 0, 0,\n",
    "       0, 0, 0]]\n",
    "\n",
    "depth4_1 = [[0, -0.822, 0,\n",
    "       0, 0,\n",
    "       0.041, -0.23, 0, 0,\n",
    "       0, 0, 0,\n",
    "       2.493, 0, 0]]\n",
    "\n",
    "depth4_2 = [[0, -0.822, 0,\n",
    "       0, 0,\n",
    "       0.583, -1.02, 0, 0,\n",
    "       0, 0, 0,\n",
    "       2.493, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934c8e3-7234-485d-a7d3-454a0bf7c656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(depth3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2358f-70d9-44ba-823d-c1869cb2dffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(depth3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22496a5-8506-444c-86d2-1e3fa62ebdc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(depth4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782383b5-decf-4938-a214-12035965c976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(depth4_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a97c3-0381-4d81-85e2-006001b5fe9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataOfInterest.CharlsonScore.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfac1c0-e120-4c31-ad12-de84bb2a6510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f20cd-8ccf-41f5-a528-770dcaa93c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_dt = best_model_dt.predict(trainingData[features_columns])\n",
    "preds_lasso = best_model_lasso.predict(trainingData[features_columns])\n",
    "prediction_table = pd.DataFrame([trainingData['12months'], preds_dt, preds_lasso]).T\n",
    "prediction_table.columns = ['y_true', 'y_pred_dt', 'y_pred_lasso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0a349-ca09-4955-bfa3-cf594957ac01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "false_positive = prediction_table[(prediction_table.y_pred_dt==1) & (prediction_table.y_true!=prediction_table.y_pred_dt)]\n",
    "false_negative = prediction_table[(prediction_table.y_pred_dt==0) & (prediction_table.y_true!=prediction_table.y_pred_dt)]\n",
    "true_positive = prediction_table[(prediction_table.y_pred_dt==1) & (prediction_table.y_true==prediction_table.y_pred_dt)]\n",
    "true_negative = prediction_table[(prediction_table.y_pred_dt==0) & (prediction_table.y_true==prediction_table.y_pred_dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3982da-bf1d-48ff-a3fb-0395c0a6f519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('True Pos')\n",
    "print(DataOfInterest.iloc[true_positive.index][VarOfInterest+['BTS_step']].describe())\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "print('False Neg')\n",
    "print(DataOfInterest.iloc[false_negative.index][VarOfInterest+['BTS_step']].describe())\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "# print('False Pos')\n",
    "# print(DataOfInterest.iloc[false_positive.index][VarOfInterest+['BTS_step']].describe())\n",
    "# print('-----------------------------------------------------------------------------------')\n",
    "# print('True Neg')\n",
    "# print(DataOfInterest.iloc[true_negative.index][VarOfInterest+['BTS_step']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c8654-fead-4a22-9602-946a25406eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "DataOfInterest.iloc[true_positive.index][VarOfInterest+['BTS_step']].boxplot(ax=ax)\n",
    "ax.set_ylim(-50,400)\n",
    "plt.show()\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "fig, ax = plt.subplots()\n",
    "DataOfInterest.iloc[false_negative.index][VarOfInterest+['BTS_step']].boxplot()\n",
    "ax.set_ylim(-50,400)\n",
    "plt.show()\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "fig, ax = plt.subplots()\n",
    "DataOfInterest.iloc[true_negative.index][VarOfInterest+['BTS_step']].boxplot()\n",
    "ax.set_ylim(-50,400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e4b6e-bb69-4c98-ae74-7711e9bf6bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DataOfInterest.iloc[true_positive.index][VarOfInterest+['BTS_step']+additionalVars+['imd_decile_0']].describe())\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "print(DataOfInterest.iloc[false_negative.index][VarOfInterest+['BTS_step']+additionalVars+['imd_decile_0']].describe())\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "print(DataOfInterest.iloc[true_negative.index][VarOfInterest+['BTS_step']+additionalVars+['imd_decile_0']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da4310-610e-4eb8-b2d3-55302c07bf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataOfInterest.iloc[true_positive.index][VarOfInterest+['BTS_step']].sample(n=20, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e060086-dc67-4232-b8fc-5c406bf7a7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataOfInterest.iloc[false_negative.index][VarOfInterest+['BTS_step']].sample(n=20, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abade78-e191-4c5e-a928-35bd1732936d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a60f6-89a5-4070-a5e5-c9a737ddad71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcbfbc12-c37e-444b-8859-be5785cb7de0",
   "metadata": {},
   "source": [
    "## Run models only on the subset of BTS == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5c8db-a5ea-4d61-9c0f-e20c98a3bd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingDataSubset = trainingData[(trainingData.BTS_step < 7)]\n",
    "validationDataSubset = validationData[(validationData.BTS_step < 7) ]\n",
    "evaluationDataSubset = evaluationData[(evaluationData.BTS_step < 7)]\n",
    "evaluationDataWalesSubset = evaluationDataWales[(evaluationDataWales.BTS_step < 7) ]\n",
    "evaluationDataScotlandSubset = evaluationDataScotland[(evaluationDataScotland.BTS_step < 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b3108-663b-4ddd-b893-6ce8e5a9372a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encode categorical data\n",
    "\n",
    "# cat_vars = ['PEFStatus','EosinophilLevel']\n",
    "onehot_vars = ['BTS_step']\n",
    "# data_categorical = trainingData[cat_vars]\n",
    "data_onehot = trainingDataSubset[onehot_vars]\n",
    "\n",
    "# #ordinal encoder\n",
    "# encoder = OrdinalEncoder(categories=[['not_recorded','less than 60', '60-80', 'more than 80'], ['unknown', 'normal', 'high']]).set_output(transform=\"pandas\")\n",
    "# data_encoded = encoder.fit_transform(data_categorical)\n",
    "# pickle.dump(encoder, open('../Models/cat_encoder.pkl', 'wb'))\n",
    "    \n",
    "#one hot encoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\n",
    "onehot_encoded = onehot_encoder.fit_transform(data_onehot)\n",
    "# pickle.dump(onehot_encoder, open('../Models/onehot_encoder.pkl', 'wb'))\n",
    "\n",
    "# trainingDataSubset = pd.concat([trainingDataSubset.drop(cat_vars, axis=1), data_encoded], axis=1)\n",
    "trainingDataSubset = pd.concat([trainingDataSubset.drop(onehot_vars, axis=1), onehot_encoded], axis=1)\n",
    "\n",
    "print('Data shape after encoding: ', trainingDataSubset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b2d79-2de5-4c41-af3c-a89b61340795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encode cat vars for validation and evaluation set\n",
    "# data_val_categorical = validationData[cat_vars]\n",
    "data_val_onehot = validationDataSubset[onehot_vars]\n",
    "# data_eval_categorical = evaluationData[cat_vars]\n",
    "data_eval_onehot = evaluationDataSubset[onehot_vars]\n",
    "# data_eval_Wales_categorical = evaluationDataWales[cat_vars]\n",
    "data_eval_Wales_onehot = evaluationDataWalesSubset[onehot_vars]\n",
    "# data_eval_Scotland_categorical = evaluationDataScotland[cat_vars]\n",
    "data_eval_Scotland_onehot = evaluationDataScotlandSubset[onehot_vars]\n",
    "\n",
    "# encoder = pickle.load(open('../Models/cat_encoder.pkl', 'rb'))\n",
    "# data_val_encoded = encoder.transform(data_val_categorical)\n",
    "# data_eval_encoded = encoder.transform(data_eval_categorical)\n",
    "# data_eval_Wales_encoded = encoder.transform(data_eval_Wales_categorical)\n",
    "# data_eval_Scotland_encoded = encoder.transform(data_eval_Scotland_categorical)\n",
    "\n",
    "# onehot_encoder = pickle.load(open('../Models/onehot_encoder.pkl', 'rb'))\n",
    "onehot_val_encoded = onehot_encoder.transform(data_val_onehot)\n",
    "onehot_eval_encoded = onehot_encoder.transform(data_eval_onehot)\n",
    "onehot_eval_Wales_encoded = onehot_encoder.transform(data_eval_Wales_onehot)\n",
    "onehot_eval_Scotland_encoded = onehot_encoder.transform(data_eval_Scotland_onehot)\n",
    "\n",
    "# validationDataSubset = pd.concat([validationDataSubset.drop(cat_vars, axis=1), data_val_encoded], axis=1)\n",
    "validationDataSubset = pd.concat([validationDataSubset.drop(onehot_vars, axis=1), onehot_val_encoded], axis=1)\n",
    "\n",
    "# evaluationDataSubset = pd.concat([evaluationDataSubset.drop(cat_vars, axis=1), data_eval_encoded], axis=1)\n",
    "evaluationDataSubset = pd.concat([evaluationDataSubset.drop(onehot_vars, axis=1), onehot_eval_encoded], axis=1)\n",
    "\n",
    "# evaluationDataWalesSubset = pd.concat([evaluationDataWalesSubset.drop(cat_vars, axis=1), data_eval_Wales_encoded], axis=1)\n",
    "evaluationDataWalesSubset = pd.concat([evaluationDataWalesSubset.drop(onehot_vars, axis=1), onehot_eval_Wales_encoded], axis=1)\n",
    "\n",
    "# evaluationDataScotlandSubset = pd.concat([evaluationDataScotlandSubset.drop(cat_vars, axis=1), data_eval_Scotland_encoded], axis=1)\n",
    "evaluationDataScotlandSubset = pd.concat([evaluationDataScotlandSubset.drop(onehot_vars, axis=1), onehot_eval_Scotland_encoded], axis=1)\n",
    "\n",
    "print('Val data shape after encoding: ', validationDataSubset.shape)\n",
    "print('Eval data shape after encoding: ', evaluationDataSubset.shape)\n",
    "print('Evaluation data Wales shape: ', evaluationDataWalesSubset.shape)\n",
    "print('Evaluation data Scotland shape: ', evaluationDataScotlandSubset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0514a-5b9c-4199-b9bb-31b7bd6b4acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingDataSubset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e982b7-1a51-4c73-b86a-d482d42d42cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingDataSubset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48836f2d-4aa4-45cc-8c26-3e2f023d790c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_columns.remove('BTS_step')\n",
    "features_columns = features_columns + ['BTS_step_0.0', 'BTS_step_1.0', 'BTS_step_2.0',\n",
    "                                       'BTS_step_3.0', 'BTS_step_4.0', 'BTS_step_5.0']\n",
    "features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d658e66-316c-430d-8eaf-3e6ddb0b1b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_subset = trainingDataSubset[features_columns]\n",
    "X_val_subset = validationDataSubset[features_columns]\n",
    "X_eval_subset = evaluationDataSubset[features_columns]\n",
    "X_eval_Wales_subset = evaluationDataWalesSubset[features_columns]\n",
    "X_eval_Scotland_subset = evaluationDataScotlandSubset[features_columns]\n",
    "target_outcomes = ['3months', '6months', '12months', '24months'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc032b87-4a5a-4477-bb41-ed250aff00ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea695565-9cf8-4368-addc-05602adb1821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_subset.shape[0]/trainingData.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fff2f6-b190-47db-ad2c-eb621aeec105",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#EXECUTE model training\n",
    "summary_result_val_subset = []\n",
    "summary_result_eval_subset = []\n",
    "summary_result_Wales_subset = []\n",
    "summary_result_Scotland_subset = []\n",
    "cols = ['model_name', 'outcome', 'class_ratio', 'acc','spec','sens','auc', 'auprc', 'balance_accuracy', 'f1_score', 'ppv', 'npv']\n",
    "model_folder = '../Models_subsetBTS/'\n",
    "fold = 0\n",
    "\n",
    "for target_outcome in target_outcomes:\n",
    "    models = pd.DataFrame(columns=['modelname', 'target_outcome', 'class_ratio'])\n",
    "    print(target_outcome)\n",
    "    y_subset = trainingDataSubset[target_outcome]\n",
    "    y_val_subset = validationDataSubset[target_outcome]\n",
    "    y_eval_subset = evaluationDataSubset[target_outcome]\n",
    "    y_eval_Wales_subset = evaluationDataWalesSubset[target_outcome]\n",
    "    y_eval_Scotland_subset = evaluationDataScotlandSubset[target_outcome]\n",
    "    #Build models -> it can be commented if the models have been trained\n",
    "    models_temp = pd.DataFrame(build_models(X_subset, y_subset, target_outcome, params_dict, model_folder, fold), columns=['modelname', 'target_outcome', 'class_ratio'])\n",
    "    models = pd.concat([models,models_temp]).reset_index(drop=True)\n",
    "\n",
    "    #evaluate model\n",
    "    for modelname, target_outcome, classratio in models.values:\n",
    "        # print('======================================================================')\n",
    "        print(modelname)\n",
    "        model = pickle.load(open(model_folder+ target_outcome + '/'+ modelname + '.sav', 'rb'))\n",
    "        summary_result_eval_subset.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval_subset, y_eval_subset, model) )\n",
    "        summary_result_Wales_subset.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval_Wales_subset, y_eval_Wales_subset, model) )       \n",
    "        summary_result_Scotland_subset.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_eval_Scotland_subset, y_eval_Scotland_subset, model) )       \n",
    "        summary_result_val_subset.append((str(modelname), target_outcome, classratio, ) + summariseResult (X_val_subset, y_val_subset, model) )       \n",
    "\n",
    "\n",
    "summary_result_eval_subset = pd.DataFrame(summary_result_eval_subset, columns=cols)\n",
    "summary_result_eval_subset['model_num'] = summary_result_eval_subset.index\n",
    "\n",
    "summary_result_Wales_subset = pd.DataFrame(summary_result_Wales_subset, columns=cols)\n",
    "summary_result_Wales_subset['model_num'] = summary_result_Wales_subset.index\n",
    "\n",
    "summary_result_Scotland_subset = pd.DataFrame(summary_result_Scotland_subset, columns=cols)\n",
    "summary_result_Scotland_subset['model_num'] = summary_result_Scotland_subset.index\n",
    "\n",
    "summary_result_val_subset = pd.DataFrame(summary_result_val_subset, columns=cols)\n",
    "summary_result_val_subset['model_num'] = summary_result_val_subset.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e665a28-683f-468b-b87a-c16ed39aab3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_result_val_subset['set'] = 'val'\n",
    "summary_result_eval_subset['set'] = 'eval'\n",
    "summary_result_Wales_subset['set'] = 'Wales'\n",
    "summary_result_Scotland_subset['set'] = 'Scotland'\n",
    "\n",
    "combine_subset = pd.concat([summary_result_val_subset, summary_result_eval_subset, \n",
    "                     summary_result_Wales_subset, summary_result_Scotland_subset,\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5a6de-3ce1-44d0-8d35-abf3ea6b3c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = combine_subset\n",
    "# data = combine[combine.set!='Training Set']\n",
    "bar = sns.catplot(x = 'model_name',       # x variable name\n",
    "            y = 'auc',       # y variable name            \n",
    "            data=data,\n",
    "            kind = \"bar\",\n",
    "            hue = 'set',\n",
    "            # hue_order=['10-fold CV', 'Evaluation Set', 'Wales', 'Scotland'],\n",
    "            height=5,\n",
    "            row='outcome',\n",
    "            aspect=3,\n",
    "            errorbar = None,)\n",
    "\n",
    "for items in bar.axes:\n",
    "    for ax in items:\n",
    "        for p in ax.patches:\n",
    "            ax.text(p.get_x() + 0.01, \n",
    "                p.get_height() * 1.005, \n",
    "                '{0:.4f}'.format(p.get_height()), \n",
    "                color='black', rotation=20, fontsize=8)\n",
    "\n",
    "ax.set_ylim(0.55, 0.76)\n",
    "# ax.set_ylabel('AUC Score', fontsize=13)\n",
    "ax.set_xlabel('Method', fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cda8ec-c791-45ef-9b99-8314b15d1947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_dt = pickle.load(open('../Models_subsetBTS/12months/DT0.sav', 'rb'))\n",
    "\n",
    "# pd.DataFrame([best_model3.feature_importances_], columns=X.columns).T.sort_values(0, ascending=False)\n",
    "sorted_idx = best_model_dt.feature_importances_.argsort()\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.barh(X_subset.columns[sorted_idx][-10:], best_model_dt.feature_importances_[sorted_idx][-10:])\n",
    "plt.xlabel(\"Decision Tree Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f8266-8e87-44d0-b10e-d3b1adb5163c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
