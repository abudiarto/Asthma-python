{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948991e-2631-468d-9845-8e69e9adc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = 70\n",
    "        self.hidden_size = 8\n",
    "        self.num_layers = 2\n",
    "        self.output_dim = 2\n",
    "        self.lstm_feature_1 = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True,\n",
    "                                      bidirectional=True)\n",
    "        self.lstm_feature_2 = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True,\n",
    "                                      bidirectional=True)\n",
    "        self.fc_feature_1 = nn.Linear((self.hidden_size * 2) + 1, 1)\n",
    "        self.fc_feature_2 = nn.Linear((self.hidden_size * 2) + 1, 1)\n",
    "\n",
    "        self.fc = nn.Linear(4, self.output_dim)\n",
    "\n",
    "    def forward(self, f, device=None):\n",
    "        if not device:\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        x_f1, f1, x_f2, f2, f3, f4 = f\n",
    "\n",
    "        # x_f1 is feature_1_seq\n",
    "        # f1 is feature_1_baseline\n",
    "        # x_f2 is feature_2_seq\n",
    "        # f2 is feature_2_baseline\n",
    "\n",
    "        # f3 and f4 are tabular features\n",
    "\n",
    "        x_f1 = x_f1.view(x_f1.shape[0], 1, -1)\n",
    "        h0_f1, c0_f1 = self.init_hidden(x_f1, device)\n",
    "        h_t_f1, c_t_f1 = self.lstm_feature_1(x_f1, (h0_f1, c0_f1))\n",
    "        x_f1 = h_t_f1\n",
    "        x_f1 = x_f1.view(x_f1.shape[0], -1)\n",
    "\n",
    "        x_f2 = x_f2.view(x_f2.shape[0], 1, -1)\n",
    "        h0_f2, c0_f2 = self.init_hidden(x_f2, device)\n",
    "        h_t_f2, c_t_f2 = self.lstm_feature_2(x_f2, (h0_f2, c0_f2))\n",
    "        x_f2 = h_t_f2\n",
    "        x_f2 = x_f2.view(x_f2.shape[0], -1)\n",
    "\n",
    "        x_f1 = torch.cat((x_f1, f1), 1)\n",
    "        x_f1 = self.fc_feature_1(x_f1)\n",
    "\n",
    "        x_f2 = torch.cat((x_f2, f2), 1)\n",
    "        x_f2 = self.fc_feature_2(x_f2)\n",
    "\n",
    "        x = torch.cat((x_f1, x_f2, f3, f4), 1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def init_hidden(self, x, device):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)\n",
    "        return h0, c0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
