{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455d01d3-b746-40dc-915a-3fd111ba52e1",
   "metadata": {},
   "source": [
    "# Sequence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e965f6-5bb0-4bfb-a2cc-50eae440bac3",
   "metadata": {},
   "source": [
    "- all available historical data\n",
    "- include static features\n",
    "\n",
    "- attention mechanism\n",
    "\n",
    "- validation on DataLoch\n",
    "\n",
    "- model explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679fb538-9356-4f8a-b54f-a86d547dca8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyreadr\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a67ef-b62d-4869-8451-311c7596d1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient = pyreadr.read_r('../ServerData_13Oct2020/d_patient_overall.Rdata')\n",
    "practice = pyreadr.read_r('../ServerData_13Oct2020/d_practice.Rdata')\n",
    "patient = patient['d_patient_overall']\n",
    "practice = practice['d_practice']\n",
    "\n",
    "#Age in 2016-01-01\n",
    "patient['age'] = patient.year_of_birth.apply(lambda x: 2016-x)\n",
    "\n",
    "patient = patient[['patid', 'practice_id', 'age']].merge(practice[['practice_id', 'Country']], how='left', on='practice_id')\n",
    "\n",
    "# Outcomes data\n",
    "outcomes = pd.read_csv(\"../FinalData/cleaned_outcomes_11072023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a5ce8-ae16-4ccd-892e-89d3523c2678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outcomes['new_12MonthsOutcome'] = outcomes.apply(lambda x: (x.outcome_15months)|(x.outcome_18months)|(x.outcome_21months)|(x.outcome_24months), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392e6ea-c4dc-4ae3-b442-adbfc2196632",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_big = pickle.load(open('../SeqModel/data_all_big0.sav', 'rb'))\n",
    "vocab_all_big = pickle.load(open('../SeqModel/vocab_all_big0.sav', 'rb'))\n",
    "for i in range(1,3):\n",
    "    temp_data = pickle.load(open('../SeqModel/data_all_big'+str(i)+'.sav', 'rb'))\n",
    "    temp_vocab = pickle.load(open('../SeqModel/vocab_all_big'+str(i)+'.sav', 'rb'))\n",
    "    data_all_big = pd.concat([data_all_big, temp_data])\n",
    "    vocab_all_big = vocab_all_big + temp_vocab\n",
    "    \n",
    "    \n",
    "#vocab and code2idx generation\n",
    "# vocab_all_big = vocab_all\n",
    "vocab_all_big=list(dict.fromkeys(vocab_all_big)) #remove duplicate\n",
    "idx_all_big = range(3, len(vocab_all_big)+3)\n",
    "code2idx_all_big = dict(zip(vocab_all_big, idx_all_big))\n",
    "idx2code_all_big = dict(zip(idx_all_big, vocab_all_big))\n",
    "code2idx_all_big['PAD'] = 0\n",
    "code2idx_all_big['start_visit'] = 1\n",
    "code2idx_all_big['end_visit'] = 2\n",
    "idx2code_all_big[0] = 'PAD'\n",
    "idx2code_all_big[1] = 'start_visit'\n",
    "idx2code_all_big[2] = 'end_visit'\n",
    "\n",
    "VOCAB_SIZE_big = len(code2idx_all_big)\n",
    "print('code2idx Size: {}'.format(len(code2idx_all_big)))\n",
    "print('idx2code Size: {}'.format(len(idx2code_all_big)))\n",
    "# print(code2idx)\n",
    "\n",
    "# padded to index, and merge with patient data (age, Country)\n",
    "# data_all_big = data_all\n",
    "data_all_big['read_code_seq_padded_idx'] = data_all_big['read_code_seq_padded'].apply(lambda x: [code2idx_all_big.get(key) for key in x])\n",
    "data_all_big = data_all_big.merge(outcomes[['patid', 'outcome_3months','outcome_combined_6months','outcome_combined_12months',\n",
    "                               'outcome_combined_24months', 'new_12MonthsOutcome']], how='inner', on='patid')\n",
    "data_all_big = data_all_big.merge(patient[['patid', 'Country', 'age']], how='left', on='patid')\n",
    "data_all_big = data_all_big.drop_duplicates(subset=['patid']).reset_index(drop=True)      \n",
    "\n",
    "print(data_all_big.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ea4b7-62b3-444e-9757-d7c7c3f5a163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #load clinical information\n",
    "# path = '../ServerData_13Oct2020/'\n",
    "# clinical_files = [join(path, f) for f in listdir(path) if (isfile(join(path, f))) & ('f_clinical_part' in f)]\n",
    "# clinical = pyreadr.read_r('../ServerData_13Oct2020/f_clinical_part1.Rdata')\n",
    "# clinical = clinical['f_clinical_part']\n",
    "# for file in clinical_files[1:3]:\n",
    "#     temp = pyreadr.read_r(file)\n",
    "#     temp = temp['f_clinical_part']\n",
    "#     print(temp.shape)\n",
    "#     clinical = pd.concat([clinical, temp])\n",
    "#     clinical.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# #data selection \n",
    "# clinical = clinical.dropna(subset=['code_id'])\n",
    "# clinical['event_date'] = pd.to_datetime(clinical['event_date'])\n",
    "# clinical = clinical.loc[(clinical['event_date'] >= '2016-01-01') & (clinical['event_date'] < '2018-01-01')]\n",
    "\n",
    "# #add paractice information\n",
    "\n",
    "# #load clinical information\n",
    "# path = '../ServerData_13Oct2020/'\n",
    "# therapy_files = [join(path, f) for f in listdir(path) if (isfile(join(path, f))) & ('f_therapy_part' in f)]\n",
    "# therapy = pyreadr.read_r('../ServerData_13Oct2020/f_therapy_part1.Rdata')\n",
    "# therapy = therapy['f_therapy_part']\n",
    "# for file in therapy_files[1:3]:\n",
    "#     temp = pyreadr.read_r(file)\n",
    "#     temp = temp['f_therapy_part']\n",
    "#     print(temp.shape)\n",
    "#     therapy = pd.concat([therapy, temp])\n",
    "#     therapy.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# #data selection \n",
    "# therapy = therapy.dropna(subset=['code_id'])\n",
    "# therapy['event_date'] = pd.to_datetime(therapy['event_date'])\n",
    "# therapy = therapy.loc[(therapy['event_date'] >= '2016-01-01') & (therapy['event_date'] < '2018-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdc0f3-e73b-4850-a0a4-fccb9d333116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_raw_data = pd.concat([clinical[['patid', 'event_date', 'code_id']],\n",
    "#            therapy[['patid', 'event_date', 'code_id']]])\n",
    "# all_raw_data.reset_index(drop=True, inplace=True)\n",
    "# #vocab and code2idx generation\n",
    "# vocab_all = all_raw_data.code_id.unique().tolist()\n",
    "# idx_all = range(1, len(vocab_all)+1)\n",
    "\n",
    "# code2idx_all = dict(zip(vocab_all, idx_all))\n",
    "# idx2code_all = dict(zip(idx_all, vocab_all))\n",
    "\n",
    "# code2idx_all['PAD'] = 0\n",
    "# code2idx_all['start_visit'] = 1\n",
    "# code2idx_all['end_visit'] = 2\n",
    "# idx2code_all[0] = 'PAD'\n",
    "# idx2code_all[1] = 'start_visit'\n",
    "# idx2code_all[2] = 'end_visit'\n",
    "# VOCAB_SIZE= len(code2idx_all)\n",
    "# print('code2idx Size: {}'.format(len(code2idx_all)))\n",
    "# print('idx2code Size: {}'.format(len(idx2code_all)))\n",
    "# # print(code2idx)\n",
    "# #extract year, month, day from event date\n",
    "# all_raw_data['day'] = all_raw_data.apply(lambda x: str(x['event_date'].day), axis=1)\n",
    "# all_raw_data['month'] = all_raw_data.apply(lambda x: str(x['event_date'].month), axis=1)\n",
    "# all_raw_data['year'] = all_raw_data.apply(lambda x: str(x['event_date'].year), axis=1)\n",
    "# all_raw_data.reset_index(drop=True, inplace=True)\n",
    "# all_raw_data['read_code_seq_perdate'] = all_raw_data.sort_values(['event_date'], ascending=True).groupby(['patid', 'event_date'])['code_id'].transform(lambda x: ', '.join(x))\n",
    "# all_raw_data['read_code_seq_perdate'] = all_raw_data['read_code_seq_perdate'].apply(lambda x: 'start_visit, ' + x + ', end_visit')\n",
    "# all_raw_data = all_raw_data.drop_duplicates(subset=['patid', 'event_date']).reset_index(drop=True)\n",
    "# all_raw_data['read_code_seq'] = all_raw_data.sort_values(['event_date'], ascending=True).groupby(['patid'])['read_code_seq_perdate'].transform(lambda x: ', '.join(x))\n",
    "# # all_raw_data['read_code_seq'] = all_raw_data.sort_values(['event_date'], ascending=True).groupby('patid')['code_id'].transform(lambda x: ', '.join(x))\n",
    "# event_data_seq_all = all_raw_data.sort_values(['event_date']).groupby('patid').agg({'day': lambda x: x.tolist(),\n",
    "#                                                           'month': lambda x: x.tolist(),\n",
    "#                                                           'year': lambda x: x.tolist()}).reset_index()\n",
    "# all_raw_data = all_raw_data.drop_duplicates(subset=['patid']).reset_index(drop=True)     \n",
    "# data_all = all_raw_data[['patid', 'read_code_seq']].merge(event_data_seq_all, how='left',  on='patid')\n",
    "# data_all = data_all.merge(outcomes[['patid', 'new_12MonthsOutcome',\n",
    "#                            'outcome_combined_24months']], how='inner', on='patid')\n",
    "\n",
    "# data_all['read_code_seq'] = data_all['read_code_seq'].apply(lambda x: x.strip('\"\"').split(', '))\n",
    "# data_all['length_read_code_seq'] = data_all['read_code_seq'].apply(lambda x: len(x))\n",
    "# data_all = data_all[data_all.length_read_code_seq > 10]\n",
    "\n",
    "# #padding at the beginning of the list\n",
    "# max_seq = 300\n",
    "# def make_uniform_data(x):\n",
    "#     if len(x) < max_seq:\n",
    "#         pads = ['PAD'] * (max_seq - len(x))\n",
    "#         return pads + x\n",
    "#     elif len(x) > max_seq:\n",
    "#         x = x[len(x)-max_seq:]\n",
    "#         return x\n",
    "#     else:\n",
    "#         return x\n",
    "    \n",
    "# data_all['read_code_seq_padded'] = data_all['read_code_seq'].apply(lambda x: make_uniform_data(x))\n",
    "# data_all['read_code_seq_padded_idx'] = data_all['read_code_seq_padded'].apply(lambda x: [code2idx_all.get(key) for key in x])\n",
    "# data_all = data_all.merge(patient[['patid', 'Country', 'age']], how='left', on='patid')\n",
    "# data_all = data_all.drop_duplicates(subset=['patid']).reset_index(drop=True)     \n",
    "\n",
    "# #SAVE all important materials\n",
    "# pickle.dump(code2idx_all, open('../SeqModel/code2idx_all_new.sav', 'wb'))\n",
    "# pickle.dump(idx2code_all, open('../SeqModel/idx2code_all_new.sav', 'wb'))\n",
    "# pickle.dump(data_all, open('../SeqModel/data_all_new.sav', 'wb'))\n",
    "# # data_all.to_csv('../SeqModel/seqData_all.csv', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d8f95-286c-4067-8985-4b7d6f9d0b6c",
   "metadata": {},
   "source": [
    "# LSTM Model using Clinical + Therapy sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1286de-3d50-4eb3-b3c7-a4404748f00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 13:49:40.917881: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-16 13:49:41.217307: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-16 13:49:41.217361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-16 13:49:41.219110: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-16 13:49:41.374685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import AUC, SensitivityAtSpecificity\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, Adamax, SGD\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import L1L2, L1, L2\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "# from tensorflow.keras.backend.tensorflow_backend import set_session\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "#internal validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score, matthews_corrcoef, auc, average_precision_score, roc_auc_score, balanced_accuracy_score, roc_curve, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "target_outcome = 'new_12MonthsOutcome'\n",
    "max_codes = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d66269-80cc-4450-9432-b75bcb598efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "    \n",
    "code2idx = pickle.load(open('../SeqModel/code2idx_all_big.sav', 'rb'))\n",
    "idx2code = pickle.load(open('../SeqModel/idx2code_all_big.sav', 'rb'))\n",
    "data = pickle.load(open('../SeqModel/data_all_big.sav', 'rb'))\n",
    "\n",
    "vocab_size = len(code2idx)\n",
    "print(vocab_size)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23098e77-76c5-422c-bdbd-a0152e9876c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Data split conventional (mixed countries)\n",
    "# trainingData, testData = train_test_split(data, test_size=0.1, stratify=data[target_outcome], random_state=1234)\n",
    "# trainingData, valData = train_test_split(trainingData, test_size=0.2, stratify=trainingData[target_outcome], random_state=1234)\n",
    "# print(trainingData.shape)\n",
    "# print(valData.shape)\n",
    "# print(testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d0809-4055-42b9-a738-5438011b7eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Data split, train=England, eval=Scot+Wales\n",
    "# data = data_all_big\n",
    "# data_all_big = [] #free up the memory\n",
    "vocab_size = len(code2idx)\n",
    "trainingData = data[(data.Country == 'England') & (data.age >= 18)]\n",
    "trainingData, valData = train_test_split(trainingData, test_size=0.3, stratify=trainingData[target_outcome], shuffle=True, random_state=1234)\n",
    "trainingData, evalData = train_test_split(trainingData, test_size=0.2, stratify=trainingData[target_outcome], shuffle=True, random_state=1234)\n",
    "testData = data[((data.Country == 'Wales') | (data.Country == 'Scotland')) & (data.age >= 18)]\n",
    "testDataWales = data[(data.Country == 'Wales') & (data.age >= 18)]\n",
    "testDataScotland = data[(data.Country == 'Scotland') & (data.age >= 18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f157bd3-b8a1-47e5-b6de-3e4f7d466bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train: ', trainingData.shape[0])\n",
    "print('Val: ', valData.shape[0])\n",
    "print('Eval (internal validation): ', evalData.shape[0])\n",
    "print('Test: ', testData.shape[0])\n",
    "print('Test - Wales: ', testDataWales.shape[0])\n",
    "print('Test - Scotland: ', testDataScotland.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf217de-6216-4a7d-b8ef-705c69fb52cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trainingData.patid.unique().shape)\n",
    "print(trainingData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8913a7a-5e44-4359-85a4-5e4bdf6501c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make sure no data leak between sets\n",
    "print(list(set(trainingData.patid.values).intersection(set(valData.patid.values))))\n",
    "print(list(set(trainingData.patid.values).intersection(set(evalData.patid.values))))\n",
    "print(list(set(valData.patid.values).intersection(set(evalData.patid.values))))\n",
    "print(list(set(valData.patid.values).intersection(set(testData.patid.values))))\n",
    "print(list(set(trainingData.patid.values).intersection(set(testData.patid.values))))\n",
    "print(len(list(set(testData.patid.values).intersection(set(testDataScotland.patid.values))))) # here data leak is expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae2fa9-09d0-4bdc-ba46-3abec45b4c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trainingData[target_outcome].value_counts(normalize=True))\n",
    "print(valData[target_outcome].value_counts(normalize=True))\n",
    "print(evalData[target_outcome].value_counts(normalize=True))\n",
    "print(testData[target_outcome].value_counts(normalize=True))\n",
    "print(testDataWales[target_outcome].value_counts(normalize=True))\n",
    "print(testDataScotland[target_outcome].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed906e-adb2-4c91-b2ef-fb50bd84ba3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X and y\n",
    "X_train = np.array(trainingData.read_code_seq_padded_idx.values)\n",
    "X_train = np.array([x for x in X_train])\n",
    "X_val = np.array(valData.read_code_seq_padded_idx.values)\n",
    "X_val = np.array([x for x in X_val])\n",
    "X_eval = np.array(evalData.read_code_seq_padded_idx.values)\n",
    "X_eval = np.array([x for x in X_eval])\n",
    "X_test = np.array(testData.read_code_seq_padded_idx.values)\n",
    "X_test = np.array([x for x in X_test])\n",
    "X_testWales = np.array(testDataWales.read_code_seq_padded_idx.values)\n",
    "X_testWales = np.array([x for x in X_testWales])\n",
    "X_testScotland = np.array(testDataScotland.read_code_seq_padded_idx.values)\n",
    "X_testScotland = np.array([x for x in X_testScotland])\n",
    "\n",
    "y_train = trainingData[target_outcome].values\n",
    "y_val = valData[target_outcome].values\n",
    "y_eval = evalData[target_outcome].values\n",
    "y_test = testData[target_outcome].values\n",
    "y_testWales = testDataWales[target_outcome].values\n",
    "y_testScotland = testDataScotland[target_outcome].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5e859-7525-40af-966e-02c99c4399af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train: ', X_train.shape[0])\n",
    "print('Val: ', X_val.shape[0])\n",
    "print('Eval (internal validation): ', X_eval.shape[0])\n",
    "print('Test: ', X_test.shape[0])\n",
    "print('Test - Wales: ', X_testWales.shape[0])\n",
    "print('Test - Scotland: ', X_testScotland.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d48bc-7f2c-49f0-89d3-6ce06ebfb9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_weight = trainingData[target_outcome].value_counts()[0]/trainingData[target_outcome].value_counts()[1]\n",
    "neg_weight = trainingData[target_outcome].value_counts()[1]/trainingData[target_outcome].value_counts()[0]\n",
    "class_weight = {0:1, 1:pos_weight}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf992d-9764-4eb0-8761-efcde1fb94fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_bias = np.array([np.log(neg_weight)])\n",
    "output_bias = Constant(output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd13bd1-f529-4385-98ed-c7b6c9f4cd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valData.new_12MonthsOutcome.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b572f71-e586-4cba-b0cf-eb194000acbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "sklearn_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight = dict(enumerate(sklearn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76806668-a75f-4958-951b-70277beadd46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pickle.dump(code2idx_all_big, open('../SeqModel/code2idx_all_big.sav', 'wb'))\n",
    "# pickle.dump(idx2code_all_big, open('../SeqModel/idx2code_all_big.sav', 'wb'))\n",
    "# pickle.dump(data, open('../SeqModel/data_all_big.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeed10f-72a8-457d-bfcb-f2662e74bfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create the model\n",
    "# embedding_vector_length = int(np.sqrt(vocab_size))\n",
    "embedding_vector_length = 350\n",
    "earlyStopping = EarlyStopping(monitor='val_auc', patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "mcp_save = ModelCheckpoint('../SeqModel/seqModel_all_new.mdl_wts.hdf5', save_best_only=True, monitor='val_auc', mode='min')\n",
    "# class_weight = {0: 1, 1: 6}\n",
    "\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_codes))\n",
    "    model.add(LSTM(128, return_sequences=True,\n",
    "                   bias_regularizer=L1L2(l1=0.0, l2=0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(128, return_sequences=True,\n",
    "                   bias_regularizer=L1L2(l1=0.0, l2=0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(128, bias_regularizer=L1L2(l1=0.0, l2=0.01)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = SGD(learning_rate=5e-3, clipvalue=0.5)\n",
    "    metrics = [\n",
    "        AUC(num_thresholds=3, name='auc', curve='ROC'),\n",
    "        tf.keras.metrics.Precision(name='prec'),\n",
    "        tf.keras.metrics.Recall(name='rec'),\n",
    "        tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "        tf.keras.metrics.TruePositives(name='TP'),\n",
    "        tf.keras.metrics.PrecisionAtRecall(0.8)\n",
    "    ]\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=metrics, )\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                        epochs=1000, batch_size=256, \n",
    "                        class_weight = class_weight, \n",
    "                        callbacks = [earlyStopping, mcp_save, PlotLossesKeras()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a56d1-042d-4fb9-98ba-55323ae95c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cf3d2-2342-4df8-98ee-ecca942d40a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29527ec-f7d3-4668-ad0e-92c91e57411e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('../SeqModel/vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('../SeqModel/metadata.tsv', 'w', encoding='utf-8')\n",
    "for index, word in enumerate(vocab):\n",
    "  if (index == 0)|(index == 1)|(index == 2):\n",
    "    continue  # skip 0,1,2, it's padding, satr and end\n",
    "  vec = embeddings_vector[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e76ce-0a39-4ca0-a8cd-d9ba570ddd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbddf77-c941-48d2-9bbd-b80b77d85e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "# plt.ylim(0.55,1)\n",
    "plt.title('model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "# plt.ylim(0.1, 1.15)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c11344-dfa9-40e6-b2d2-63388d84ba09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    # model.evaluate(X_train, y_train)\n",
    "    model.evaluate(X_val, y_val, batch_size=300)\n",
    "    model.evaluate(X_eval, y_eval, batch_size=300)\n",
    "    model.evaluate(X_test, y_test, batch_size=300)\n",
    "    model.evaluate(X_testWales, y_testWales, batch_size=300)\n",
    "    model.evaluate(X_testScotland, y_testScotland, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fd35d-2903-48e4-a655-b55f16b99da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model evaluation function\n",
    "def summariseResult (testY, preds):\n",
    "    tn, fp, fn, tp = confusion_matrix(testY, preds).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    ppv = 100*tp/(tp+fp)\n",
    "    npv = 100*tn/(fn+tn)\n",
    "    acc = accuracy_score(testY, preds)\n",
    "    f1score = f1_score(testY, preds, average = 'binary')\n",
    "    balanceacc = balanced_accuracy_score(testY, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(testY, preds, pos_label=1)\n",
    "    aucscore = auc(fpr, tpr)\n",
    "    # aucscore = roc_auc_score(testY, preds)\n",
    "    auprc = average_precision_score(testY, preds)\n",
    "    # plot_confusion_matrix(model, testX, testY, cmap='viridis')  \n",
    "    return np.round(acc,4), np.round(specificity,4), np.round(sensitivity,4), np.round(aucscore,4), np.round(auprc,4), np.round(balanceacc,4), np.round(f1score,4), np.round(ppv,4), np.round(npv,4)\n",
    "\n",
    "data_test_Xs = [X_eval, X_test, X_testWales, X_testScotland]\n",
    "data_test_ys = [y_eval, y_test, y_testWales, y_testScotland]\n",
    "for data_test_X, data_test_y in zip(data_test_Xs, data_test_ys):\n",
    "    with tf.device('/CPU:0'):\n",
    "        preds = model.predict(data_test_X)\n",
    "    preds = [0 if pred <0.5 else 1 for pred in preds]\n",
    "    print(summariseResult(data_test_y, np.squeeze(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a50962-2f80-4931-8eb7-df84792130ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.save('../SeqModel/model_all.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f42bb-89f7-4693-852f-ae329cc7afe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c4b94-3df3-47ba-9ef3-ab2d5c61df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = tf.keras.Model(inputs=model.inputs, outputs=model.get_layer('embedding').output)\n",
    "embeddings_vector = model.layers[0].get_weights()[0]\n",
    "\n",
    "# `embeddings` has a shape of (num_vocab, embedding_dim) \n",
    "\n",
    "# `word_to_index` is a mapping (i.e. dict) from words to their index, e.g. `love`: 69\n",
    "words_embeddings = {w:embeddings_vector[idx] for w, idx in code2idx.items()}\n",
    "\n",
    "vocab = list(code2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cf9ea-6871-4009-9e7e-f626adbde85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_2dim = PCA(random_state=0).fit_transform(embeddings_vector)[:,:2]\n",
    "pca_2dim = pd.DataFrame(pca_2dim, columns=['PC1', 'PC2'])\n",
    "pca_2dim['code'] = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59d49e-2c2f-4fb3-8327-62ce56522beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asthma_readcodes = pd.read_csv('../RCodes/ClinicalCodes/definite_asthma_read_codes.csv')[['readcode_new']]\n",
    "ethnicity = pd.read_csv('../RCodes/ClinicalCodes/Ethinicity.csv')[['Read.Code']]\n",
    "smoking = pd.read_csv('../RCodes/ClinicalCodes/SmokingReadcodes.csv')[['Read Code']]\n",
    "ICS = pd.read_csv('../RCodes/ClinicalCodes/ICS_Asthma.csv')[['code_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb211b9-1c91-421a-9c6b-6fada17fe001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asthma_readcodes.columns = ['ReadCode']\n",
    "asthma_readcodes['label'] = 'asthma specific'\n",
    "asthma_readcodes['label_num'] = 0\n",
    "ethnicity.columns = ['ReadCode']\n",
    "ethnicity['label'] = 'ethnicity'\n",
    "ethnicity['label_num'] = '1'\n",
    "smoking.columns = ['ReadCode']\n",
    "smoking['label'] = 'smoking'\n",
    "smoking['label_num'] = '2'\n",
    "ICS.columns = ['ReadCode']\n",
    "ICS['label'] = 'ICS'\n",
    "ICS['label_num'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0effbdc3-8838-4044-8832-7896bc25f704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat([asthma_readcodes, ethnicity, smoking, ICS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aafc54-69d3-4a8c-99d2-7604318ba548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_subset = pca_2dim[pca_2dim.code.isin(df.ReadCode.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758107c-f214-4ac1-b0a0-4d237b5c6793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_subset = df.merge(pca_subset, how='right', left_on='ReadCode', right_on='code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e4cba-da65-4c65-b756-7fcbf75920e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(pca_subset.PC1.values,pca_subset.PC2.values, c=pca_subset.label_num.values)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
