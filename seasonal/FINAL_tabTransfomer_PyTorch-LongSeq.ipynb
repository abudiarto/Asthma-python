{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7beaaf7c-63b5-494e-a159-2aa47abb6fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done!\n"
     ]
    }
   ],
   "source": [
    "## Import dependencies \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy\n",
    "# %matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "import os, sys, pathlib, gc\n",
    "import re, math, random, time\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Union, Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from torcheval.metrics.functional.aggregation.auc import auc\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('import done!')\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56bf441c-24a6-42d1-8517-e04efc521f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds setted!\n"
     ]
    }
   ],
   "source": [
    "## For reproducible results    \n",
    "def seed_all(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    # tf.random.set_seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(s) \n",
    "    print('Seeds setted!')\n",
    "global_seed = 2\n",
    "seed_all(global_seed)\n",
    "\n",
    "## Limit GPU Memory in TensorFlow\n",
    "## Because TensorFlow, by default, allocates the full amount of available GPU memory when it is launched. \n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# if len(physical_devices) > 0:\n",
    "#     for device in physical_devices:\n",
    "#         tf.config.experimental.set_memory_growth(device, True)\n",
    "#         print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n",
    "# else:\n",
    "#     print(\"Not enough GPU hardware devices available\")\n",
    "    \n",
    "## For Seaborn Setting\n",
    "custom_params = {\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    'grid.alpha': 0.3,\n",
    "    'figure.figsize': (16, 6),\n",
    "    'axes.titlesize': 'Large',\n",
    "    'axes.labelsize': 'Large',\n",
    "    'figure.facecolor': '#fdfcf6',\n",
    "    'axes.facecolor': '#fdfcf6',\n",
    "}\n",
    "cluster_colors = ['#b4d2b1', '#568f8b', '#1d4a60', '#cd7e59', '#ddb247', '#d15252']\n",
    "sns.set_theme(\n",
    "    style='whitegrid',\n",
    "    #palette=sns.color_palette(cluster_colors),\n",
    "    rc=custom_params,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7261c-2b46-4ade-a2cb-8c6df1511ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate seasonal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7e969-91dc-4f7b-b2c4-2ffc72e23b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#######Create seasonal data\n",
    "\n",
    "\n",
    "gridSearchData, crossValData, internalEvaluationData, externalEvaluationData = pickle.load(open('../Clean_data/dataset_scaled_2vs1_25102024.sav', 'rb'))\n",
    "features = pickle.load(open('../Clean_data/sequence_features_long_full.sav', 'rb'))\n",
    "\n",
    "gridSearch_seq = gridSearchData[['patid', 'outcome_12months']].merge(features, on = 'patid', how='inner').fillna(0).drop_duplicates('patid').reset_index(drop=True)\n",
    "crossVal_seq = crossValData[['patid', 'outcome_12months']].merge(features, on = 'patid', how='inner').fillna(0).drop_duplicates('patid').reset_index(drop=True)\n",
    "internalVal_seq = internalEvaluationData[['patid', 'outcome_12months']].merge(features, on = 'patid', how='inner').fillna(0).drop_duplicates('patid').reset_index(drop=True)\n",
    "externalVal_seq = externalEvaluationData[['patid', 'outcome_12months']].merge(features, on = 'patid', how='inner').fillna(0).drop_duplicates('patid').reset_index(drop=True)\n",
    "\n",
    "\n",
    "categorical_seq_columns = gridSearch_seq.columns[gridSearch_seq.columns.str.contains('BTS')].tolist()\n",
    "numerical_seq_columns = gridSearch_seq.columns[(~gridSearch_seq.columns.str.contains('BTS'))&(gridSearch_seq.columns.str.contains('_month'))].tolist()\n",
    "print(f'total features:  {len(categorical_seq_columns)+len(numerical_seq_columns)}')\n",
    "\n",
    "\n",
    "#fix BTS step column from object to int\n",
    "for var in categorical_seq_columns:\n",
    "    gridSearch_seq[var] = gridSearch_seq[var].astype('int')\n",
    "    crossVal_seq[var] = crossVal_seq[var].astype('int')\n",
    "    internalVal_seq[var] = internalVal_seq[var].astype('int')\n",
    "    externalVal_seq[var] = externalVal_seq[var].astype('int')\n",
    "\n",
    "\n",
    "var_seq = gridSearch_seq.columns[gridSearch_seq.columns.str.contains('_month111')]\n",
    "for var in var_seq:\n",
    "    var = var[:-3]\n",
    "    print(var, 'season 1')\n",
    "    gridSearch_seq[var+'_season1'] = gridSearch_seq.apply(lambda x: x[var+'1'] + x[var+'2'], axis=1)\n",
    "    start = 3\n",
    "    for i in range(1, 40):\n",
    "        print(var, 'season', str(i+1))\n",
    "        if ('BTS' in var):\n",
    "            gridSearch_seq[var+'_season'+str(i+1)] = gridSearch_seq.apply(lambda x: np.round(np.mean([x[var+str(i*start)], x[var+str(i*start+1)], x[var+str(i*3+2)]])), axis=1)\n",
    "        else:\n",
    "            gridSearch_seq[var+'_season'+str(i+1)] = gridSearch_seq.apply(lambda x: x[var+str(i*start)] + x[var+str(i*start+1)] + x[var+str(i*3+2)], axis=1)\n",
    "    gridSearch_seq[var+'_season'+str(i+2)] = gridSearch_seq.apply(lambda x: x[var+'120'], axis=1)\n",
    "    print(var, 'season 41')\n",
    "\n",
    "var_seq = crossVal_seq.columns[crossVal_seq.columns.str.contains('_month111')]\n",
    "for var in var_seq:\n",
    "    var = var[:-3]\n",
    "    print(var, 'season 1')\n",
    "    crossVal_seq[var+'_season1'] = crossVal_seq.apply(lambda x: x[var+'1'] + x[var+'2'], axis=1)\n",
    "    start = 3\n",
    "    for i in range(1, 40):\n",
    "        print(var, 'season', str(i+1))\n",
    "        if ('BTS' in var):\n",
    "            crossVal_seq[var+'_season'+str(i+1)] = crossVal_seq.apply(lambda x: np.round(np.mean([x[var+str(i*start)], x[var+str(i*start+1)], x[var+str(i*3+2)]])), axis=1)\n",
    "        else:\n",
    "            crossVal_seq[var+'_season'+str(i+1)] = crossVal_seq.apply(lambda x: x[var+str(i*start)] + x[var+str(i*start+1)] + x[var+str(i*3+2)], axis=1)\n",
    "    crossVal_seq[var+'_season'+str(i+2)] = crossVal_seq.apply(lambda x: x[var+'120'], axis=1)\n",
    "    print(var, 'season 41')\n",
    "\n",
    "var_seq = internalVal_seq.columns[internalVal_seq.columns.str.contains('_month111')]\n",
    "for var in var_seq:\n",
    "    var = var[:-3]\n",
    "    print(var, 'season 1')\n",
    "    internalVal_seq[var+'_season1'] = internalVal_seq.apply(lambda x: x[var+'1'] + x[var+'2'], axis=1)\n",
    "    start = 3\n",
    "    for i in range(1, 40):\n",
    "        print(var, 'season', str(i+1))\n",
    "        if ('BTS' in var):\n",
    "            internalVal_seq[var+'_season'+str(i+1)] = internalVal_seq.apply(lambda x: np.round(np.mean([x[var+str(i*start)], x[var+str(i*start+1)], x[var+str(i*3+2)]])), axis=1)\n",
    "        else:\n",
    "            internalVal_seq[var+'_season'+str(i+1)] = internalVal_seq.apply(lambda x: x[var+str(i*start)] + x[var+str(i*start+1)] + x[var+str(i*3+2)], axis=1)\n",
    "    internalVal_seq[var+'_season'+str(i+2)] = internalVal_seq.apply(lambda x: x[var+'120'], axis=1)\n",
    "    print(var, 'season 41')\n",
    "\n",
    "var_seq = externalVal_seq.columns[externalVal_seq.columns.str.contains('_month111')]\n",
    "for var in var_seq:\n",
    "    var = var[:-3]\n",
    "    print(var, 'season 1')\n",
    "    externalVal_seq[var+'_season1'] = externalVal_seq.apply(lambda x: x[var+'1'] + x[var+'2'], axis=1)\n",
    "    start = 3\n",
    "    for i in range(1, 40):\n",
    "        print(var, 'season', str(i+1))\n",
    "        if ('BTS' in var):\n",
    "            externalVal_seq[var+'_season'+str(i+1)] = externalVal_seq.apply(lambda x: np.round(np.mean([x[var+str(i*start)], x[var+str(i*start+1)], x[var+str(i*3+2)]])), axis=1)\n",
    "        else:\n",
    "            externalVal_seq[var+'_season'+str(i+1)] = externalVal_seq.apply(lambda x: x[var+str(i*start)] + x[var+str(i*start+1)] + x[var+str(i*3+2)], axis=1)\n",
    "    externalVal_seq[var+'_season'+str(i+2)] = externalVal_seq.apply(lambda x: x[var+'120'], axis=1)\n",
    "    print(var, 'season 41')\n",
    "\n",
    "\n",
    "gridSearch_seq = gridSearch_seq[['patid']+gridSearch_seq.columns[gridSearch_seq.columns.str.contains('season')].tolist()]\n",
    "crossVal_seq = crossVal_seq[['patid']+crossVal_seq.columns[crossVal_seq.columns.str.contains('season')].tolist()]\n",
    "internalVal_seq = internalVal_seq[['patid']+internalVal_seq.columns[internalVal_seq.columns.str.contains('season')].tolist()]\n",
    "externalVal_seq = externalVal_seq[['patid']+externalVal_seq.columns[externalVal_seq.columns.str.contains('season')].tolist()]\n",
    "\n",
    "\n",
    "# pickle.dump([gridSearch_seq, crossVal_seq, internalVal_seq, externalVal_seq], open('./OPCRD_ASTHMA/Clean_data/seasonal_long_dataset_28102024.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a73d87-62ab-43e1-af25-4d51fdee0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = gridSearch_seq, crossVal_seq, internalVal_seq, externalVal_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411236b-d075-4692-9a50-be84de21d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(datasets, open('../Clean_data/seasonal_long_dataset_28102024.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611e72b-2a2a-48f8-bcd8-5149d05f04fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prepare sequence of categorical vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fbb70-6aab-4025-b30a-862eb1d40371",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_seq, crossVal_seq, internalVal_seq, externalVal_seq = pickle.load(open('../Clean_data/seasonal_long_dataset_28102024.sav', 'rb'))\n",
    "datasets = [gridSearch_seq, crossVal_seq, internalVal_seq, externalVal_seq]\n",
    "datasets_name = ['gridSearch_seq', 'crossVal_seq', 'internalVal_seq', 'externalVal_seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d674199-6228-4ae9-8dff-c12162851ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_continuous(x):\n",
    "    if x==0:\n",
    "        return '0'\n",
    "    elif x==1:\n",
    "        return '1'\n",
    "    elif x==2:\n",
    "        return '2'\n",
    "    elif (x>2) :\n",
    "        return '>=3'\n",
    "    \n",
    "#Average daily ICS\n",
    "def cat_ics(x):\n",
    "    if (x==0):\n",
    "        return '0'\n",
    "    elif (x>=1) & (x<=200):\n",
    "        return '1-200'\n",
    "    elif (x>200) & (x<=400):\n",
    "        return '201-400'\n",
    "    elif (x>400):\n",
    "        return '>400'\n",
    "    \n",
    "# features['cat_average_daily_dose_ICS'] = features.average_daily_dose_ICS.apply(lambda x: cat_ics(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6418f96-51fc-4639-af00-45e816b9cf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['numAsthmaAttacks_',\n",
    "       'numAntibioticsEvents_',\n",
    "       'numAntibioticswithLRTI_', 'numOCSEvents_',\n",
    "       'numOCSwithLRTI_', 'numHospEvents_',\n",
    "       'numPCS_', 'numPCSAsthma_']\n",
    "ics_vars = ['average_daily_dose_ICS_', 'prescribed_daily_dose_ICS_']\n",
    "count=0\n",
    "output = []\n",
    "for data, data_name in zip(datasets, datasets_name):\n",
    "    print(data_name)\n",
    "    for num_var in num_vars:\n",
    "        season_vars = data.columns[data.columns.str.contains(num_var)]\n",
    "        for var in season_vars:\n",
    "            print(var)\n",
    "            data[var] = data[var].apply(lambda x: cat_continuous(x))\n",
    "    for ics_var in ics_vars:\n",
    "        season_vars = data.columns[data.columns.str.contains(ics_var)]\n",
    "        for var in season_vars:\n",
    "            print(var)\n",
    "            data[var] = data[var].apply(lambda x: cat_ics(x))\n",
    "    output.append(data)\n",
    "pickle.dump(output, open('../Clean_data/seasonal_cat_dataset_04112024.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7f6c2-9b4f-4f3d-a4ae-1b40e0600920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e880f94c-e1a8-469f-a2f8-919242496e54",
   "metadata": {},
   "source": [
    "## Load data asthma OPCRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621bc36a-e6bf-4d0a-9f0c-5b49b414e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters setted!\n"
     ]
    }
   ],
   "source": [
    "exp_config = {\n",
    "    'n_bins': 10,\n",
    "    'n_splits': 5,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 0.0001,\n",
    "    'train_epochs': 50,\n",
    "    'finalize': True,\n",
    "    'finalize_epochs': 8,\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'cat_embedding_dim': 2,\n",
    "    'num_transformer_blocks': 4,\n",
    "    'num_heads': 2,\n",
    "    'tf_dropout_rates': [0.3, 0.3, 0.3, 0.3,],\n",
    "    'ff_dropout_rates': [0.3, 0.3, 0.3, 0.3,],\n",
    "    'mlp_dropout_rates': [0.2, 0.1],\n",
    "    'mlp_hidden_units_factors': [2, 1],\n",
    "    'patience': 5\n",
    "}\n",
    "\n",
    "print('Parameters setted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccce9e37-7d39-4601-956f-07090dc3c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_seq, crossVal_seq, internalVal_seq, externalVal_seq = pickle.load(open('../Clean_data/seasonal_cat_dataset_04112024.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9f632-c361-4b5f-8d67-265c21daa696",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex', 'BMI_cat', 'ethnic_group', 'smokingStatus' , 'PEFStatus', 'EosinophilLevel', \n",
    "                       'DeviceType', 'imd_decile', 'PriorEducation', 'rhinitis', 'cardiovascular', 'heartfailure', \n",
    "                       'psoriasis', 'anaphylaxis', 'diabetes', 'ihd', 'anxiety', 'eczema', 'nasalpolyps',\n",
    "                      'BTS_step', 'age_cat',\n",
    "                      ]\n",
    "\n",
    "# numerical_columns = ['age', \n",
    "#                      'numOCSEvents', 'numPCS', 'numPCSAsthma', 'numAntibioticsEvents',\n",
    "#                       'numAntibioticswithLRTI', 'numOCSEvents', 'numOCSwithLRTI',\n",
    "#                       'numAsthmaAttacks', 'numAcuteRespEvents', 'numHospEvents',\n",
    "#                     ]\n",
    "\n",
    "# categorical_columns=['sex', 'rhinitis', 'cardiovascular', 'heartfailure', 'psoriasis', 'anaphylaxis', 'diabetes', 'ihd', \n",
    "#                   'anxiety', 'eczema', 'nasalpolyps', 'asthmaPlan', 'BMI_cat_normal', 'BMI_cat_not recorded', \n",
    "#                   'BMI_cat_obese', 'BMI_cat_overweight', 'BMI_cat_underweight', 'ethnic_group_Asian', 'ethnic_group_Black', \n",
    "#                   'ethnic_group_Mixed', 'ethnic_group_Other', 'ethnic_group_White', 'ethnic_group_not recorded', \n",
    "#                   'smokingStatus_current', 'smokingStatus_former', 'smokingStatus_never', 'imd_decile_0', \n",
    "#                   'imd_decile_1', 'imd_decile_2', 'imd_decile_3', 'imd_decile_4', 'imd_decile_5', 'imd_decile_6', \n",
    "#                   'imd_decile_7', 'imd_decile_8', 'imd_decile_9', 'imd_decile_10', 'CharlsonScore_0.0', \n",
    "#                   'CharlsonScore_1.0', 'CharlsonScore_2.0', 'CharlsonScore_3.0', 'CharlsonScore_4.0', 'CharlsonScore_5.0', \n",
    "#                   'CharlsonScore_6.0', 'CharlsonScore_7.0', 'CharlsonScore_8.0', 'CharlsonScore_9.0', 'CharlsonScore_10.0', \n",
    "#                   'CharlsonScore_11.0', 'CharlsonScore_12.0', 'PEFStatus_60-80', 'PEFStatus_less than 60', 'PEFStatus_more than 80', \n",
    "#                   'PEFStatus_not recorded', 'EosinophilLevel_high', 'EosinophilLevel_normal', 'EosinophilLevel_not recorded', \n",
    "#                   'BTS_step_0', 'BTS_step_1', 'BTS_step_2', 'BTS_step_3', 'BTS_step_4', 'BTS_step_5', 'DeviceType_BAI', \n",
    "#                   'DeviceType_DPI', 'DeviceType_NEB', 'DeviceType_not recorded', 'DeviceType_pMDI', 'PriorEducation_No', \n",
    "#                   'PriorEducation_Yes']\n",
    "numerical_columns = ['age', 'numAsthmaManagement', 'numAsthmaReview', 'numAsthmaMedReview', 'numAsthmaReviewRCP']\n",
    "\n",
    "identifier = ['patid']\n",
    "outcome = ['outcome_12months']\n",
    "\n",
    "\n",
    "print('total features: ', len(categorical_columns)+len(numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6701416-abbe-484d-ad0f-26b17883afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch, crossVal, internalEvaluation, externalEvaluation = pickle.load(open('../Clean_data/dataset_scaled_2vs1_25102024.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e53679-96da-4441-a498-2795989d3c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, '0', '1', '>=3', 2, 1, '2'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridSearch.numAsthmaReview_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc26ccf-0d42-4ccf-9502-ad8408bd9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch, crossVal, internalEvaluation, externalEvaluation = pickle.load(open('../Clean_data/dataset_2vs1_25102024.sav', 'rb'))\n",
    "features = pickle.load(open('../Clean_data/cleaned_features_22102024.sav', 'rb'))\n",
    "# gridSearch_fold = gridSearch[identifier+outcome].merge(features[identifier+categorical_columns+numerical_columns], on='patid', how='inner').reset_index(drop=True)\n",
    "# train_fold = crossVal[identifier+outcome].merge(features[identifier+categorical_columns+numerical_columns], on='patid', how='inner').reset_index(drop=True)\n",
    "# valid_fold = internalEvaluation[identifier+outcome].merge(features[identifier+categorical_columns+numerical_columns], on='patid', how='inner').reset_index(drop=True)\n",
    "# test = externalEvaluation[identifier+outcome].merge(features[identifier+categorical_columns+numerical_columns], on='patid', how='inner').reset_index(drop=True)\n",
    "\n",
    "gridSearch_fold = gridSearch[identifier+categorical_columns+numerical_columns+outcome]\n",
    "train_fold = crossVal[identifier+categorical_columns+numerical_columns+outcome]\n",
    "valid_fold = internalEvaluation[identifier+categorical_columns+numerical_columns+outcome]\n",
    "test = externalEvaluation[identifier+categorical_columns+numerical_columns+outcome]\n",
    "print('Grid search: {:d}, Train set: {:d}, Validation set: {:d}, Test set: {:d}'.\\\n",
    "      format(gridSearch_fold.shape[0], train_fold.shape[0], valid_fold.shape[0], test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c9753-a8c1-4c2c-a0fe-8dcf812ea213",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_fold = gridSearch_fold.merge(gridSearch_seq, on='patid', how='inner').reset_index(drop=True)\n",
    "train_fold = train_fold.merge(crossVal_seq, on='patid', how='inner').reset_index(drop=True)\n",
    "valid_fold = valid_fold.merge(internalVal_seq, on='patid', how='inner').reset_index(drop=True)\n",
    "test = test.merge(externalVal_seq, on='patid', how='inner').reset_index(drop=True)\n",
    "print('Grid search: {:d}, Train set: {:d}, Validation set: {:d}, Test set: {:d}'.\\\n",
    "      format(gridSearch_fold.shape[0], train_fold.shape[0], valid_fold.shape[0], test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294d837-73e1-49c0-ab57-60b5226cdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling data 30%\n",
    "\n",
    "train_fold = train_fold.sample(frac=.03, random_state=global_seed).reset_index(drop=True)\n",
    "valid_fold = valid_fold.sample(frac=.05, random_state=global_seed).reset_index(drop=True)\n",
    "test = test.sample(frac=.2, random_state=global_seed).reset_index(drop=True)\n",
    "\n",
    "print('Train set: {:d}, Validation set: {:d}, Test set: {:d}'.\\\n",
    "      format(train_fold.shape[0], valid_fold.shape[0], test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19065c4d-7df4-4025-93eb-0fe45292f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridSearch_fold[outcome].value_counts(normalize=True))\n",
    "print(train_fold[outcome].value_counts(normalize=True))\n",
    "print(valid_fold[outcome].value_counts(normalize=True))\n",
    "print(test[outcome].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f31d1-6011-4e43-8c52-df7dc8ad8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_seq_columns = training_seq.columns[training_seq.columns.str.contains('BTS')].tolist()\n",
    "# numerical_seq_columns = training_seq.columns[(~training_seq.columns.str.contains('BTS'))&(training_seq.columns.str.contains('_season'))].tolist()\n",
    "# print(f'total features:  {len(categorical_seq_columns)+len(numerical_seq_columns)}')\n",
    "\n",
    "# categorical_columns \n",
    "# # = categorical_columns + categorical_seq_columns\n",
    "# numerical_columns = numerical_columns + numerical_seq_columns\n",
    "\n",
    "# print('total features: ', len(categorical_columns)+len(numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa28d0-71b7-42c1-8c4d-015246c79f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#numerical to categorical\n",
    "\n",
    "# def convertNum2Cat(x):\n",
    "#     if x == 0:\n",
    "#         return 0\n",
    "#     elif (x > 0) & (x<2):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 2\n",
    "\n",
    "# datasets = [train_fold, valid_fold, test]\n",
    "# for dataset in datasets:\n",
    "#     for var in numerical_seq_columns:\n",
    "#         print(var)\n",
    "#         dataset[var] = dataset[var].apply(lambda x: convertNum2Cat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c3276-5f00-4f40-b0a0-19eadedfd36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Making Lookup table of categorical featurs and target\n",
    "# Using sklearn.preprocessing.OrdinalEncoder\n",
    "oe = OrdinalEncoder(handle_unknown='error',\n",
    "                    dtype=np.int64, )\n",
    "\n",
    "encoded = oe.fit_transform(train_fold[categorical_columns].values)\n",
    "#decoded = oe.inverse_transform(encoded)\n",
    "train_fold[categorical_columns] = encoded\n",
    "\n",
    "valid_fold[categorical_columns] = oe.transform(valid_fold[categorical_columns].values)\n",
    "gridSearch_fold[categorical_columns] = oe.transform(gridSearch_fold[categorical_columns].values)\n",
    "test[categorical_columns] = oe.transform(test[categorical_columns].values)\n",
    "\n",
    "encoder_categories = oe.categories_\n",
    "print(len(encoder_categories))\n",
    "# encoder_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ce6ee-8772-4d9b-a897-4b88618cf1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_seq_columns = training_seq.columns[(training_seq.columns.str.contains('_season'))].tolist()\n",
    "numerical_seq_columns = gridSearch_seq.columns[(~gridSearch_seq.columns.str.contains('BTS'))&(gridSearch_seq.columns.str.contains('_season'))].tolist()\n",
    "print(f'total features:  {len(numerical_seq_columns)}')\n",
    "\n",
    "# categorical_columns = categorical_columns + categorical_seq_columns\n",
    "numerical_columns = numerical_columns + numerical_seq_columns\n",
    "\n",
    "print('total features: ', len(categorical_columns)+len(numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f550aa-f69c-41c2-9bc3-54f3e4daf1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, numerical_columns,\n",
    "                 categorical_columns, target=None):\n",
    "        self.df = df\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        \n",
    "        for nc in self.numerical_columns:\n",
    "            x = torch.tensor(self.df[nc][index],\n",
    "                             dtype=torch.float32)\n",
    "            x = torch.unsqueeze(x, dim=0)\n",
    "            data[nc] = x\n",
    "            \n",
    "        for cc in self.categorical_columns:\n",
    "            x = torch.tensor(self.df[cc][index],\n",
    "                             dtype=torch.int32)\n",
    "            x = torch.unsqueeze(x, dim=0)\n",
    "            data[cc] = x\n",
    "\n",
    "       \n",
    "        if self.target is not None:\n",
    "            label = torch.tensor(self.df[self.target][index],\n",
    "                                 dtype=torch.float32)\n",
    "            label = torch.unsqueeze(label, dim=-1)\n",
    "            return data, label\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b426680-5e60-4aed-a30e-d8d6e63e3e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create Datasets\n",
    "gridSearch_ds = BuildDataset(\n",
    "    gridSearch_fold,\n",
    "    numerical_columns,\n",
    "    categorical_columns,\n",
    "    target=outcome[0]\n",
    ")\n",
    "\n",
    "train_ds = BuildDataset(\n",
    "    train_fold,\n",
    "    numerical_columns,\n",
    "    categorical_columns,\n",
    "    target=outcome[0]\n",
    ")\n",
    "\n",
    "val_ds = BuildDataset(\n",
    "    valid_fold,\n",
    "    numerical_columns, \n",
    "    categorical_columns, \n",
    "    target=outcome[0]\n",
    ")\n",
    "\n",
    "test_ds = BuildDataset(\n",
    "    test,\n",
    "    numerical_columns, \n",
    "    categorical_columns,\n",
    "    target=outcome[0]\n",
    ")\n",
    "\n",
    "## Operation Check\n",
    "index = 0\n",
    "print(gridSearch_ds.__getitem__(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b465c4-f1de-477b-af8a-e2fad85f8c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create DataLoaders\n",
    "batch_size = exp_config['batch_size']\n",
    "\n",
    "gridSearch_dl = torch.utils.data.DataLoader(\n",
    "    gridSearch_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dl = torch.utils.data.DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    test_ds, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    # drop_last=False\n",
    ")\n",
    "\n",
    "dl_dict = {'train': train_dl, 'val': val_dl}\n",
    "\n",
    "## Operation Check\n",
    "sample_data, sample_label = next(iter(dl_dict['train']))\n",
    "input_dtypes = {}\n",
    "for key in sample_data:\n",
    "    input_dtypes[key] = sample_data[key].dtype\n",
    "    print(f'{key}, shape:{sample_data[key].shape}, dtype:{sample_data[key].dtype}')\n",
    "\n",
    "print('Label shape: ', sample_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc00fd-6be3-4514-9698-cbbfefc942d9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d5bb4-77ac-4bcc-8432-57d24add6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(nn.Module):\n",
    "    def __init__(self, numerical_columns, categorical_columns, encoder_categories, emb_dim):\n",
    "        super().__init__()\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.encoder_categories = encoder_categories\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embed_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, categorical in enumerate(categorical_columns):\n",
    "            num_embeddings = len(self.encoder_categories[i])\n",
    "            embedding = nn.Embedding(\n",
    "                num_embeddings=num_embeddings,\n",
    "                embedding_dim=self.emb_dim,\n",
    "            )\n",
    "            self.embed_layers[categorical] = embedding\n",
    "\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x_nums = []\n",
    "        for numerical in self.numerical_columns:\n",
    "            x_num = torch.unsqueeze(x[numerical], dim=1)\n",
    "            x_nums.append(x_num)\n",
    "        if len(x_nums) > 0:\n",
    "            x_nums = torch.cat(x_nums, dim=1)\n",
    "        else:\n",
    "            x_nums = torch.tensor(x_nums, dtype=torch.float32)\n",
    "        \n",
    "        x_cats = []\n",
    "        for categorical in self.categorical_columns:\n",
    "            x_cat = self.embed_layers[categorical](x[categorical])\n",
    "            x_cats.append(x_cat)\n",
    "        if len(x_cats) > 0:\n",
    "            x_cats = torch.cat(x_cats, dim=1)\n",
    "        else:\n",
    "            x_cats = torch.tensor(x_cats, dtype=torch.float32)\n",
    "        \n",
    "        return x_nums, x_cats\n",
    "    \n",
    "## Operation Check\n",
    "preprocessor = Preprocessor(numerical_columns,\n",
    "                            categorical_columns,\n",
    "                            encoder_categories,\n",
    "                            emb_dim=model_config['cat_embedding_dim'])\n",
    "x_nums, x_cats = preprocessor(sample_data)\n",
    "x_nums = x_nums.reshape(exp_config['batch_size'],len(numerical_columns))\n",
    "x_nums.shape, x_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cf6e8-4516-4b0b-be8c-eb5a6eba86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, n_features, hidden_units,\n",
    "                 dropout_rates):\n",
    "        super().__init__()\n",
    "        self.mlp_layers = nn.Sequential()\n",
    "        num_features = n_features\n",
    "        for i, units in enumerate(hidden_units):\n",
    "            self.mlp_layers.add_module(f'norm_{i}', nn.BatchNorm1d(num_features))\n",
    "            self.mlp_layers.add_module(f'dense_{i}', nn.Linear(num_features, units))\n",
    "            self.mlp_layers.add_module(f'act_{i}', nn.SELU())\n",
    "            self.mlp_layers.add_module(f'dropout_{i}', nn.Dropout(dropout_rates[i]))\n",
    "            num_features = units\n",
    "            \n",
    "    def forward(self, x):\n",
    "        y = self.mlp_layers(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff171cf5-3bb7-4519-b523-9595da8bd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabTransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim,\n",
    "                 attn_dropout_rate, ff_dropout_rate):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, num_heads,\n",
    "                                          dropout=attn_dropout_rate,\n",
    "                                          batch_first=True)\n",
    "        self.norm_1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm_2 = nn.LayerNorm(emb_dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ff_dropout_rate), \n",
    "            nn.Linear(emb_dim*4, emb_dim))\n",
    "        \n",
    "    def forward(self, x_cat):\n",
    "        attn_output, attn_output_weights = self.attn(x_cat, x_cat, x_cat)\n",
    "        x_skip_1 = x_cat + attn_output\n",
    "        x_skip_1 = self.norm_1(x_skip_1)\n",
    "        feedforward_output = self.feedforward(x_skip_1)\n",
    "        x_skip_2 = x_skip_1 + feedforward_output\n",
    "        x_skip_2 = self.norm_2(x_skip_2)\n",
    "        return x_skip_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714ec1b-758e-4ef3-98b9-a850c589bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabTransformer(nn.Module): \n",
    "    def __init__(self, numerical_columns, categorical_columns,\n",
    "                 num_transformer_blocks, num_heads, emb_dim,\n",
    "                 attn_dropout_rates, ff_dropout_rates,\n",
    "                 mlp_dropout_rates,\n",
    "                 mlp_hidden_units_factors,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.transformers = nn.Sequential()\n",
    "        for i in range(num_transformer_blocks):\n",
    "            self.transformers.add_module(f'transformer_{i}', \n",
    "                                        TabTransformerBlock(num_heads,\n",
    "                                                            emb_dim,\n",
    "                                                            attn_dropout_rates[i],\n",
    "                                                            ff_dropout_rates[i]))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.num_norm = nn.LayerNorm(len(numerical_columns))\n",
    "        \n",
    "        self.n_features = (len(categorical_columns) * emb_dim) + len(numerical_columns)\n",
    "        mlp_hidden_units = [int(factor * self.n_features) \\\n",
    "                            for factor in mlp_hidden_units_factors]\n",
    "        self.mlp = MLPBlock(self.n_features, mlp_hidden_units,\n",
    "                            mlp_dropout_rates)\n",
    "        \n",
    "        self.final_dense = nn.Linear(mlp_hidden_units[-1], 1)\n",
    "        self.final_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_nums, x_cats):\n",
    "        contextualized_x_cats = self.transformers(x_cats)\n",
    "        contextualized_x_cats = self.flatten(contextualized_x_cats)\n",
    "        \n",
    "        if x_nums.shape[-1] > 0:\n",
    "            x_nums = self.num_norm(x_nums)\n",
    "            features = torch.cat((x_nums, contextualized_x_cats), -1)\n",
    "        else:\n",
    "            features = contextualized_x_cats\n",
    "            \n",
    "        mlp_output = self.mlp(features)\n",
    "        model_output = self.final_dense(mlp_output)\n",
    "        output = self.final_sigmoid(model_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12d135-145b-43b6-85aa-6422e32d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TabTransformer Model Check\n",
    "\n",
    "## Settings for TabTransformer\n",
    "emb_dim = model_config['cat_embedding_dim']\n",
    "num_transformer_blocks = model_config['num_transformer_blocks']\n",
    "num_heads = model_config['num_heads']\n",
    "attn_dropout_rates = model_config['tf_dropout_rates']\n",
    "ff_dropout_rates = model_config['ff_dropout_rates']\n",
    "mlp_dropout_rates = model_config['mlp_dropout_rates']\n",
    "mlp_hidden_units_factors = model_config['mlp_hidden_units_factors']\n",
    "\n",
    "## Building Models\n",
    "preprocessor = Preprocessor(numerical_columns, categorical_columns,\n",
    "                            encoder_categories, emb_dim)\n",
    "\n",
    "model = TabTransformer(numerical_columns, categorical_columns,\n",
    "                       num_transformer_blocks, num_heads, emb_dim,\n",
    "                       attn_dropout_rates, ff_dropout_rates,\n",
    "                       mlp_dropout_rates, mlp_hidden_units_factors)\n",
    "\n",
    "## Operation, Parameters and Model Structure Check\n",
    "x_nums, x_cats = preprocessor(sample_data)\n",
    "x_nums = x_nums.reshape(exp_config['batch_size'],len(numerical_columns))\n",
    "y = model(x_nums, x_cats)\n",
    "print('Numerical Input shape: ', x_nums.shape)\n",
    "print('Categorical Input shape: ', x_cats.shape)\n",
    "print('Output shape: ', y.shape)\n",
    "\n",
    "print('# of Preprocessor parameters: ',\n",
    "      sum(p.numel() for p in preprocessor.parameters() if p.requires_grad))\n",
    "print('# of N-BEATS parameters: ',\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e492b8-e564-4c6c-b067-a4519af15e4f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff657b3-97e3-4020-81dc-22c4ae3f558c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3985dcc-7783-4ccc-9f52-5d1f7199a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCELoss_class_weighted(weights):\n",
    "    def loss(input, target):\n",
    "        input = torch.clamp(input,min=1e-7,max=1-1e-7)\n",
    "        bce = - weights[1] * target * torch.log(input) - (1 - target) * weights[0] * torch.log(1 - input)\n",
    "        return torch.mean(bce)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0100cb-2342-4576-8ceb-ed9a372bc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss Function\n",
    "pos_weight = np.round(train_fold[outcome].value_counts()[0]/train_fold[outcome].value_counts()[1], 2)\n",
    "weights = torch.FloatTensor([1, pos_weight])  #class weight\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = BCELoss_class_weighted(weights)\n",
    "\n",
    "## Optimizer and Learning Rate Scheduler\n",
    "epochs = exp_config['train_epochs']\n",
    "batch_size = exp_config['batch_size']\n",
    "steps_per_epoch = len(train_fold) // batch_size\n",
    "\n",
    "learning_rate = exp_config['learning_rate']\n",
    "weight_decay = exp_config['weight_decay']\n",
    "params = list(preprocessor.parameters()) + list(model.parameters())\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=params,\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer=optimizer,\n",
    "    T_max=epochs*steps_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106556c-4997-4341-a45e-11f588e2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Displaying Learning Rate\n",
    "def lr_plot(lr_scheduler, steps):\n",
    "    lrs = []\n",
    "    for _ in range(steps):\n",
    "        optimizer.step()\n",
    "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "        lr_scheduler.step()\n",
    "    xs = [i+1 for i in range(steps)]\n",
    "    plt.figure(figsize=(7,5))\n",
    "    # print(lrs)\n",
    "    ax = sns.lineplot(x = xs, y = lrs)\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    \n",
    "\n",
    "\n",
    "## Create New Optimizer and Lr_scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=params,\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer=optimizer,\n",
    "    T_max=epochs*steps_per_epoch\n",
    ")\n",
    "lr_plot(lr_scheduler, epochs*steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6511c-63d8-4216-b6dc-30fa6c8ce57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "def auc_metric(predictions, targets):\n",
    "    # Convert predictions and targets to numpy arrays\n",
    "    # predictions = predictions.detach().numpy()\n",
    "    # targets = targets.detach().numpy()\n",
    "\n",
    "    # Calculate AUC using sklearn\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(targets, predictions)\n",
    "\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d8f68-7c6d-416c-9696-a366c570cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for the Model Training\n",
    "def train_model(model, preprocessor,\n",
    "                dl_dict, criterion,\n",
    "                optimizer, lr_scheduler,\n",
    "                num_epochs, best_model_path, \n",
    "                patience, finalize=False):\n",
    "    ## Checking usability of GUP\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'device: {device}')\n",
    "    print('-------Start Training-------')\n",
    "    model.to(device)\n",
    "    \n",
    "    ## training and validation loop\n",
    "    if finalize:\n",
    "        phases = ['train']\n",
    "    else:\n",
    "        phases = ['train', 'val']\n",
    "        \n",
    "    losses = {phase: [] for phase in phases}\n",
    "    AUCs = {phase: [] for phase in phases}\n",
    "    best_auc = 0\n",
    "    break_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        break_flag = False\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                preprocessor.train()\n",
    "                model.train()\n",
    "            else:\n",
    "                preprocessor.eval()\n",
    "                model.eval()\n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            epoch_auc = 0.0\n",
    "            raw_preds = torch.empty((0, 1), dtype=torch.float32).to(device)\n",
    "            final_preds = torch.empty((0, 1), dtype=torch.float32).to(device)\n",
    "            all_labels = torch.empty((0, 1), dtype=torch.float32).to(device)\n",
    "            for data, labels in tqdm(dl_dict[phase]):\n",
    "                x_nums, x_cats = preprocessor(data)\n",
    "                x_nums = x_nums.reshape(x_nums.shape[0],x_nums.shape[1])\n",
    "                # x_nums = x_nums.to(device)\n",
    "                # x_cats = x_cats.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                ## Optimizer Initialization\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                ## Forward Processing\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = model(x_nums.to(device), x_cats.to(device))\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.where(outputs>0.5, 1., 0.)\n",
    "                    \n",
    "                    ## Backward Processing and Optimization\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        \n",
    "                    epoch_loss += loss.item() * x_cats.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels)\n",
    "                    raw_preds=torch.cat([raw_preds, outputs])\n",
    "                    final_preds=torch.cat([final_preds, preds])\n",
    "                    all_labels = torch.cat([all_labels, labels])\n",
    "            \n",
    "            epoch_loss = epoch_loss / len(dl_dict[phase].dataset)\n",
    "            losses[phase].append(epoch_loss)\n",
    "            epoch_acc = epoch_corrects / len(dl_dict[phase].dataset)\n",
    "            raw_preds = raw_preds.detach().cpu().flatten().numpy()\n",
    "            final_preds = final_preds.detach().cpu().flatten().numpy()\n",
    "            all_labels = all_labels.detach().cpu().flatten().numpy()\n",
    "            epoch_auc = roc_auc_score(all_labels, raw_preds)\n",
    "            AUCs[phase].append(epoch_auc)\n",
    "            ## Displaying results\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f} AUC: {:.4f} '.\\\n",
    "                  format(epoch+1, num_epochs, phase, epoch_loss, epoch_acc, epoch_auc))\n",
    "            if phase == 'val':\n",
    "                if epoch_auc > best_auc: #save model with best auc\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'val loss': epoch_loss,\n",
    "                        'val auc' : epoch_auc\n",
    "                        }, best_model_path)\n",
    "                    best_auc = epoch_auc #update best model\n",
    "                    break_counter = 0\n",
    "                else:\n",
    "                    break_counter+=1\n",
    "                    if break_counter>patience:\n",
    "                        break_flag = True\n",
    "                        break #break from epoch loop\n",
    "        if break_flag==True: \n",
    "            print('no more AUC improvement . . . . . . ')\n",
    "            break #break all the loops\n",
    "            \n",
    "    return model, preprocessor, losses, AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832be12c-1a8e-4f41-b11f-50fe6400f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Plotting Losses\n",
    "def plot_losses(losses, title=None):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    losses = pd.DataFrame(losses)\n",
    "    losses.index = [i+1 for i in range(len(losses))]\n",
    "    ax = sns.lineplot(data=losses)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfed46-e883-4cc2-9888-bca4cef17beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initiate model and optimiser instances\n",
    "model = TabTransformer(numerical_columns, categorical_columns,\n",
    "                           num_transformer_blocks, num_heads, emb_dim,\n",
    "                           attn_dropout_rates, ff_dropout_rates,\n",
    "                           mlp_dropout_rates, mlp_hidden_units_factors)\n",
    "    \n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=params,\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "\n",
    "continue_training = False #set to True if you want to continue training using a pretrained modedl\n",
    "\n",
    "if continue_training: #load pretrained model\n",
    "    best_model_path= '../MODELS/TabTransformerLong_22072024.tar' #specify pretrained model here\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) #load saved parameter to the model\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']) #load saved parameter to the optimiser\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['val loss']\n",
    "    auc = checkpoint['val auc']\n",
    "    \n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    #     optimizer=optimizer,\n",
    "    #     T_max=epochs*steps_per_epoch\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3e2b5-c9c6-4fb2-94b9-6e616f8b91b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Training\n",
    "best_model_path= '../MODELS/TabTransformerLong.tar'\n",
    "model_trained, preprocessor_trained, losses, AUCs = train_model(\n",
    "    model,\n",
    "    preprocessor,\n",
    "    dl_dict,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    epochs,\n",
    "    best_model_path,\n",
    "    model_config['patience']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be39c5d-2d8b-41e9-a5fe-0651876b4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Losses\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309fea1-7828-461c-9060-1150bf2def34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Losses\n",
    "plot_losses(AUCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8c9ea-fb7f-4b7f-be83-8d9305eda0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './OPCRD_ASTHMA/MODELS/tabTranasformer_seqLong_09072024.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac79651-a4f2-4d63-8c4b-572ff2b0dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds = torch.empty((0, 1), dtype=torch.float32).to('cuda')\n",
    "final_preds = torch.empty((0, 1), dtype=torch.float32).to('cuda')\n",
    "all_labels = torch.empty((0, 1), dtype=torch.float32).to('cuda')\n",
    "for data, labels in tqdm(test_dl):\n",
    "    labels = labels.to('cuda')\n",
    "    x_nums, x_cats = preprocessor(data)\n",
    "    x_nums = x_nums.reshape(x_nums.shape[0],x_nums.shape[1])\n",
    "    outputs = model(x_nums.to('cuda'), x_cats.to('cuda'))\n",
    "    preds = torch.where(outputs>0.5, 1., 0.)\n",
    "    raw_preds=torch.cat([raw_preds, outputs])\n",
    "    final_preds=torch.cat([final_preds, preds])\n",
    "    all_labels = torch.cat([all_labels, labels])\n",
    "\n",
    "raw_preds = raw_preds.detach().cpu().flatten().numpy()\n",
    "final_preds = final_preds.detach().cpu().flatten().numpy()\n",
    "all_labels = all_labels.detach().cpu().flatten().numpy()\n",
    "print(roc_auc_score(all_labels, raw_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e406e3c-4825-425c-9529-3214a649407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905e108-0a64-43ef-83ea-6db804ab900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(all_labels, final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f01fd-5ae6-4a98-b35b-2f54bc38e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Losses\n",
    "# plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17c366-45fa-4409-a170-152d4962c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ea5da-efc9-4036-86bb-18c473a7b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f75b5b-883c-4434-88f9-153e20cb97c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
