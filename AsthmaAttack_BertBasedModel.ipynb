{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b401ae-37c7-4b10-b363-9f7d080b03ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install transformers\n",
    "# !pip3 install accelerate\n",
    "# !pip3 install datasets\n",
    "# !pip3 install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f410ba-63ee-4c53-b3ee-072e0f6bacb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69add76f-7235-4300-94cb-aa0edb3082a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_clinical = pickle.load(open('../SeqModel/all_vocab_clinical_new.sav', 'rb'))\n",
    "vocab_therapy = pickle.load(open('../SeqModel/all_vocab_therapy_new.sav', 'rb'))\n",
    "\n",
    "data_clinical = pd.read_feather('../SeqModel/all_data_clinical_new.feather')\n",
    "data_therapy = pd.read_feather('../SeqModel/all_data_therapy_new.feather')\n",
    "\n",
    "vocab_stat_clinical = pd.read_csv('../FinalData/pivotClinicalCodesbyCountry.csv')\n",
    "vocab_stat_therapy = pd.read_csv('../FinalData/pivotTherapyCodesbyCountry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd673854-ea30-4bbd-92d1-de75607724b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_padding(x):\n",
    "    return np.delete(x, np.where(x=='PAD'))\n",
    "data_clinical['read_code_seq_padded_noPAD'] = data_clinical.read_code_seq_padded.apply(lambda x: np.delete(x, np.where(x=='PAD')))\n",
    "data_clinical['text'] = data_clinical['read_code_seq_padded_noPAD'].apply(lambda x: ' '.join(x))\n",
    "data_clinical['text_no_dot'] = data_clinical.text.apply(lambda x: x.replace('.', '^'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548cb301-e721-4f64-94ae-1da5a53b5c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001d5d24-f708-4381-ac84-e1aca8c6c38e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52714b0b-5917-4b0b-ba75-817e8f1a1dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b29cb0a-4dad-4411-803e-08434930bbd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenizer NLTK\n",
    "from tokenization_nltk import NlktTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605076b9-38f6-4f2f-9e43-a93dc23bba09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab = pickle.load(open('../SeqModel/all_vocab_clinical_new.sav', 'rb'))\n",
    "# with open(\"../SeqModel/vocab_clinical_nltk.txt\", \"w\") as txt_file:\n",
    "#     for code in vocab:\n",
    "#         txt_file.write(\"\".join(code) + \"\\n\") # works with any number of elements in a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7746d44-9c7d-477b-b9e6-ba95d5fd9e96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['42A^^', 'X77Wi', '44O^^', '423^^', '428^^', '42ZD^', '44J3^', 'Xab9D', 'XaEJK', '5372^', '892^^', '892^^', '8C15^', 'XaPkd', 'XaPVj', 'XaIrp', 'XaIvz', 'XaIrp', '9N42^', 'XaFBm', '9^^^^', 'XaZfY', '1713^', 'XaIqy', '9OJ^^', 'XaCH9', 'XaJYi', 'XaCJ0', 'XaORP', 'Ub1na', '8B314', 'XaQVY', 'H33^^', '136^^', 'XM0aD', '22K^^', 'XaZ4m', 'XaIyE', 'XaK6I', 'XaMiI', '2469^', '73050', 'XaIIW', 'XaKbt', '2431^', '8CA5^', '22A^^', 'XM1YA', '8795^', '246A^', 'XaIUi', 'XaQHq', '663g1', 'XaIQ0', 'XaIeq', 'XaY2V', 'XaINb', 'XaIfK', '242^^', '663Q^', 'XaIuD', '8CA4^', '246^^', 'XaPPD', 'XaLIn', 'XaEES', 'XaMiI', '44P^^', '44J3^', '44M4^', 'XaERu', '44I5^', 'XaEUq', 'XaELV', 'X77Wi', '44g1^', 'XM0lt', '44P5^', '44O^^', '44I4^', 'X77WP', 'XaLJx', '44F^^', 'XaK8y', '44Q^^', '44P6^', '1719^', '1719^', 'XaMiI', '1719^', 'XaMiI', 'Xa9Sm', 'XaMiI', '2431^', '1719^', '1719^', 'XaMiI', '535^^', 'XaMiI', 'H33^^', '3395^', '1719^', 'XaMiI', '9N4F^', 'XaIw3', 'Xa0Yg', '1719^', 'XaMiI', 'XaMiI', 'XaMiI', 'XaIw3', 'Xa0Yg', '1719^', 'XaMiI', 'XaMiI', 'XaMiI', 'XaAfm', '1719^', 'XaOwL', 'XaIgO', 'XaLr4', '1Y^^^', '68NV^', 'XaZ0d', 'XaMiI', 'XabYS', 'XSLCP', '73050', '246^^', 'C368^', '171^^', '2469^', '246A^', 'XaIgE', 'H2^^^', 'XaNwS', '44J3^', 'XaK8y', '44CC^', '171^^', '246A^', 'XaMiI', 'Xa0Yg', '44I4^', 'XaIw3', '246^^', 'X77Wi', 'XM0lt', '44I5^', '2469^']\n"
     ]
    }
   ],
   "source": [
    "tokenizerNLTK = NlktTokenizer(vocab_file='../SeqModel/vocab_clinical_nltk_new.txt', eos_token = \"<s>\")\n",
    "\n",
    "text = data_clinical.text_no_dot[1]\n",
    "tokens = tokenizerNLTK.tokenize(text)\n",
    "print(\"Tokens:\", tokens) #ouput: Tokens: ['Hello', 'Shirin', ',', 'How', 'are', 'you', '?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4168943f-2957-44f1-bee4-c81717b35540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 36, 118, 43, 47, 433, 51, 29, 40, 276, 156, 156, 1111, 147, 105, 1659, 1709, 1659, 79, 3985, 41, 206, 924, 861, 738, 306, 3784, 459, 127, 67, 50, 104, 25, 45, 333, 3, 404, 371, 158, 27, 4, 391, 151, 30, 59, 208, 2, 502, 203, 5, 499, 152, 95, 145, 1, 128, 72, 63, 35, 7, 1737, 176, 34, 478, 322, 114, 27, 96, 51, 64, 33, 52, 49, 28, 36, 217, 22, 38, 118, 53, 26, 19, 69, 24, 115, 84, 238, 238, 27, 238, 27, 3533, 27, 59, 238, 238, 27, 189, 27, 25, 61, 238, 27, 122, 729, 2087, 238, 27, 27, 27, 729, 2087, 238, 27, 27, 27, 713, 238, 493, 91, 3621, 805, 235, 23, 27, 289, 6981, 391, 34, 4918, 180, 4, 5, 226, 18253, 526, 51, 24, 328, 180, 5, 27, 2087, 53, 729, 34, 36, 22, 52, 4]\n"
     ]
    }
   ],
   "source": [
    "token_ids = [tokenizerNLTK.convert_token_to_id(token) for token in tokens]#lower() because the vocab.txt is all in lower case for us\n",
    "print(token_ids) #output: [1997, 4634, 2004, 11560, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7633ffc6-18a5-43ce-ba95-941c3a0fcced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_clinical = data_clinical.iloc[:1000]\n",
    "# dataset = data_clinical[['read_code_string']]\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "dataset = DatasetDict({'train': Dataset.from_pandas(data_clinical[['text']])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b80ef300-40c7-49c7-a3ed-6ce494208559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc0af702-15b0-4f22-bc23-b5fb53f864a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "963444c6-084c-4ae6-8b2f-7dede91b994e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f43f35bf-8774-4b6d-b12b-a41c7964d614",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_base(dataset[\"train\"]['text'][1]).input_ids\n",
    "# [tokenizer_base.decode(id) for id in tokens]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6ea2d30-1515-4f1b-89dc-443f102a0648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    dataset['train'][i : i + 1000][\"text\"]\n",
    "    for i in range(0, len(dataset), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05f93dd8-ea5b-4086-8e0d-aa6bd3c06d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    dataset['train'][\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d42c15c7-d893-4270-a4b1-af5f38d6e83d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "# all_code_list = []\n",
    "# for text in dataset['train']['text']:\n",
    "#     all_code_list = all_code_list + text.split(' ')\n",
    "\n",
    "# #total unique tokens in corpus\n",
    "# vocabsize = len(set(all_code_list))\n",
    "vocabsize = 3000\n",
    "print(vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ff43e4f-960b-4c4e-9279-8daf3ca220a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train tokenizer with train_new_from_iterator method\n",
    "tokenizer = tokenizer_base.train_new_from_iterator(training_corpus, vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a56ffb8-92c3-473c-a860-3721d4d62f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e2b31-da8a-4b6b-a19b-b17f194b3e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"../SeqModel/tokenizer_BERTEHR_08032024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59fb3f64-b96e-419a-a087-40f8392e10ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37659425\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "config = BertConfig(\n",
    "    hidden_size = 384,\n",
    "    vocab_size= len(tokenizerNLTK.vocab),\n",
    "    num_hidden_layers = 6,\n",
    "    num_attention_heads = 6,\n",
    "    intermediate_size = 1024,\n",
    "    max_position_embeddings = 100\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config=config)\n",
    "print(model.num_parameters()) #10457864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c28849-99b7-4586-9a14-6f7f3bb03495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78dabd-8002-46ed-9a0b-4f8e553eda3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8163391-f547-4ae0-995b-5e8aa7d684e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 14:39:40.892796: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-18 14:39:40.927011: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-18 14:39:40.927040: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-18 14:39:40.927060: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-18 14:39:40.934529: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizerNLTK, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "510b2f25-c4ba-4b55-9fc3-ac0807bc9720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, raw_datasets, max_length: int):\n",
    "        self.padding = \"max_length\"\n",
    "        self.text_column_name = 'text'\n",
    "        self.max_length = max_length\n",
    "        self.accelerator = Accelerator(gradient_accumulation_steps=1)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with self.accelerator.main_process_first():\n",
    "            self.tokenized_datasets = raw_datasets.map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=4,\n",
    "                remove_columns=[self.text_column_name],\n",
    "                desc=\"Running tokenizer on dataset line_by_line\",\n",
    "            )\n",
    "            self.tokenized_datasets.set_format('torch',columns=['input_ids'],dtype=torch.long)\n",
    "\n",
    "    def tokenize_function(self,examples):\n",
    "        examples[self.text_column_name] = [\n",
    "            line for line in examples[self.text_column_name] if len(line[0]) > 0 and not line[0].isspace()\n",
    "        ]\n",
    "        return self.tokenizer(\n",
    "            examples[self.text_column_name],\n",
    "            padding=self.padding,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_datasets)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokenized_datasets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bc1b219-36da-4fb2-b778-36ce20bec8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method LineByLineTextDataset.tokenize_function of <__main__.LineByLineTextDataset object at 0x1456b42f06d0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072804ebf1004ac2b6ef7a154c34875f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset line_by_line (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NlktTokenizer' object has no attribute '_in_target_context_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 623, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3482, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3361, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/tmp/ipykernel_5825/2718376119.py\", line 27, in tokenize_function\n    return self.tokenizer(\n  File \"/opt/conda/envs/rapids/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2827, in __call__\n    if not self._in_target_context_manager:\nAttributeError: 'NlktTokenizer' object has no attribute '_in_target_context_manager'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5825/818507531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tokenized_dataset_train = LineByLineTextDataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtokenizerNLTK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mraw_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# adjust this based on your requrements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_5825/2718376119.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, raw_datasets, max_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_process_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             self.tokenized_datasets = raw_datasets.map(\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 868\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    869\u001b[0m                 k: dataset.map(\n\u001b[1;32m    870\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m         return DatasetDict(\n\u001b[1;32m    868\u001b[0m             {\n\u001b[0;32m--> 869\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    870\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m         }\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3195\u001b[0m                         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\" (num_proc={num_proc})\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m                     ) as pbar:\n\u001b[0;32m-> 3197\u001b[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001b[0m\u001b[1;32m   3198\u001b[0m                             \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_per_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3199\u001b[0m                         ):\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m_write_generator_to_queue\u001b[0;34m()\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_write_generator_to_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3480\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3481\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3482\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3483\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m                 processed_inputs = {\n",
      "\u001b[0;32m/tmp/ipykernel_5825/2718376119.py\u001b[0m in \u001b[0;36mtokenize_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_column_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         ]\n\u001b[0;32m---> 27\u001b[0;31m         return self.tokenizer(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2825\u001b[0m             \u001b[0;31m# The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m             \u001b[0;31m# input mode in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2827\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2828\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2829\u001b[0m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NlktTokenizer' object has no attribute '_in_target_context_manager'"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_train = LineByLineTextDataset(\n",
    "    tokenizer= tokenizerNLTK,\n",
    "    raw_datasets = dataset,\n",
    "    max_length=50, # adjust this based on your requrements\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60bf8b52-2621-48ca-90ba-6cbd001b592a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def age_vocab(max_age, mon=1, symbol=None):\n",
    "    age2idx = {}\n",
    "    idx2age = {}\n",
    "    if symbol is None:\n",
    "        symbol = ['PAD', 'UNK']\n",
    "\n",
    "    for i in range(len(symbol)):\n",
    "        age2idx[str(symbol[i])] = i\n",
    "        idx2age[i] = str(symbol[i])\n",
    "\n",
    "    if mon == 12:\n",
    "        for i in range(max_age):\n",
    "            age2idx[str(i)] = len(symbol) + i\n",
    "            idx2age[len(symbol) + i] = str(i)\n",
    "    elif mon == 1:\n",
    "        for i in range(max_age * 12):\n",
    "            age2idx[str(i)] = len(symbol) + i\n",
    "            idx2age[len(symbol) + i] = str(i)\n",
    "    else:\n",
    "        age2idx = None\n",
    "        idx2age = None\n",
    "    return age2idx, idx2age\n",
    "\n",
    "global_params = {\n",
    "    'max_seq_len': 64,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1a728-3719-43c6-8723-90bad41e9f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ageVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6df879ab-83e1-4530-a919-1ab23112444e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._in_target_context_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859cc46-59dd-4fa1-a723-e92cc90765a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset_train['train']['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb48a4c-afe1-4dbc-89a1-bfca1b42170f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train']['text'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f2254-95c8-4198-b94b-43c9e4aab11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset_train['train'][5]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc8bfe-c940-4fe9-b9b8-fc023ec4130f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../SeqModel\",\n",
    "    overwrite_output_dir=True,\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=\"asthmaAttack\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_steps=500,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    report_to='none',\n",
    "    hub_private_repo = True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset_train['train'],\n",
    "    eval_dataset= tokenized_dataset_train['train'], # change to your actual evaluation dataset\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518fa6f-57ce-4449-857d-884a97732e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"../SeqModel/transformer_08032024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff43486-c54f-40c5-b852-4214bfaea21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcc6bb-edc1-412d-a09f-aaffdfb8e8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc07835-0c14-4ec6-b224-8ed36520fc36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7581c55f-8ea8-4c8d-a1b0-630f341257cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.base_model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94ddca-cf55-4a74-af9d-5098273b1a8b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33d331-d2e0-4606-920e-15fc8711ff89",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelDownstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe305ecb-0889-462a-8808-7e99891cbbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0dd284-3534-4e23-85b5-fce818cf39b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for code in all_code_list:\n",
    "#     if ('Op' in code):\n",
    "#         print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1de37-cc1a-4d9d-84e9-1f554d6b4222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6eafb-f16e-4237-af2c-e40a426be1ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set(all_code_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39545506-5a5f-4beb-88f4-33ddaec9958a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d6506-c79b-4a07-b06f-d40cc34179ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd4d8395-09fd-422c-8eb2-430df07089f4",
   "metadata": {},
   "source": [
    "# Fine Tune for downstream classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f6c5b-6a7f-452a-8926-4f7e9f24543e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f806bd6-03b4-46d3-8420-894237bb95a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('yelp_review_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e7eaf-fa5d-422f-aea7-49946a5dad61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc024e28-3c19-47f7-a548-52c053fa1fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_clinical = pd.read_feather('../SeqModel/all_data_clinical_new.feather')\n",
    "data_clinical['read_code_seq_padded_noPAD'] = data_clinical.read_code_seq_padded.apply(lambda x: np.delete(x, np.where(x=='PAD')))\n",
    "data_clinical['text'] = data_clinical['read_code_seq_padded_noPAD'].apply(lambda x: ' '.join(x))\n",
    "train_downstream = data_clinical.iloc[100000:200000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64f1b9-b316-4e4e-ad51-47a8f8005295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainingData, validationData = train_test_split(train_downstream, test_size=0.2, stratify=train_downstream['12months'], random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8f8ff-c5ac-4e7e-a258-f9bb6e625d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49551e23-8c3a-462b-9551-23df7ece558e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData['label'] = trainingData['12months']\n",
    "validationData['label'] = validationData['12months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cabc6b-9f75-448c-8a03-8735488b0cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = data_clinical[['read_code_string']]\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "dataset = DatasetDict({'train': Dataset.from_pandas(trainingData[['label', 'text']]),\n",
    "                      'test': Dataset.from_pandas(validationData[['label', 'text']])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8afc4-2b78-4704-bab5-c021baebbb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a60ce-fdbb-4396-a0cc-1d3bb243d3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"../SeqModel/tokenizer_BERTEHR_08032024/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b1a4c-508a-469b-a4d7-a625dc12bc75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb89ada-dbbc-4d25-82ff-1cb63669ce2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, raw_datasets, max_length: int):\n",
    "        self.padding = \"max_length\"\n",
    "        self.text_column_name = 'text'\n",
    "        self.max_length = max_length\n",
    "        self.accelerator = Accelerator(gradient_accumulation_steps=1)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with self.accelerator.main_process_first():\n",
    "            self.tokenized_datasets = raw_datasets.map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=4,\n",
    "                remove_columns=[self.text_column_name],\n",
    "                desc=\"Running tokenizer on dataset line_by_line\",\n",
    "            )\n",
    "            self.tokenized_datasets.set_format('torch',columns=['text'],dtype=torch.long)\n",
    "\n",
    "    def tokenize_function(self,examples):\n",
    "        examples[self.text_column_name] = [\n",
    "            line for line in examples[self.text_column_name] if len(line[0]) > 0 and not line[0].isspace()\n",
    "        ]\n",
    "        return self.tokenizer(\n",
    "            examples[self.text_column_name],\n",
    "            padding=self.padding,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_special_tokens_mask=False,\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_datasets)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokenized_datasets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f0faa-ddd4-4424-990d-60ca29c4b02f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets = LineByLineTextDataset(\n",
    "#     tokenizer= tokenizer,\n",
    "#     raw_datasets = dataset,\n",
    "#     max_length=50, # adjust this based on your requrements\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c96a10-02c6-42c2-aef5-9f95423fce59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=50, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07075f-6af3-4cb9-b34a-c34ee398c3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "modelDownstream = AutoModelForSequenceClassification.from_pretrained(\"../SeqModel/transformer_08032024/\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e2a97-dc05-4678-95be-aee38eac0803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# training_args = TrainingArguments(output_dir=\"../SeqModel/testResult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d5c2f-5398-43e4-9a04-25b033be017b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = roc_auc_score(y_true=labels, y_score=predictions)\n",
    "    return {\"AUC\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf27fb-9686-4a89-afe1-424e0b8f31a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28c5cb-f3af-4f0d-8bca-ddfdbe73e48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = roc_auc_score(y_true=labels, y_score=predictions)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e57d8-8363-412d-ab97-5f04e940430f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../SeqModel\",\n",
    "    overwrite_output_dir=True,\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=\"asthmaAttackDownstream\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_steps=500,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    report_to='none',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=modelDownstream,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset= tokenized_datasets['test'], # change to your actual evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10669df6-b25b-4b1f-a217-15d22b066288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaf530-e833-47c0-9dab-6a80f953df29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556e149-a68b-4d0d-b4ef-ab164c75da55",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embedding visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36175a-bb67-4829-8411-d53db3df9720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.base_model.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad026a2-f971-4449-8749-36fcb8e8d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_figure(ax, title:str='Title'):\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, **tfont)\n",
    "    ax.get_legend().set_title(\"\")\n",
    "    ax.get_legend().prop.set_family(lfont['fontname'])\n",
    "    ax.get_legend().prop.set_size(lfont['fontsize'])\n",
    "    ax.get_legend().get_frame().set_linewidth(0.0)\n",
    "    \n",
    "f, axs = plt.subplots(1,2,figsize=(14,6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue='ddc1', s=5, alpha=0.1, ax=axs[0]);\n",
    "tune_figure(axs[0], 'DDC1 Group')\n",
    "\n",
    "sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue='ddc2', s=5, alpha=0.1, ax=axs[1]);\n",
    "tune_figure(axs[1], 'DDC2 Group')\n",
    "\n",
    "# If you want to save the output then uncomment the next line\n",
    "#plt.savefig(os.path.join('data','DDC_Plot.png'), dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
